{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's this TensorFlow business?\n",
    "\n",
    "You've written a lot of code in this assignment to provide a whole host of neural network functionality. Dropout, Batch Norm, and 2D convolutions are some of the workhorses of deep learning in computer vision. You've also worked hard to make your code efficient and vectorized.\n",
    "\n",
    "For the last part of this assignment, though, we're going to leave behind your beautiful codebase and instead migrate to one of two popular deep learning frameworks: in this instance, TensorFlow (or PyTorch, if you switch over to that notebook)\n",
    "\n",
    "#### What is it?\n",
    "TensorFlow is a system for executing computational graphs over Tensor objects, with native support for performing backpropagation for its Variables. In it, we work with Tensors which are n-dimensional arrays analogous to the numpy ndarray.\n",
    "\n",
    "#### Why?\n",
    "\n",
    "* Our code will now run on GPUs! Much faster training. Writing your own modules to run on GPUs is beyond the scope of this class, unfortunately.\n",
    "* We want you to be ready to use one of these frameworks for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. \n",
    "* We want you to stand on the shoulders of giants! TensorFlow and PyTorch are both excellent frameworks that will make your lives a lot easier, and now that you understand their guts, you are free to use them :) \n",
    "* We want you to be exposed to the sort of deep learning code you might run into in academia or industry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How will I learn TensorFlow?\n",
    "\n",
    "TensorFlow has many excellent tutorials available, including those from [Google themselves](https://www.tensorflow.org/get_started/get_started).\n",
    "\n",
    "Otherwise, this notebook will walk you through much of what you need to do to train models in TensorFlow. See the end of the notebook for some links to helpful tutorials if you want to learn more or need further clarification on topics that aren't fully explained here.\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "This notebook has 5 parts. We will walk through TensorFlow at three different levels of abstraction, which should help you better understand it and prepare you for working on your project.\n",
    "\n",
    "1. Preparation: load the CIFAR-10 dataset.\n",
    "2. Barebone TensorFlow: we will work directly with low-level TensorFlow graphs. \n",
    "3. Keras Model API: we will use `tf.keras.Model` to define arbitrary neural network architecture. \n",
    "4. Keras Sequential API: we will use `tf.keras.Sequential` to define a linear feed-forward network very conveniently. \n",
    "5. CIFAR-10 open-ended challenge: please implement your own network to get as high accuracy as possible on CIFAR-10. You can experiment with any layer, optimizer, hyperparameters or other advanced features. \n",
    "\n",
    "Here is a table of comparison:\n",
    "\n",
    "| API           | Flexibility | Convenience |\n",
    "|---------------|-------------|-------------|\n",
    "| Barebone      | High        | Low         |\n",
    "| `tf.keras.Model`     | High        | Medium      |\n",
    "| `tf.keras.Sequential` | Low         | High        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a few minutes to download the first time you run it, but after that the files should be cached on disk and loading should be faster.\n",
    "\n",
    "In previous parts of the assignment we used CS231N-specific code to download and read the CIFAR-10 dataset; however the `tf.keras.datasets` package in TensorFlow provides prebuilt utility functions for loading many common datasets.\n",
    "\n",
    "For the purposes of this assignment we will still write our own code to preprocess the data and iterate through it in minibatches. The `tf.data` package in TensorFlow provides tools for automating this process, but working with this package adds extra complication and is beyond the scope of this notebook. However using `tf.data` can be much more efficient than the simple approach used in this notebook, so you should consider using it for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,) int32\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "def load_cifar10(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
    "    cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation: Dataset object\n",
    "\n",
    "For our own convenience we'll define a lightweight `Dataset` class which lets us iterate over data and labels. This is not the most flexible or most efficient way to iterate through data, but it will serve our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (64, 32, 32, 3) (64,)\n",
      "1 (64, 32, 32, 3) (64,)\n",
      "2 (64, 32, 32, 3) (64,)\n",
      "3 (64, 32, 32, 3) (64,)\n",
      "4 (64, 32, 32, 3) (64,)\n",
      "5 (64, 32, 32, 3) (64,)\n",
      "6 (64, 32, 32, 3) (64,)\n"
     ]
    }
   ],
   "source": [
    "# We can iterate through a dataset like this:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally **use GPU by setting the flag to True below**. It's not neccessary to use a GPU for this assignment; if you are working on Google Cloud then we recommend that you do not use a GPU, as it will be significantly more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  /cpu:0\n"
     ]
    }
   ],
   "source": [
    "# Set up some global variables\n",
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Barebone TensorFlow\n",
    "TensorFlow ships with various high-level APIs which make it very convenient to define and train neural networks; we will cover some of these constructs in Part III and Part IV of this notebook. In this section we will start by building a model with basic TensorFlow constructs to help you better understand what's going on under the hood of the higher-level APIs.\n",
    "\n",
    "TensorFlow is primarily a framework for working with **static computational graphs**. Nodes in the computational graph are Tensors which will hold n-dimensional arrays when the graph is run; edges in the graph represent functions that will operate on Tensors when the graph is run to actually perform useful computation.\n",
    "\n",
    "This means that a typical TensorFlow program is written in two distinct phases:\n",
    "\n",
    "1. Build a computational graph that describes the computation that you want to perform. This stage doesn't actually perform any computation; it just builds up a symbolic representation of your computation. This stage will typically define one or more `placeholder` objects that represent inputs to the computational graph.\n",
    "2. Run the computational graph many times. Each time the graph is run you will specify which parts of the graph you want to compute, and pass a `feed_dict` dictionary that will give concrete values to any `placeholder`s in the graph.\n",
    "\n",
    "### TensorFlow warmup: Flatten Function\n",
    "\n",
    "We can see this in action by defining a simple `flatten` function that will reshape image data for use in a fully-connected network.\n",
    "\n",
    "In TensorFlow, data for convolutional feature maps is typically stored in a Tensor of shape N x H x W x C where:\n",
    "\n",
    "- N is the number of datapoints (minibatch size)\n",
    "- H is the height of the feature map\n",
    "- W is the width of the feature map\n",
    "- C is the number of channels in the feature map\n",
    "\n",
    "This is the right way to represent the data when we are doing something like a 2D convolution, that needs spatial understanding of where the intermediate features are relative to each other. When we use fully connected affine layers to process the image, however, we want each datapoint to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data. So, we use a \"flatten\" operation to collapse the `H x W x C` values per representation into a single long vector. The flatten function below first reads in the value of N from a given batch of data, and then returns a \"view\" of that data. \"View\" is analogous to numpy's \"reshape\" method: it reshapes x's dimensions to be N x ??, where ?? is allowed to be anything (in this case, it will be H x W x C, but we don't need to specify that explicitly). \n",
    "\n",
    "**NOTE**: TensorFlow and PyTorch differ on the default Tensor layout; TensorFlow uses N x H x W x C but PyTorch uses N x C x H x W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    - TensorFlow Tensor of shape (N, D1, ..., DM)\n",
    "    \n",
    "    Output:\n",
    "    - TensorFlow Tensor of shape (N, D1 * ... * DM)\n",
    "    \"\"\"\n",
    "    N = tf.shape(x)[0]\n",
    "    return tf.reshape(x, (N, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  <class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"Placeholder:0\", dtype=float32, device=/device:CPU:0)\n",
      "x_flat:  <class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"Reshape:0\", shape=(?, ?), dtype=float32, device=/device:CPU:0)\n",
      "\n",
      "x_np:\n",
      " [[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]] \n",
      "\n",
      "x_flat_np:\n",
      " [[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11.]\n",
      " [12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23.]] \n",
      "\n",
      "x_np:\n",
      " [[[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]]\n",
      "\n",
      " [[ 6  7]\n",
      "  [ 8  9]\n",
      "  [10 11]]] \n",
      "\n",
      "x_flat_np:\n",
      " [[ 0.  1.  2.  3.  4.  5.]\n",
      " [ 6.  7.  8.  9. 10. 11.]]\n"
     ]
    }
   ],
   "source": [
    "def test_flatten():\n",
    "    # Clear the current TensorFlow graph.\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Stage I: Define the TensorFlow graph describing our computation.\n",
    "    # In this case the computation is trivial: we just want to flatten\n",
    "    # a Tensor using the flatten function defined above.\n",
    "    \n",
    "    # Our computation will have a single input, x. We don't know its\n",
    "    # value yet, so we define a placeholder which will hold the value\n",
    "    # when the graph is run. We then pass this placeholder Tensor to\n",
    "    # the flatten function; this gives us a new Tensor which will hold\n",
    "    # a flattened view of x when the graph is run. The tf.device\n",
    "    # context manager tells TensorFlow whether to place these Tensors\n",
    "    # on CPU or GPU.\n",
    "    with tf.device(device):\n",
    "        x = tf.placeholder(tf.float32)\n",
    "        x_flat = flatten(x)\n",
    "    \n",
    "    # At this point we have just built the graph describing our computation,\n",
    "    # but we haven't actually computed anything yet. If we print x and x_flat\n",
    "    # we see that they don't hold any data; they are just TensorFlow Tensors\n",
    "    # representing values that will be computed when the graph is run.\n",
    "    print('x: ', type(x), x)\n",
    "    print('x_flat: ', type(x_flat), x_flat)\n",
    "    print()\n",
    "    \n",
    "    # We need to use a TensorFlow Session object to actually run the graph.\n",
    "    with tf.Session() as sess:\n",
    "        # Construct concrete values of the input data x using numpy\n",
    "        x_np = np.arange(24).reshape((2, 3, 4))\n",
    "        print('x_np:\\n', x_np, '\\n')\n",
    "    \n",
    "        # Run our computational graph to compute a concrete output value.\n",
    "        # The first argument to sess.run tells TensorFlow which Tensor\n",
    "        # we want it to compute the value of; the feed_dict specifies\n",
    "        # values to plug into all placeholder nodes in the graph. The\n",
    "        # resulting value of x_flat is returned from sess.run as a\n",
    "        # numpy array.\n",
    "        x_flat_np = sess.run(x_flat, feed_dict={x: x_np})\n",
    "        print('x_flat_np:\\n', x_flat_np, '\\n')\n",
    "\n",
    "        # We can reuse the same graph to perform the same computation\n",
    "        # with different input data\n",
    "        x_np = np.arange(12).reshape((2, 3, 2))\n",
    "        print('x_np:\\n', x_np, '\\n')\n",
    "        x_flat_np = sess.run(x_flat, feed_dict={x: x_np})\n",
    "        print('x_flat_np:\\n', x_flat_np)\n",
    "test_flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Two-Layer Network\n",
    "We will now implement our first neural network with TensorFlow: a fully-connected ReLU network with two hidden layers and no biases on the CIFAR10 dataset. For now we will use only low-level TensorFlow operators to define the network; later we will see how to use the higher-level abstractions provided by `tf.keras` to simplify the process.\n",
    "\n",
    "We will define the forward pass of the network in the function `two_layer_fc`; this will accept TensorFlow Tensors for the inputs and weights of the network, and return a TensorFlow Tensor for the scores. It's important to keep in mind that calling the `two_layer_fc` function **does not** perform any computation; instead it just sets up the computational graph for the forward computation. To actually run the network we need to enter a TensorFlow Session and feed data to the computational graph.\n",
    "\n",
    "After defining the network architecture in the `two_layer_fc` function, we will test the implementation by setting up and running a computational graph, feeding zeros to the network and checking the shape of the output.\n",
    "\n",
    "It's important that you read and understand this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_layer_fc(x, params):\n",
    "    \"\"\"\n",
    "    A fully-connected neural network; the architecture is:\n",
    "    fully-connected layer -> ReLU -> fully connected layer.\n",
    "    Note that we only need to define the forward pass here; TensorFlow will take\n",
    "    care of computing the gradients for us.\n",
    "    \n",
    "    The input to the network will be a minibatch of data, of shape\n",
    "    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,\n",
    "    and the output layer will produce scores for C classes.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A TensorFlow Tensor of shape (N, d1, ..., dM) giving a minibatch of\n",
    "      input data.\n",
    "    - params: A list [w1, w2] of TensorFlow Tensors giving weights for the\n",
    "      network, where w1 has shape (D, H) and w2 has shape (H, C).\n",
    "    \n",
    "    Returns:\n",
    "    - scores: A TensorFlow Tensor of shape (N, C) giving classification scores\n",
    "      for the input data x.\n",
    "    \"\"\"\n",
    "    w1, w2 = params  # Unpack the parameters\n",
    "    x = flatten(x)   # Flatten the input; now x has shape (N, D)\n",
    "    h = tf.nn.relu(tf.matmul(x, w1)) # Hidden layer: h has shape (N, H)\n",
    "    scores = tf.matmul(h, w2)        # Compute scores of shape (N, C)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_test():\n",
    "    # TensorFlow's default computational graph is essentially a hidden global\n",
    "    # variable. To avoid adding to this default graph when you rerun this cell,\n",
    "    # we clear the default graph before constructing the graph we care about.\n",
    "    tf.reset_default_graph()\n",
    "    hidden_layer_size = 42\n",
    "\n",
    "    # Scoping our computational graph setup code under a tf.device context\n",
    "    # manager lets us tell TensorFlow where we want these Tensors to be\n",
    "    # placed.\n",
    "    with tf.device(device):\n",
    "        # Set up a placehoder for the input of the network, and constant\n",
    "        # zero Tensors for the network weights. Here we declare w1 and w2\n",
    "        # using tf.zeros instead of tf.placeholder as we've seen before - this\n",
    "        # means that the values of w1 and w2 will be stored in the computational\n",
    "        # graph itself and will persist across multiple runs of the graph; in\n",
    "        # particular this means that we don't have to pass values for w1 and w2\n",
    "        # using a feed_dict when we eventually run the graph.\n",
    "        x = tf.placeholder(tf.float32)\n",
    "        w1 = tf.zeros((32 * 32 * 3, hidden_layer_size))\n",
    "        w2 = tf.zeros((hidden_layer_size, 10))\n",
    "        \n",
    "        # Call our two_layer_fc function to set up the computational\n",
    "        # graph for the forward pass of the network.\n",
    "        scores = two_layer_fc(x, [w1, w2])\n",
    "    \n",
    "    # Use numpy to create some concrete data that we will pass to the\n",
    "    # computational graph for the x placeholder.\n",
    "    x_np = np.zeros((64, 32, 32, 3))\n",
    "    with tf.Session() as sess:\n",
    "        # The calls to tf.zeros above do not actually instantiate the values\n",
    "        # for w1 and w2; the following line tells TensorFlow to instantiate\n",
    "        # the values of all Tensors (like w1 and w2) that live in the graph.\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Here we actually run the graph, using the feed_dict to pass the\n",
    "        # value to bind to the placeholder for x; we ask TensorFlow to compute\n",
    "        # the value of the scores Tensor, which it returns as a numpy array.\n",
    "        scores_np = sess.run(scores, feed_dict={x: x_np})\n",
    "        print(scores_np.shape)\n",
    "\n",
    "two_layer_fc_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Three-Layer ConvNet\n",
    "Here you will complete the implementation of the function `three_layer_convnet` which will perform the forward pass of a three-layer convolutional network. The network should have the following architecture:\n",
    "\n",
    "1. A convolutional layer (with bias) with `channel_1` filters, each with shape `KW1 x KH1`, and zero-padding of two\n",
    "2. ReLU nonlinearity\n",
    "3. A convolutional layer (with bias) with `channel_2` filters, each with shape `KW2 x KH2`, and zero-padding of one\n",
    "4. ReLU nonlinearity\n",
    "5. Fully-connected layer with bias, producing scores for `C` classes.\n",
    "\n",
    "**HINT**: For convolutions: https://www.tensorflow.org/api_docs/python/tf/nn/conv2d; be careful with padding!\n",
    "\n",
    "**HINT**: For biases: https://www.tensorflow.org/performance/xla/broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def three_layer_convnet(x, params):\n",
    "    \"\"\"\n",
    "    A three-layer convolutional network with the architecture described above.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: A TensorFlow Tensor of shape (N, H, W, 3) giving a minibatch of images\n",
    "    - params: A list of TensorFlow Tensors giving the weights and biases for the\n",
    "      network; should contain the following:\n",
    "      - conv_w1: TensorFlow Tensor of shape (KH1, KW1, 3, channel_1) giving\n",
    "        weights for the first convolutional layer.\n",
    "      - conv_b1: TensorFlow Tensor of shape (channel_1,) giving biases for the\n",
    "        first convolutional layer.\n",
    "      - conv_w2: TensorFlow Tensor of shape (KH2, KW2, channel_1, channel_2)\n",
    "        giving weights for the second convolutional layer\n",
    "      - conv_b2: TensorFlow Tensor of shape (channel_2,) giving biases for the\n",
    "        second convolutional layer.\n",
    "      - fc_w: TensorFlow Tensor giving weights for the fully-connected layer.\n",
    "        Can you figure out what the shape should be?\n",
    "      - fc_b: TensorFlow Tensor giving biases for the fully-connected layer.\n",
    "        Can you figure out what the shape should be?\n",
    "    \"\"\"\n",
    "    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params\n",
    "    scores = None\n",
    "    ############################################################################\n",
    "    # TODO: Implement the forward pass for the three-layer ConvNet.            #\n",
    "    ############################################################################\n",
    "    x_pad = tf.pad(x, paddings = [[0, 0], [2, 2], [2, 2], [0, 0]],\n",
    "                  mode = 'CONSTANT', constant_values = 0)\n",
    "    z1 = tf.nn.conv2d(x_pad, filter = conv_w1, padding = 'VALID',\n",
    "                      strides = [1, 1, 1, 1]) + conv_b1\n",
    "    a1 = tf.nn.relu(z1)\n",
    "    a1_pad = tf.pad(a1, paddings = [[0, 0], [1, 1], [1, 1], [0, 0]],\n",
    "                   mode = 'CONSTANT', constant_values = 0)\n",
    "    z2 = tf.nn.conv2d(a1_pad, filter = conv_w2, padding = 'VALID',\n",
    "                     strides = [1, 1, 1, 1]) + conv_b2\n",
    "    a2 = tf.nn.relu(z2)\n",
    "    a2_flat = tf.reshape(a2, (tf.shape(a2)[0], -1))\n",
    "    scores = tf.matmul(a2_flat, fc_w) + fc_b\n",
    "    ############################################################################\n",
    "    #                              END OF YOUR CODE                            #\n",
    "    ############################################################################\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defing the forward pass of the three-layer ConvNet above, run the following cell to test your implementation. Like the two-layer network, we use the `three_layer_convnet` function to set up the computational graph, then run the graph on a batch of zeros just to make sure the function doesn't crash, and produces outputs of the correct shape.\n",
    "\n",
    "When you run this function, `scores_np` should have shape `(64, 10)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_np has shape:  (64, 10)\n"
     ]
    }
   ],
   "source": [
    "def three_layer_convnet_test():\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    with tf.device(device):\n",
    "        x = tf.placeholder(tf.float32)\n",
    "        conv_w1 = tf.zeros((5, 5, 3, 6))\n",
    "        conv_b1 = tf.zeros((6,))\n",
    "        conv_w2 = tf.zeros((3, 3, 6, 9))\n",
    "        conv_b2 = tf.zeros((9,))\n",
    "        fc_w = tf.zeros((32 * 32 * 9, 10))\n",
    "        fc_b = tf.zeros((10,))\n",
    "        params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "        scores = three_layer_convnet(x, params)\n",
    "\n",
    "    # Inputs to convolutional layers are 4-dimensional arrays with shape\n",
    "    # [batch_size, height, width, channels]\n",
    "    x_np = np.zeros((64, 32, 32, 3))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores, feed_dict={x: x_np})\n",
    "        print('scores_np has shape: ', scores_np.shape)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    three_layer_convnet_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Training Step\n",
    "We now define the `training_step` function which sets up the part of the computational graph that performs a single training step. This will take three basic steps:\n",
    "\n",
    "1. Compute the loss\n",
    "2. Compute the gradient of the loss with respect to all network weights\n",
    "3. Make a weight update step using (stochastic) gradient descent.\n",
    "\n",
    "Note that the step of updating the weights is itself an operation in the computational graph - the calls to `tf.assign_sub` in `training_step` return TensorFlow operations that mutate the weights when they are executed. There is an important bit of subtlety here - when we call `sess.run`, TensorFlow does not execute all operations in the computational graph; it only executes the minimal subset of the graph necessary to compute the outputs that we ask TensorFlow to produce. As a result, naively computing the loss would not cause the weight update operations to execute, since the operations needed to compute the loss do not depend on the output of the weight update. To fix this problem, we insert a **control dependency** into the graph, adding a duplicate `loss` node to the graph that does depend on the outputs of the weight update operations; this is the object that we actually return from the `training_step` function. As a result, asking TensorFlow to evaluate the value of the `loss` returned from `training_step` will also implicitly update the weights of the network using that minibatch of data.\n",
    "\n",
    "We need to use a few new TensorFlow functions to do all of this:\n",
    "- For computing the cross-entropy loss we'll use `tf.nn.sparse_softmax_cross_entropy_with_logits`: https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\n",
    "- For averaging the loss across a minibatch of data we'll use `tf.reduce_mean`:\n",
    "https://www.tensorflow.org/api_docs/python/tf/reduce_mean\n",
    "- For computing gradients of the loss with respect to the weights we'll use `tf.gradients`:  https://www.tensorflow.org/api_docs/python/tf/gradients\n",
    "- We'll mutate the weight values stored in a TensorFlow Tensor using `tf.assign_sub`: https://www.tensorflow.org/api_docs/python/tf/assign_sub\n",
    "- We'll add a control dependency to the graph using `tf.control_dependencies`: https://www.tensorflow.org/api_docs/python/tf/control_dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_step(scores, y, params, learning_rate):\n",
    "    \"\"\"\n",
    "    Set up the part of the computational graph which makes a training step.\n",
    "\n",
    "    Inputs:\n",
    "    - scores: TensorFlow Tensor of shape (N, C) giving classification scores for\n",
    "      the model.\n",
    "    - y: TensorFlow Tensor of shape (N,) giving ground-truth labels for scores;\n",
    "      y[i] == c means that c is the correct class for scores[i].\n",
    "    - params: List of TensorFlow Tensors giving the weights of the model\n",
    "    - learning_rate: Python scalar giving the learning rate to use for gradient\n",
    "      descent step.\n",
    "      \n",
    "    Returns:\n",
    "    - loss: A TensorFlow Tensor of shape () (scalar) giving the loss for this\n",
    "      batch of data; evaluating the loss also performs a gradient descent step\n",
    "      on params (see above).\n",
    "    \"\"\"\n",
    "    # First compute the loss; the first line gives losses for each example in\n",
    "    # the minibatch, and the second averages the losses across the batch\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "\n",
    "    # Compute the gradient of the loss with respect to each parameter of the\n",
    "    # network. This is a very magical function call: TensorFlow internally\n",
    "    # traverses the computational graph starting at loss backward to each element\n",
    "    # of params, and uses backpropagation to figure out how to compute gradients;\n",
    "    # it then adds new operations to the computational graph which compute the\n",
    "    # requested gradients, and returns a list of TensorFlow Tensors that will\n",
    "    # contain the requested gradients when evaluated.\n",
    "    grad_params = tf.gradients(loss, params)\n",
    "    \n",
    "    # Make a gradient descent step on all of the model parameters.\n",
    "    new_weights = []   \n",
    "    for w, grad_w in zip(params, grad_params):\n",
    "        new_w = tf.assign_sub(w, learning_rate * grad_w)\n",
    "        new_weights.append(new_w)\n",
    "\n",
    "    # Insert a control dependency so that evaluating the loss causes a weight\n",
    "    # update to happen; see the discussion above.\n",
    "    with tf.control_dependencies(new_weights):\n",
    "        return tf.identity(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Training Loop\n",
    "Now we set up a basic training loop using low-level TensorFlow operations. We will train the model using stochastic gradient descent without momentum. The `training_step` function sets up the part of the computational graph that performs the training step, and the function `train_part2` iterates through the training data, making training steps on each minibatch, and periodically evaluates accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_part2(model_fn, init_fn, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_fn: A Python function that performs the forward pass of the model\n",
    "      using TensorFlow; it should have the following signature:\n",
    "      scores = model_fn(x, params) where x is a TensorFlow Tensor giving a\n",
    "      minibatch of image data, params is a list of TensorFlow Tensors holding\n",
    "      the model weights, and scores is a TensorFlow Tensor of shape (N, C)\n",
    "      giving scores for all elements of x.\n",
    "    - init_fn: A Python function that initializes the parameters of the model.\n",
    "      It should have the signature params = init_fn() where params is a list\n",
    "      of TensorFlow Tensors holding the (randomly initialized) weights of the\n",
    "      model.\n",
    "    - learning_rate: Python float giving the learning rate to use for SGD.\n",
    "    \"\"\"\n",
    "    # First clear the default graph\n",
    "    tf.reset_default_graph()\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    # Set up the computational graph for performing forward and backward passes,\n",
    "    # and weight updates.\n",
    "    with tf.device(device):\n",
    "        # Set up placeholders for the data and labels\n",
    "        x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        params = init_fn()           # Initialize the model parameters\n",
    "        scores = model_fn(x, params) # Forward pass of the model\n",
    "        loss = training_step(scores, y, params, learning_rate)\n",
    "\n",
    "    # Now we actually run the graph many times using the training data\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize variables that will live in the graph\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for t, (x_np, y_np) in enumerate(train_dset):\n",
    "            # Run the graph on a batch of training data; recall that asking\n",
    "            # TensorFlow to evaluate loss will cause an SGD step to happen.\n",
    "            feed_dict = {x: x_np, y: y_np}\n",
    "            loss_np = sess.run(loss, feed_dict=feed_dict)\n",
    "            \n",
    "            # Periodically print the loss and check accuracy on the val set\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss_np))\n",
    "                check_accuracy(sess, val_dset, x, scores, is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Check Accuracy\n",
    "When training the model we will use the following function to check the accuracy of our model on the training or validation sets. Note that this function accepts a TensorFlow Session object as one of its arguments; this is needed since the function must actually run the computational graph many times on the data that it loads from the dataset `dset`.\n",
    "\n",
    "Also note that we reuse the same computational graph both for taking training steps and for evaluating the model; however since the `check_accuracy` function never evalutes the `loss` value in the computational graph, the part of the graph that updates the weights of the graph do not execute on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_accuracy(sess, dset, x, scores, is_training=None):\n",
    "    \"\"\"\n",
    "    Check accuracy on a classification model.\n",
    "    \n",
    "    Inputs:\n",
    "    - sess: A TensorFlow Session that will be used to run the graph\n",
    "    - dset: A Dataset object on which to check accuracy\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - scores: A TensorFlow Tensor representing the scores output from the\n",
    "      model; this is the Tensor we will ask TensorFlow to evaluate.\n",
    "      \n",
    "    Returns: Nothing, but prints the accuracy of the model\n",
    "    \"\"\"\n",
    "    num_correct, num_samples = 0, 0\n",
    "    for x_batch, y_batch in dset:\n",
    "        feed_dict = {x: x_batch, is_training: 0}\n",
    "        scores_np = sess.run(scores, feed_dict=feed_dict)\n",
    "        y_pred = scores_np.argmax(axis=1)\n",
    "        num_samples += x_batch.shape[0]\n",
    "        num_correct += (y_pred == y_batch).sum()\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Initialization\n",
    "We'll use the following utility method to initialize the weight matrices for our models using Kaiming's normalization method.\n",
    "\n",
    "[1] He et al, *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n",
    "*, ICCV 2015, https://arxiv.org/abs/1502.01852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kaiming_normal(shape):\n",
    "    if len(shape) == 2:\n",
    "        fan_in, fan_out = shape[0], shape[1]\n",
    "    elif len(shape) == 4:\n",
    "        fan_in, fan_out = np.prod(shape[:3]), shape[3]\n",
    "    return tf.random_normal(shape) * np.sqrt(2.0 / fan_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Train a Two-Layer Network\n",
    "We are finally ready to use all of the pieces defined above to train a two-layer fully-connected network on CIFAR-10.\n",
    "\n",
    "We just need to define a function to initialize the weights of the model, and call `train_part2`.\n",
    "\n",
    "Defining the weights of the network introduces another important piece of TensorFlow API: `tf.Variable`. A TensorFlow Variable is a Tensor whose value is stored in the graph and persists across runs of the computational graph; however unlike constants defined with `tf.zeros` or `tf.random_normal`, the values of a Variable can be mutated as the graph runs; these mutations will persist across graph runs. Learnable parameters of the network are usually stored in Variables.\n",
    "\n",
    "You don't need to tune any hyperparameters, but you should achieve accuracies above 40% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.8887\n",
      "Got 123 / 1000 correct (12.30%)\n",
      "Iteration 100, loss = 1.8260\n",
      "Got 370 / 1000 correct (37.00%)\n",
      "Iteration 200, loss = 1.4732\n",
      "Got 405 / 1000 correct (40.50%)\n",
      "Iteration 300, loss = 1.8455\n",
      "Got 357 / 1000 correct (35.70%)\n",
      "Iteration 400, loss = 1.7811\n",
      "Got 431 / 1000 correct (43.10%)\n",
      "Iteration 500, loss = 1.8252\n",
      "Got 438 / 1000 correct (43.80%)\n",
      "Iteration 600, loss = 1.9769\n",
      "Got 411 / 1000 correct (41.10%)\n",
      "Iteration 700, loss = 1.9931\n",
      "Got 436 / 1000 correct (43.60%)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_init():\n",
    "    \"\"\"\n",
    "    Initialize the weights of a two-layer network, for use with the\n",
    "    two_layer_network function defined above.\n",
    "    \n",
    "    Inputs: None\n",
    "    \n",
    "    Returns: A list of:\n",
    "    - w1: TensorFlow Variable giving the weights for the first layer\n",
    "    - w2: TensorFlow Variable giving the weights for the second layer\n",
    "    \"\"\"\n",
    "    hidden_layer_size = 4000\n",
    "    w1 = tf.Variable(kaiming_normal((3 * 32 * 32, 4000)))\n",
    "    w2 = tf.Variable(kaiming_normal((4000, 10)))\n",
    "    return [w1, w2]\n",
    "\n",
    "learning_rate = 1e-2\n",
    "train_part2(two_layer_fc, two_layer_fc_init, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Train a three-layer ConvNet\n",
    "We will now use TensorFlow to train a three-layer ConvNet on CIFAR-10.\n",
    "\n",
    "You need to implement the `three_layer_convnet_init` function. Recall that the architecture of the network is:\n",
    "\n",
    "1. Convolutional layer (with bias) with 32 5x5 filters, with zero-padding 2\n",
    "2. ReLU\n",
    "3. Convolutional layer (with bias) with 16 3x3 filters, with zero-padding 1\n",
    "4. ReLU\n",
    "5. Fully-connected layer (with bias) to compute scores for 10 classes\n",
    "\n",
    "You don't need to do any hyperparameter tuning, but you should see accuracies above 43% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.8141\n",
      "Got 128 / 1000 correct (12.80%)\n",
      "Iteration 100, loss = 1.7539\n",
      "Got 360 / 1000 correct (36.00%)\n",
      "Iteration 200, loss = 1.5105\n",
      "Got 415 / 1000 correct (41.50%)\n",
      "Iteration 300, loss = 1.5913\n",
      "Got 416 / 1000 correct (41.60%)\n",
      "Iteration 400, loss = 1.6683\n",
      "Got 447 / 1000 correct (44.70%)\n",
      "Iteration 500, loss = 1.6382\n",
      "Got 444 / 1000 correct (44.40%)\n",
      "Iteration 600, loss = 1.6323\n",
      "Got 473 / 1000 correct (47.30%)\n",
      "Iteration 700, loss = 1.5959\n",
      "Got 481 / 1000 correct (48.10%)\n"
     ]
    }
   ],
   "source": [
    "def three_layer_convnet_init():\n",
    "    \"\"\"\n",
    "    Initialize the weights of a Three-Layer ConvNet, for use with the\n",
    "    three_layer_convnet function defined above.\n",
    "    \n",
    "    Inputs: None\n",
    "    \n",
    "    Returns a list containing:\n",
    "    - conv_w1: TensorFlow Variable giving weights for the first conv layer\n",
    "    - conv_b1: TensorFlow Variable giving biases for the first conv layer\n",
    "    - conv_w2: TensorFlow Variable giving weights for the second conv layer\n",
    "    - conv_b2: TensorFlow Variable giving biases for the second conv layer\n",
    "    - fc_w: TensorFlow Variable giving weights for the fully-connected layer\n",
    "    - fc_b: TensorFlow Variable giving biases for the fully-connected layer\n",
    "    \"\"\"\n",
    "    params = None\n",
    "    ############################################################################\n",
    "    # TODO: Initialize the parameters of the three-layer network.              #\n",
    "    ############################################################################\n",
    "    conv_w1 = tf.Variable(kaiming_normal((5, 5, 3, 32))) # 5x5 filters, 3 channels, 32 filters\n",
    "    conv_b1 = tf.Variable(tf.zeros((32,)))\n",
    "    conv_w2 = tf.Variable(kaiming_normal((3, 3, 32, 16))) # 3x3 filters, 32 channels, 16 filters\n",
    "    conv_b2 = tf.Variable(tf.zeros((16,)))\n",
    "    fc_w = tf.Variable(kaiming_normal((32*32*16, 10)))\n",
    "    fc_b = tf.Variable(tf.zeros((10,)))\n",
    "    \n",
    "    params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "    return params\n",
    "\n",
    "learning_rate = 3e-3\n",
    "train_part2(three_layer_convnet, three_layer_convnet_init, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Keras Model API\n",
    "Implementing a neural network using the low-level TensorFlow API is a good way to understand how TensorFlow works, but it's a little inconvenient - we had to manually keep track of all Tensors holding learnable parameters, and we had to use a control dependency to implement the gradient descent update step. This was fine for a small network, but could quickly become unwieldy for a large complex model.\n",
    "\n",
    "Fortunately TensorFlow provides higher-level packages such as `tf.keras` and `tf.layers` which make it easy to build models out of modular, object-oriented layers; `tf.train` allows you to easily train these models using a variety of different optimization algorithms.\n",
    "\n",
    "In this part of the notebook we will define neural network models using the `tf.keras.Model` API. To implement your own model, you need to do the following:\n",
    "\n",
    "1. Define a new class which subclasses `tf.keras.model`. Give your class an intuitive name that describes it, like `TwoLayerFC` or `ThreeLayerConvNet`.\n",
    "2. In the initializer `__init__()` for your new class, define all the layers you need as class attributes. The `tf.layers` package provides many common neural-network layers, like `tf.layers.Dense` for fully-connected layers and `tf.layers.Conv2D` for convolutional layers. Under the hood, these layers will construct `Variable` Tensors for any learnable parameters. **Warning**: Don't forget to call `super().__init__()` as the first line in your initializer!\n",
    "3. Implement the `call()` method for your class; this implements the forward pass of your model, and defines the *connectivity* of your network. Layers defined in `__init__()` implement `__call__()` so they can be used as function objects that transform input Tensors into output Tensors. Don't define any new layers in `call()`; any layers you want to use in the forward pass should be defined in `__init__()`.\n",
    "\n",
    "After you define your `tf.keras.Model` subclass, you can instantiate it and use it like the model functions from Part II.\n",
    "\n",
    "### Module API: Two-Layer Network\n",
    "\n",
    "Here is a concrete example of using the `tf.keras.Model` API to define a two-layer network. There are a few new bits of API to be aware of here:\n",
    "\n",
    "We use an `Initializer` object to set up the initial values of the learnable parameters of the layers; in particular `tf.variance_scaling_initializer` gives behavior similar to the Kaiming initialization method we used in Part II. You can read more about it here: https://www.tensorflow.org/api_docs/python/tf/variance_scaling_initializer\n",
    "\n",
    "We construct `tf.layers.Dense` objects to represent the two fully-connected layers of the model. In addition to multiplying their input by a weight matrix and adding a bias vector, these layer can also apply a nonlinearity for you. For the first layer we specify a ReLU activation function by passing `activation=tf.nn.relu` to the constructor; the second layer does not apply any activation function.\n",
    "\n",
    "Unfortunately the `flatten` function we defined in Part II is not compatible with the `tf.keras.Model` API; fortunately we can use `tf.layers.flatten` to perform the same operation. The issue with our `flatten` function from Part II has to do with static vs dynamic shapes for Tensors, which is beyond the scope of this notebook; you can read more about the distinction [in the documentation](https://www.tensorflow.org/programmers_guide/faq#tensor_shapes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerFC(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super().__init__()        \n",
    "        initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "        self.fc1 = tf.layers.Dense(hidden_size, activation=tf.nn.relu,\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.layers.Dense(num_classes,\n",
    "                                   kernel_initializer=initializer)\n",
    "    def call(self, x, training=None):\n",
    "        x = tf.layers.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "    # As usual in TensorFlow, we first need to define our computational graph.\n",
    "    # To this end we first construct a TwoLayerFC object, then use it to construct\n",
    "    # the scores Tensor.\n",
    "    model = TwoLayerFC(hidden_size, num_classes)\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, input_size))\n",
    "        scores = model(x)\n",
    "\n",
    "    # Now that our computational graph has been defined we can run the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores)\n",
    "        print(scores_np.shape)\n",
    "        \n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Funtional API: Two-Layer Network\n",
    "The `tf.layers` package provides two different higher-level APIs for defining neural network models. In the example above we used the **object-oriented API**, where each layer of the neural network is represented as a Python object (like `tf.layers.Dense`). Here we showcase the **functional API**, where each layer is a Python function (like `tf.layers.dense`) which inputs and outputs TensorFlow Tensors, and which internally sets up Tensors in the computational graph to hold any learnable weights.\n",
    "\n",
    "To construct a network, one needs to pass the input tensor to the first layer, and construct the subsequent layers sequentially. Here's an example of how to construct the same two-layer network with the functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_functional(inputs, hidden_size, num_classes):     \n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    flattened_inputs = tf.layers.flatten(inputs)\n",
    "    fc1_output = tf.layers.dense(flattened_inputs, hidden_size, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "    scores = tf.layers.dense(fc1_output, num_classes,\n",
    "                             kernel_initializer=initializer)\n",
    "    return scores\n",
    "\n",
    "def test_two_layer_fc_functional():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "    # As usual in TensorFlow, we first need to define our computational graph.\n",
    "    # To this end we first construct a two layer network graph by calling the\n",
    "    # two_layer_network() function. This function constructs the computation\n",
    "    # graph and outputs the score tensor.\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, input_size))\n",
    "        scores = two_layer_fc_functional(x, hidden_size, num_classes)\n",
    "\n",
    "    # Now that our computational graph has been defined we can run the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores)\n",
    "        print(scores_np.shape)\n",
    "        \n",
    "test_two_layer_fc_functional()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model API: Three-Layer ConvNet\n",
    "Now it's your turn to implement a three-layer ConvNet using the `tf.keras.Model` API. Your model should have the same architecture used in Part II:\n",
    "\n",
    "1. Convolutional layer with 5 x 5 kernels, with zero-padding of 2\n",
    "2. ReLU nonlinearity\n",
    "3. Convolutional layer with 3 x 3 kernels, with zero-padding of 1\n",
    "4. ReLU nonlinearity\n",
    "5. Fully-connected layer to give class scores\n",
    "\n",
    "You should initialize the weights of your network using the same initialization method as was used in the two-layer network above.\n",
    "\n",
    "**Hint**: Refer to the documentation for `tf.layers.Conv2D` and `tf.layers.Dense`:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/layers/Conv2D\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/layers/Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #\n",
    "        # should instantiate layer objects to be used in the forward pass.     #\n",
    "        ########################################################################\n",
    "        initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "        self.conv1 = tf.layers.Conv2D(filters=channel_1, kernel_size=(5, 5),\n",
    "                                      strides=(1, 1), padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      activation=tf.nn.relu)\n",
    "        self.conv2 = tf.layers.Conv2D(filters=channel_2, kernel_size=(3, 3),\n",
    "                                      strides=(1, 1), padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      activation=tf.nn.relu)\n",
    "        self.fc = tf.layers.Dense(num_classes, kernel_initializer=initializer)\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward pass for a three-layer ConvNet. You      #\n",
    "        # should use the layer objects defined in the __init__ method.         #\n",
    "        ########################################################################\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.layers.flatten(x)\n",
    "        scores = self.fc(x)\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete the implementation of the `ThreeLayerConvNet` above you can run the following to ensure that your implementation does not crash and produces outputs of the expected shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def test_ThreeLayerConvNet():\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    channel_1, channel_2, num_classes = 12, 8, 10\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 3, 32, 32))\n",
    "        scores = model(x)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores)\n",
    "        print(scores_np.shape)\n",
    "\n",
    "test_ThreeLayerConvNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model API: Training Loop\n",
    "We need to implement a slightly different training loop when using the `tf.keras.Model` API. Instead of computing gradients and updating the weights of the model manually, we use an `Optimizer` object from the `tf.train` package which takes care of these details for us. You can read more about `Optimizer`s here: https://www.tensorflow.org/api_docs/python/tf/train/Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during trainingn\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()    \n",
    "    with tf.device(device):\n",
    "        # Construct the computational graph we will use to train the model. We\n",
    "        # use the model_init_fn to construct the model, declare placeholders for\n",
    "        # the data and labels\n",
    "        x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # We need a place holder to explicitly specify if the model is in the training\n",
    "        # phase or not. This is because a number of layers behaves differently in\n",
    "        # training and in testing, e.g., dropout and batch normalization.\n",
    "        # We pass this variable to the computation graph through feed_dict as shown below.\n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        \n",
    "        # Use the model function to build the forward pass.\n",
    "        scores = model_init_fn(x, is_training)\n",
    "\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        # Use the optimizer_fn to construct an Optimizer, then use the optimizer\n",
    "        # to set up the training step. Asking TensorFlow to evaluate the\n",
    "        # train_op returned by optimizer.minimize(loss) will cause us to make a\n",
    "        # single update step using the current minibatch of data.\n",
    "        \n",
    "        # Note that we use tf.control_dependencies to force the model to run\n",
    "        # the tf.GraphKeys.UPDATE_OPS at each training step. tf.GraphKeys.UPDATE_OPS\n",
    "        # holds the operators that update the states of the network.\n",
    "        # For example, the tf.layers.batch_normalization function adds the running mean\n",
    "        # and variance update operators to tf.GraphKeys.UPDATE_OPS.\n",
    "        optimizer = optimizer_init_fn()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.minimize(loss)\n",
    "\n",
    "    # Now we can run the computational graph many times to train the model.\n",
    "    # When we call sess.run we ask it to evaluate train_op, which causes the\n",
    "    # model to update.\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Starting epoch %d' % epoch)\n",
    "            for x_np, y_np in train_dset:\n",
    "                feed_dict = {x: x_np, y: y_np, is_training:1}\n",
    "                loss_np, _ = sess.run([loss, train_op], feed_dict=feed_dict)\n",
    "                if t % print_every == 0:\n",
    "                    print('Iteration %d, loss = %.4f' % (t, loss_np))\n",
    "                    check_accuracy(sess, val_dset, x, scores, is_training=is_training)\n",
    "                    print()\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model API: Train a Two-Layer Network\n",
    "We can now use the tools defined above to train a two-layer network on CIFAR-10. We define the `model_init_fn` and `optimizer_init_fn` that construct the model and optimizer respectively when called. Here we want to train the model using stochastic gradient descent with no momentum, so we construct a `tf.train.GradientDescentOptimizer` function; you can [read about it here](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer).\n",
    "\n",
    "You don't need to tune any hyperparameters here, but you should achieve accuracies above 40% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.9137\n",
      "Got 124 / 1000 correct (12.40%)\n",
      "\n",
      "Iteration 100, loss = 1.8680\n",
      "Got 365 / 1000 correct (36.50%)\n",
      "\n",
      "Iteration 200, loss = 1.3291\n",
      "Got 405 / 1000 correct (40.50%)\n",
      "\n",
      "Iteration 300, loss = 1.6878\n",
      "Got 389 / 1000 correct (38.90%)\n",
      "\n",
      "Iteration 400, loss = 1.7458\n",
      "Got 421 / 1000 correct (42.10%)\n",
      "\n",
      "Iteration 500, loss = 1.6872\n",
      "Got 435 / 1000 correct (43.50%)\n",
      "\n",
      "Iteration 600, loss = 1.8543\n",
      "Got 442 / 1000 correct (44.20%)\n",
      "\n",
      "Iteration 700, loss = 1.8219\n",
      "Got 450 / 1000 correct (45.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return TwoLayerFC(hidden_size, num_classes)(inputs)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model API: Train a Two-Layer Network (functional API)\n",
    "Similarly, we train the two-layer network constructed using the functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.9877\n",
      "Got 127 / 1000 correct (12.70%)\n",
      "\n",
      "Iteration 100, loss = 1.8301\n",
      "Got 378 / 1000 correct (37.80%)\n",
      "\n",
      "Iteration 200, loss = 1.4451\n",
      "Got 405 / 1000 correct (40.50%)\n",
      "\n",
      "Iteration 300, loss = 1.7302\n",
      "Got 374 / 1000 correct (37.40%)\n",
      "\n",
      "Iteration 400, loss = 1.7147\n",
      "Got 414 / 1000 correct (41.40%)\n",
      "\n",
      "Iteration 500, loss = 1.7207\n",
      "Got 430 / 1000 correct (43.00%)\n",
      "\n",
      "Iteration 600, loss = 1.9221\n",
      "Got 425 / 1000 correct (42.50%)\n",
      "\n",
      "Iteration 700, loss = 1.8113\n",
      "Got 442 / 1000 correct (44.20%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return two_layer_fc_functional(inputs, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model API: Train a Three-Layer ConvNet\n",
    "Here you should use the tools we've defined above to train a three-layer ConvNet on CIFAR-10. Your ConvNet should use 32 filters in the first convolutional layer and 16 filters in the second layer.\n",
    "\n",
    "To train the model you should use gradient descent with Nesterov momentum 0.9. \n",
    "\n",
    "**HINT**: https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer\n",
    "\n",
    "You don't need to perform any hyperparameter tuning, but you should achieve accuracies above 45% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.9271\n",
      "Got 84 / 1000 correct (8.40%)\n",
      "\n",
      "Iteration 100, loss = 1.6797\n",
      "Got 360 / 1000 correct (36.00%)\n",
      "\n",
      "Iteration 200, loss = 1.6216\n",
      "Got 444 / 1000 correct (44.40%)\n",
      "\n",
      "Iteration 300, loss = 1.4693\n",
      "Got 463 / 1000 correct (46.30%)\n",
      "\n",
      "Iteration 400, loss = 1.3619\n",
      "Got 498 / 1000 correct (49.80%)\n",
      "\n",
      "Iteration 500, loss = 1.5431\n",
      "Got 508 / 1000 correct (50.80%)\n",
      "\n",
      "Iteration 600, loss = 1.4996\n",
      "Got 524 / 1000 correct (52.40%)\n",
      "\n",
      "Iteration 700, loss = 1.3839\n",
      "Got 519 / 1000 correct (51.90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 10\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes) # object-oriented API\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return model(inputs) # executes call function with inputs\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV: Keras Sequential API\n",
    "In Part III we introduced the `tf.keras.Model` API, which allows you to define models with any number of learnable layers and with arbitrary connectivity between layers.\n",
    "\n",
    "However for many models you don't need such flexibility - a lot of models can be expressed as a sequential stack of layers, with the output of each layer fed to the next layer as input. If your model fits this pattern, then there is an even easier way to define your model: using `tf.keras.Sequential`. You don't need to write any custom classes; you simply call the `tf.keras.Sequential` constructor with a list containing a sequence of layer objects.\n",
    "\n",
    "One complication with `tf.keras.Sequential` is that you must define the shape of the input to the model by passing a value to the `input_shape` of the first layer in your model.\n",
    "\n",
    "### Keras Sequential API: Two-Layer Network\n",
    "Here we rewrite the two-layer fully-connected network using `tf.keras.Sequential`, and train it using the training loop defined above.\n",
    "\n",
    "You don't need to perform any hyperparameter tuning here, but you should see accuracies above 40% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.6311\n",
      "Got 114 / 1000 correct (11.40%)\n",
      "\n",
      "Iteration 100, loss = 1.8398\n",
      "Got 387 / 1000 correct (38.70%)\n",
      "\n",
      "Iteration 200, loss = 1.3627\n",
      "Got 406 / 1000 correct (40.60%)\n",
      "\n",
      "Iteration 300, loss = 1.7587\n",
      "Got 382 / 1000 correct (38.20%)\n",
      "\n",
      "Iteration 400, loss = 1.8052\n",
      "Got 426 / 1000 correct (42.60%)\n",
      "\n",
      "Iteration 500, loss = 1.7611\n",
      "Got 430 / 1000 correct (43.00%)\n",
      "\n",
      "Iteration 600, loss = 1.8045\n",
      "Got 423 / 1000 correct (42.30%)\n",
      "\n",
      "Iteration 700, loss = 1.8305\n",
      "Got 447 / 1000 correct (44.70%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    input_shape = (32, 32, 3)\n",
    "    hidden_layer_size, num_classes = 4000, 10\n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    layers = [\n",
    "        tf.layers.Flatten(input_shape=input_shape),\n",
    "        tf.layers.Dense(hidden_layer_size, activation=tf.nn.relu,\n",
    "                        kernel_initializer=initializer),\n",
    "        tf.layers.Dense(num_classes, kernel_initializer=initializer),\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model(inputs)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Sequential API: Three-Layer ConvNet\n",
    "Here you should use `tf.keras.Sequential` to reimplement the same three-layer ConvNet architecture used in Part II and Part III. As a reminder, your model should have the following architecture:\n",
    "\n",
    "1. Convolutional layer with 16 5x5 kernels, using zero padding of 2\n",
    "2. ReLU nonlinearity\n",
    "3. Convolutional layer with 32 3x3 kernels, using zero padding of 1\n",
    "4. ReLU nonlinearity\n",
    "5. Fully-connected layer giving class scores\n",
    "\n",
    "You should initialize the weights of the model using a `tf.variance_scaling_initializer` as above.\n",
    "\n",
    "You should train the model using Nesterov momentum 0.9.\n",
    "\n",
    "You don't need to perform any hyperparameter search, but you should achieve accuracy above 45% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.3578\n",
      "Got 107 / 1000 correct (10.70%)\n",
      "\n",
      "Iteration 100, loss = 1.7984\n",
      "Got 397 / 1000 correct (39.70%)\n",
      "\n",
      "Iteration 200, loss = 1.4684\n",
      "Got 465 / 1000 correct (46.50%)\n",
      "\n",
      "Iteration 300, loss = 1.4892\n",
      "Got 461 / 1000 correct (46.10%)\n",
      "\n",
      "Iteration 400, loss = 1.5059\n",
      "Got 480 / 1000 correct (48.00%)\n",
      "\n",
      "Iteration 500, loss = 1.6553\n",
      "Got 491 / 1000 correct (49.10%)\n",
      "\n",
      "Iteration 600, loss = 1.5298\n",
      "Got 495 / 1000 correct (49.50%)\n",
      "\n",
      "Iteration 700, loss = 1.4715\n",
      "Got 503 / 1000 correct (50.30%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a three-layer ConvNet using tf.keras.Sequential.         #\n",
    "    ############################################################################\n",
    "    input_shape = (32, 32, 3) # 32x32 RGB image (3 channels)\n",
    "    channel_1, channel_2, num_classes = 16, 32, 10\n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    \n",
    "    layers = [\n",
    "        # Need to pass input_shape to first layer of model\n",
    "        tf.layers.Conv2D(channel_1, input_shape=input_shape, kernel_size=(5,5),\n",
    "                         strides=(1,1), padding='same', activation=tf.nn.relu,\n",
    "                         kernel_initializer=initializer),\n",
    "        tf.layers.Conv2D(channel_2, kernel_size=(3,3), strides=(1,1),\n",
    "                         padding='same', activation=tf.nn.relu,\n",
    "                         kernel_initializer=initializer),\n",
    "        tf.layers.Flatten(),\n",
    "        tf.layers.Dense(num_classes, kernel_initializer=initializer)\n",
    "    ]\n",
    "    \n",
    "    model = tf.keras.Sequential(layers)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return model(inputs)\n",
    "\n",
    "learning_rate = 5e-4\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V: CIFAR-10 open-ended challenge\n",
    "\n",
    "In this section you can experiment with whatever ConvNet architecture you'd like on CIFAR-10.\n",
    "\n",
    "You should experiment with architectures, hyperparameters, loss functions, regularization, or anything else you can think of to train a model that achieves **at least 70%** accuracy on the **validation** set within 10 epochs. You can use the `check_accuracy` and `train` functions from above, or you can implement your own training loop.\n",
    "\n",
    "Describe what you did at the end of the notebook.\n",
    "\n",
    "### Some things you can try:\n",
    "- **Filter size**: Above we used 5x5 and 3x3; is this optimal?\n",
    "- **Number of filters**: Above we used 16 and 32 filters. Would more or fewer do better?\n",
    "- **Pooling**: We didn't use any pooling above. Would this improve the model?\n",
    "- **Normalization**: Would your model be improved with batch normalization, layer normalization, group normalization, or some other normalization strategy?\n",
    "- **Network architecture**: The ConvNet above has only three layers of trainable parameters. Would a deeper model do better?\n",
    "- **Global average pooling**: Instead of flattening after the final convolutional layer, would global average pooling do better? This strategy is used for example in Google's Inception network and in Residual Networks.\n",
    "- **Regularization**: Would some kind of regularization improve performance? Maybe weight decay or dropout?\n",
    "\n",
    "### WARNING: Batch Normalization / Dropout\n",
    "Batch Normalization and Dropout **WILL NOT WORK CORRECTLY** if you use the `train_part34()` function with the object-oriented `tf.keras.Model` or `tf.keras.Sequential` APIs; if you want to use these layers with this training loop then you **must use the tf.layers functional API**.\n",
    "\n",
    "We wrote `train_part34()` to explicitly demonstrate how TensorFlow works; however there are some subtleties that make it tough to handle the object-oriented batch normalization layer in a simple training loop. In practice both `tf.keras` and `tf` provide higher-level APIs which handle the training loop for you, such as [keras.fit](https://keras.io/models/sequential/) and [tf.Estimator](https://www.tensorflow.org/programmers_guide/estimators), both of which will properly handle batch normalization when using the object-oriented API.\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and other hyperparameters. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these, but don't miss the fun if you have time!\n",
    "\n",
    "- Alternative optimizers: you can try Adam, Adagrad, RMSprop, etc.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "  \n",
    "### Have fun and happy training! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use a small portion of training set to check training accuracy\n",
    "train_dset2 = Dataset(X_train[:1000], y_train[:1000], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model_init_fn, optimizer_init_fn, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during training\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()    \n",
    "    with tf.device(device):\n",
    "        # Construct the computational graph we will use to train the model. We\n",
    "        # use the model_init_fn to construct the model, declare placeholders for\n",
    "        # the data and labels\n",
    "        x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # We need a place holder to explicitly specify if the model is in the training\n",
    "        # phase or not. This is because a number of layers behaves differently in\n",
    "        # training and in testing, e.g., dropout and batch normalization.\n",
    "        # We pass this variable to the computation graph through feed_dict as shown below.\n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        \n",
    "        # Use the model function to build the forward pass.\n",
    "        scores = model_init_fn(x, is_training)\n",
    "\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        # Use the optimizer_fn to construct an Optimizer, then use the optimizer\n",
    "        # to set up the training step. Asking TensorFlow to evaluate the\n",
    "        # train_op returned by optimizer.minimize(loss) will cause us to make a\n",
    "        # single update step using the current minibatch of data.\n",
    "        \n",
    "        # Note that we use tf.control_dependencies to force the model to run\n",
    "        # the tf.GraphKeys.UPDATE_OPS at each training step. tf.GraphKeys.UPDATE_OPS\n",
    "        # holds the operators that update the states of the network.\n",
    "        # For example, the tf.layers.batch_normalization function adds the running mean\n",
    "        # and variance update operators to tf.GraphKeys.UPDATE_OPS.\n",
    "        optimizer = optimizer_init_fn()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.minimize(loss)\n",
    "\n",
    "    # Now we can run the computational graph many times to train the model.\n",
    "    # When we call sess.run we ask it to evaluate train_op, which causes the\n",
    "    # model to update.\n",
    "    \n",
    "    losses = []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Starting epoch %d' % epoch)\n",
    "            for x_np, y_np in train_dset:\n",
    "                feed_dict = {x: x_np, y: y_np, is_training:1}\n",
    "                loss_np, _ = sess.run([loss, train_op], feed_dict=feed_dict)\n",
    "                losses.append((t, loss_np))\n",
    "                if t % print_every == 0:\n",
    "                    print('Iteration %d, loss = %.4f' % (t, loss_np))\n",
    "                    report_accuracy(sess, train_dset2, x, scores, is_training=is_training)\n",
    "                    acc = report_accuracy(sess, val_dset, x, scores, is_training=is_training)\n",
    "                    val_acc_history.append((t, acc))\n",
    "                    print()\n",
    "                t += 1\n",
    "        \n",
    "        # Report test accuracy on the final model after training\n",
    "        print('Accuracy of final model on the test set:')\n",
    "        report_accuracy(sess, test_dset, x, scores, is_training=is_training)\n",
    "    # Plot validation accuracies vs. iteration\n",
    "    #plt.plot(iteration, val_acc_history)\n",
    "    #plt.xlabel('Iteration')\n",
    "    #plt.ylabel('Validation accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_accuracy(sess, dset, x, scores, is_training=None):\n",
    "    \"\"\"\n",
    "    Check accuracy on a classification model.\n",
    "    \n",
    "    Inputs:\n",
    "    - sess: A TensorFlow Session that will be used to run the graph\n",
    "    - dset: A Dataset object on which to check accuracy\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - scores: A TensorFlow Tensor representing the scores output from the\n",
    "      model; this is the Tensor we will ask TensorFlow to evaluate.\n",
    "      \n",
    "    Returns: Nothing, but prints the accuracy of the model\n",
    "    - acc: the accuracy of the model\n",
    "    \"\"\"\n",
    "    num_correct, num_samples = 0, 0\n",
    "    for x_batch, y_batch in dset:\n",
    "        feed_dict = {x: x_batch, is_training: 0}\n",
    "        scores_np = sess.run(scores, feed_dict=feed_dict)\n",
    "        y_pred = scores_np.argmax(axis=1)\n",
    "        num_samples += x_batch.shape[0]\n",
    "        num_correct += (y_pred == y_batch).sum()\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.5971\n",
      "Got 109 / 1000 correct (10.90%)\n",
      "\n",
      "Iteration 100, loss = 1.7447\n",
      "Got 422 / 1000 correct (42.20%)\n",
      "\n",
      "Iteration 200, loss = 1.4628\n",
      "Got 465 / 1000 correct (46.50%)\n",
      "\n",
      "Iteration 300, loss = 1.5040\n",
      "Got 481 / 1000 correct (48.10%)\n",
      "\n",
      "Iteration 400, loss = 1.4576\n",
      "Got 485 / 1000 correct (48.50%)\n",
      "\n",
      "Iteration 500, loss = 1.5234\n",
      "Got 498 / 1000 correct (49.80%)\n",
      "\n",
      "Iteration 600, loss = 1.6411\n",
      "Got 529 / 1000 correct (52.90%)\n",
      "\n",
      "Iteration 700, loss = 1.3984\n",
      "Got 536 / 1000 correct (53.60%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 1.2365\n",
      "Got 540 / 1000 correct (54.00%)\n",
      "\n",
      "Iteration 900, loss = 1.1983\n",
      "Got 543 / 1000 correct (54.30%)\n",
      "\n",
      "Iteration 1000, loss = 1.3071\n",
      "Got 549 / 1000 correct (54.90%)\n",
      "\n",
      "Iteration 1100, loss = 1.4036\n",
      "Got 553 / 1000 correct (55.30%)\n",
      "\n",
      "Iteration 1200, loss = 1.0782\n",
      "Got 562 / 1000 correct (56.20%)\n",
      "\n",
      "Iteration 1300, loss = 1.1436\n",
      "Got 557 / 1000 correct (55.70%)\n",
      "\n",
      "Iteration 1400, loss = 1.2784\n",
      "Got 565 / 1000 correct (56.50%)\n",
      "\n",
      "Iteration 1500, loss = 1.2236\n",
      "Got 562 / 1000 correct (56.20%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 0.9346\n",
      "Got 584 / 1000 correct (58.40%)\n",
      "\n",
      "Iteration 1700, loss = 0.9506\n",
      "Got 568 / 1000 correct (56.80%)\n",
      "\n",
      "Iteration 1800, loss = 0.9768\n",
      "Got 568 / 1000 correct (56.80%)\n",
      "\n",
      "Iteration 1900, loss = 0.9696\n",
      "Got 584 / 1000 correct (58.40%)\n",
      "\n",
      "Iteration 2000, loss = 0.9639\n",
      "Got 572 / 1000 correct (57.20%)\n",
      "\n",
      "Iteration 2100, loss = 1.0219\n",
      "Got 593 / 1000 correct (59.30%)\n",
      "\n",
      "Iteration 2200, loss = 1.0190\n",
      "Got 585 / 1000 correct (58.50%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2300, loss = 1.2198\n",
      "Got 600 / 1000 correct (60.00%)\n",
      "\n",
      "Iteration 2400, loss = 1.0070\n",
      "Got 601 / 1000 correct (60.10%)\n",
      "\n",
      "Iteration 2500, loss = 0.9778\n",
      "Got 603 / 1000 correct (60.30%)\n",
      "\n",
      "Iteration 2600, loss = 0.9125\n",
      "Got 581 / 1000 correct (58.10%)\n",
      "\n",
      "Iteration 2700, loss = 1.0129\n",
      "Got 600 / 1000 correct (60.00%)\n",
      "\n",
      "Iteration 2800, loss = 0.9438\n",
      "Got 601 / 1000 correct (60.10%)\n",
      "\n",
      "Iteration 2900, loss = 0.7078\n",
      "Got 595 / 1000 correct (59.50%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-fbef70a2fbb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-9179340a65bd>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model_init_fn, optimizer_init_fn, num_epochs)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_np\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mloss_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "    ############################################################################\n",
    "    channel_1, channel_2, num_classes = 32, 16, 10\n",
    "    \n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    net = model(inputs)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return net\n",
    "\n",
    "pass\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct an optimizer that performs well on CIFAR-10              #\n",
    "    ############################################################################\n",
    "    optimizer = tf.train.MomentumOptimizer(1e-3, momentum=0.9, use_nesterov=True)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 10\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #\n",
    "        # should instantiate layer objects to be used in the forward pass.     #\n",
    "        ########################################################################\n",
    "        initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "        self.conv1 = tf.layers.Conv2D(filters=channel_1, kernel_size=(5, 5),\n",
    "                                      strides=(1, 1), padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      activation=tf.nn.relu)\n",
    "        self.conv2 = tf.layers.Conv2D(filters=channel_2, kernel_size=(3, 3),\n",
    "                                      strides=(1, 1), padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      activation=tf.nn.relu)\n",
    "        self.conv3 = tf.layers.Conv2D(filters=channel_1, kernel_size=(3, 3),\n",
    "                                      strides=(1, 1), padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      activation=None)\n",
    "        self.fc = tf.layers.Dense(num_classes, kernel_initializer=initializer)\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward pass for a three-layer ConvNet. You      #\n",
    "        # should use the layer objects defined in the __init__ method.         #\n",
    "        ########################################################################\n",
    "        a1 = self.conv1(x)\n",
    "        a2 = self.conv2(a1)\n",
    "        h3 = self.conv3(a2)\n",
    "        x = tf.nn.relu(h3 + a1)\n",
    "        x = tf.layers.flatten(x)\n",
    "        scores = self.fc(x)\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.8117\n",
      "Got 108 / 1000 correct (10.80%)\n",
      "Got 93 / 1000 correct (9.30%)\n",
      "\n",
      "Iteration 100, loss = 1.7010\n",
      "Got 536 / 1000 correct (53.60%)\n",
      "Got 456 / 1000 correct (45.60%)\n",
      "\n",
      "Iteration 200, loss = 1.3628\n",
      "Got 550 / 1000 correct (55.00%)\n",
      "Got 485 / 1000 correct (48.50%)\n",
      "\n",
      "Iteration 300, loss = 1.3286\n",
      "Got 560 / 1000 correct (56.00%)\n",
      "Got 506 / 1000 correct (50.60%)\n",
      "\n",
      "Iteration 400, loss = 1.3468\n",
      "Got 579 / 1000 correct (57.90%)\n",
      "Got 536 / 1000 correct (53.60%)\n",
      "\n",
      "Iteration 500, loss = 1.4180\n",
      "Got 584 / 1000 correct (58.40%)\n",
      "Got 528 / 1000 correct (52.80%)\n",
      "\n",
      "Iteration 600, loss = 1.4140\n",
      "Got 586 / 1000 correct (58.60%)\n",
      "Got 554 / 1000 correct (55.40%)\n",
      "\n",
      "Iteration 700, loss = 1.2897\n",
      "Got 611 / 1000 correct (61.10%)\n",
      "Got 567 / 1000 correct (56.70%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "    ############################################################################\n",
    "    channel_1, channel_2, num_classes = 32, 16, 10\n",
    "    \n",
    "    model = ResConvNet(channel_1, channel_2, num_classes)\n",
    "    net = model(inputs)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return net\n",
    "\n",
    "pass\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct an optimizer that performs well on CIFAR-10              #\n",
    "    ############################################################################\n",
    "    optimizer = tf.train.MomentumOptimizer(1e-3, momentum=0.9, use_nesterov=True)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 1\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual block: Conv layer with 32 5x5 filters, Conv layer with 16 3x3 filters, Conv layer with 32 (to match first layer) 3x3 filters. Achieved 56.70% in one epoch. Training accuracy and validation accuracy are similar and low, so probably underfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.6206\n",
      "Got 79 / 1000 correct (7.90%)\n",
      "\n",
      "Iteration 100, loss = 1.9774\n",
      "Got 361 / 1000 correct (36.10%)\n",
      "\n",
      "Iteration 200, loss = 1.3908\n",
      "Got 440 / 1000 correct (44.00%)\n",
      "\n",
      "Iteration 300, loss = 1.5133\n",
      "Got 456 / 1000 correct (45.60%)\n",
      "\n",
      "Iteration 400, loss = 1.4152\n",
      "Got 486 / 1000 correct (48.60%)\n",
      "\n",
      "Iteration 500, loss = 1.4692\n",
      "Got 492 / 1000 correct (49.20%)\n",
      "\n",
      "Iteration 600, loss = 1.5866\n",
      "Got 451 / 1000 correct (45.10%)\n",
      "\n",
      "Iteration 700, loss = 1.3827\n",
      "Got 472 / 1000 correct (47.20%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 1.2985\n",
      "Got 512 / 1000 correct (51.20%)\n",
      "\n",
      "Iteration 900, loss = 1.3165\n",
      "Got 529 / 1000 correct (52.90%)\n",
      "\n",
      "Iteration 1000, loss = 1.2987\n",
      "Got 517 / 1000 correct (51.70%)\n",
      "\n",
      "Iteration 1100, loss = 1.3629\n",
      "Got 557 / 1000 correct (55.70%)\n",
      "\n",
      "Iteration 1200, loss = 1.1156\n",
      "Got 557 / 1000 correct (55.70%)\n",
      "\n",
      "Iteration 1300, loss = 1.1717\n",
      "Got 554 / 1000 correct (55.40%)\n",
      "\n",
      "Iteration 1400, loss = 1.2907\n",
      "Got 554 / 1000 correct (55.40%)\n",
      "\n",
      "Iteration 1500, loss = 1.0904\n",
      "Got 558 / 1000 correct (55.80%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 0.8540\n",
      "Got 549 / 1000 correct (54.90%)\n",
      "\n",
      "Iteration 1700, loss = 1.0743\n",
      "Got 566 / 1000 correct (56.60%)\n",
      "\n",
      "Iteration 1800, loss = 0.8716\n",
      "Got 558 / 1000 correct (55.80%)\n",
      "\n",
      "Iteration 1900, loss = 0.9745\n",
      "Got 567 / 1000 correct (56.70%)\n",
      "\n",
      "Iteration 2000, loss = 0.7255\n",
      "Got 568 / 1000 correct (56.80%)\n",
      "\n",
      "Iteration 2100, loss = 1.0141\n",
      "Got 558 / 1000 correct (55.80%)\n",
      "\n",
      "Iteration 2200, loss = 1.2497\n",
      "Got 566 / 1000 correct (56.60%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2300, loss = 1.0989\n",
      "Got 587 / 1000 correct (58.70%)\n",
      "\n",
      "Iteration 2400, loss = 0.9157\n",
      "Got 581 / 1000 correct (58.10%)\n",
      "\n",
      "Iteration 2500, loss = 1.0665\n",
      "Got 557 / 1000 correct (55.70%)\n",
      "\n",
      "Iteration 2600, loss = 0.8655\n",
      "Got 561 / 1000 correct (56.10%)\n",
      "\n",
      "Iteration 2700, loss = 0.9780\n",
      "Got 546 / 1000 correct (54.60%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-22cd90d8c459>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-9179340a65bd>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model_init_fn, optimizer_init_fn, num_epochs)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_np\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mloss_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "    ############################################################################\n",
    "    channel_1, channel_2, num_classes = 32, 16, 10\n",
    "    \n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    net = model(inputs)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return net\n",
    "\n",
    "pass\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct an optimizer that performs well on CIFAR-10              #\n",
    "    ############################################################################\n",
    "    optimizer = tf.train.MomentumOptimizer(1e-2, momentum=0.9, use_nesterov=True)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 5\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.5797\n",
      "Got 112 / 1000 correct (11.20%)\n",
      "\n",
      "Iteration 100, loss = 2.3113\n",
      "Got 105 / 1000 correct (10.50%)\n",
      "\n",
      "Iteration 200, loss = 2.3142\n",
      "Got 102 / 1000 correct (10.20%)\n",
      "\n",
      "Iteration 300, loss = 2.3079\n",
      "Got 98 / 1000 correct (9.80%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-c90010819a5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-9179340a65bd>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model_init_fn, optimizer_init_fn, num_epochs)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_np\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mloss_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "    ############################################################################\n",
    "    channel_1, channel_2, num_classes = 32, 16, 10\n",
    "    \n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    net = model(inputs)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return net\n",
    "\n",
    "pass\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct an optimizer that performs well on CIFAR-10              #\n",
    "    ############################################################################\n",
    "    optimizer = tf.train.MomentumOptimizer(1e-1, momentum=0.9, use_nesterov=True)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 10\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.6126\n",
      "Got 99 / 1000 correct (9.90%)\n",
      "Got 79 / 1000 correct (7.90%)\n",
      "\n",
      "Iteration 100, loss = 1.8946\n",
      "Got 391 / 1000 correct (39.10%)\n",
      "Got 369 / 1000 correct (36.90%)\n",
      "\n",
      "Iteration 200, loss = 1.5468\n",
      "Got 429 / 1000 correct (42.90%)\n",
      "Got 414 / 1000 correct (41.40%)\n",
      "\n",
      "Iteration 300, loss = 1.7155\n",
      "Got 448 / 1000 correct (44.80%)\n",
      "Got 425 / 1000 correct (42.50%)\n",
      "\n",
      "Iteration 400, loss = 1.4486\n",
      "Got 499 / 1000 correct (49.90%)\n",
      "Got 481 / 1000 correct (48.10%)\n",
      "\n",
      "Iteration 500, loss = 1.5209\n",
      "Got 508 / 1000 correct (50.80%)\n",
      "Got 512 / 1000 correct (51.20%)\n",
      "\n",
      "Iteration 600, loss = 1.3952\n",
      "Got 541 / 1000 correct (54.10%)\n",
      "Got 516 / 1000 correct (51.60%)\n",
      "\n",
      "Iteration 700, loss = 1.2876\n",
      "Got 558 / 1000 correct (55.80%)\n",
      "Got 539 / 1000 correct (53.90%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 1.2397\n",
      "Got 699 / 1000 correct (69.90%)\n",
      "Got 561 / 1000 correct (56.10%)\n",
      "\n",
      "Iteration 900, loss = 1.3828\n",
      "Got 644 / 1000 correct (64.40%)\n",
      "Got 543 / 1000 correct (54.30%)\n",
      "\n",
      "Iteration 1000, loss = 1.2268\n",
      "Got 609 / 1000 correct (60.90%)\n",
      "Got 526 / 1000 correct (52.60%)\n",
      "\n",
      "Iteration 1100, loss = 1.3958\n",
      "Got 637 / 1000 correct (63.70%)\n",
      "Got 549 / 1000 correct (54.90%)\n",
      "\n",
      "Iteration 1200, loss = 1.1151\n",
      "Got 623 / 1000 correct (62.30%)\n",
      "Got 570 / 1000 correct (57.00%)\n",
      "\n",
      "Iteration 1300, loss = 0.9746\n",
      "Got 609 / 1000 correct (60.90%)\n",
      "Got 568 / 1000 correct (56.80%)\n",
      "\n",
      "Iteration 1400, loss = 1.2396\n",
      "Got 627 / 1000 correct (62.70%)\n",
      "Got 569 / 1000 correct (56.90%)\n",
      "\n",
      "Iteration 1500, loss = 1.1814\n",
      "Got 636 / 1000 correct (63.60%)\n",
      "Got 591 / 1000 correct (59.10%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 0.7972\n",
      "Got 734 / 1000 correct (73.40%)\n",
      "Got 568 / 1000 correct (56.80%)\n",
      "\n",
      "Iteration 1700, loss = 1.1386\n",
      "Got 691 / 1000 correct (69.10%)\n",
      "Got 587 / 1000 correct (58.70%)\n",
      "\n",
      "Iteration 1800, loss = 0.8727\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "Got 562 / 1000 correct (56.20%)\n",
      "\n",
      "Iteration 1900, loss = 0.8972\n",
      "Got 692 / 1000 correct (69.20%)\n",
      "Got 569 / 1000 correct (56.90%)\n",
      "\n",
      "Iteration 2000, loss = 0.9509\n",
      "Got 684 / 1000 correct (68.40%)\n",
      "Got 569 / 1000 correct (56.90%)\n",
      "\n",
      "Iteration 2100, loss = 0.8822\n",
      "Got 664 / 1000 correct (66.40%)\n",
      "Got 579 / 1000 correct (57.90%)\n",
      "\n",
      "Iteration 2200, loss = 1.0893\n",
      "Got 656 / 1000 correct (65.60%)\n",
      "Got 561 / 1000 correct (56.10%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2300, loss = 1.0180\n",
      "Got 671 / 1000 correct (67.10%)\n",
      "Got 586 / 1000 correct (58.60%)\n",
      "\n",
      "Iteration 2400, loss = 0.9121\n",
      "Got 760 / 1000 correct (76.00%)\n",
      "Got 572 / 1000 correct (57.20%)\n",
      "\n",
      "Iteration 2500, loss = 1.0460\n",
      "Got 740 / 1000 correct (74.00%)\n",
      "Got 566 / 1000 correct (56.60%)\n",
      "\n",
      "Iteration 2600, loss = 0.8202\n",
      "Got 691 / 1000 correct (69.10%)\n",
      "Got 563 / 1000 correct (56.30%)\n",
      "\n",
      "Iteration 2700, loss = 1.0453\n",
      "Got 707 / 1000 correct (70.70%)\n",
      "Got 583 / 1000 correct (58.30%)\n",
      "\n",
      "Iteration 2800, loss = 1.0777\n",
      "Got 691 / 1000 correct (69.10%)\n",
      "Got 590 / 1000 correct (59.00%)\n",
      "\n",
      "Iteration 2900, loss = 0.6123\n",
      "Got 674 / 1000 correct (67.40%)\n",
      "Got 558 / 1000 correct (55.80%)\n",
      "\n",
      "Iteration 3000, loss = 0.9548\n",
      "Got 694 / 1000 correct (69.40%)\n",
      "Got 563 / 1000 correct (56.30%)\n",
      "\n",
      "Starting epoch 4\n",
      "Iteration 3100, loss = 0.7503\n",
      "Got 849 / 1000 correct (84.90%)\n",
      "Got 582 / 1000 correct (58.20%)\n",
      "\n",
      "Iteration 3200, loss = 0.9160\n",
      "Got 771 / 1000 correct (77.10%)\n",
      "Got 558 / 1000 correct (55.80%)\n",
      "\n",
      "Iteration 3300, loss = 0.9018\n",
      "Got 778 / 1000 correct (77.80%)\n",
      "Got 570 / 1000 correct (57.00%)\n",
      "\n",
      "Iteration 3400, loss = 1.0524\n",
      "Got 733 / 1000 correct (73.30%)\n",
      "Got 561 / 1000 correct (56.10%)\n",
      "\n",
      "Iteration 3500, loss = 0.7508\n",
      "Got 728 / 1000 correct (72.80%)\n",
      "Got 546 / 1000 correct (54.60%)\n",
      "\n",
      "Iteration 3600, loss = 1.1679\n",
      "Got 723 / 1000 correct (72.30%)\n",
      "Got 572 / 1000 correct (57.20%)\n",
      "\n",
      "Iteration 3700, loss = 0.7361\n",
      "Got 702 / 1000 correct (70.20%)\n",
      "Got 554 / 1000 correct (55.40%)\n",
      "\n",
      "Iteration 3800, loss = 0.8071\n",
      "Got 688 / 1000 correct (68.80%)\n",
      "Got 549 / 1000 correct (54.90%)\n",
      "\n",
      "Starting epoch 5\n",
      "Iteration 3900, loss = 0.9709\n",
      "Got 805 / 1000 correct (80.50%)\n",
      "Got 557 / 1000 correct (55.70%)\n",
      "\n",
      "Iteration 4000, loss = 1.1493\n",
      "Got 776 / 1000 correct (77.60%)\n",
      "Got 568 / 1000 correct (56.80%)\n",
      "\n",
      "Iteration 4100, loss = 0.6455\n",
      "Got 746 / 1000 correct (74.60%)\n",
      "Got 540 / 1000 correct (54.00%)\n",
      "\n",
      "Iteration 4200, loss = 0.5268\n",
      "Got 737 / 1000 correct (73.70%)\n",
      "Got 550 / 1000 correct (55.00%)\n",
      "\n",
      "Iteration 4300, loss = 0.7547\n",
      "Got 739 / 1000 correct (73.90%)\n",
      "Got 570 / 1000 correct (57.00%)\n",
      "\n",
      "Iteration 4400, loss = 0.5131\n",
      "Got 737 / 1000 correct (73.70%)\n",
      "Got 542 / 1000 correct (54.20%)\n",
      "\n",
      "Iteration 4500, loss = 0.7693\n",
      "Got 708 / 1000 correct (70.80%)\n",
      "Got 528 / 1000 correct (52.80%)\n",
      "\n",
      "Starting epoch 6\n",
      "Iteration 4600, loss = 0.6125\n",
      "Got 752 / 1000 correct (75.20%)\n",
      "Got 532 / 1000 correct (53.20%)\n",
      "\n",
      "Iteration 4700, loss = 0.6884\n",
      "Got 795 / 1000 correct (79.50%)\n",
      "Got 547 / 1000 correct (54.70%)\n",
      "\n",
      "Iteration 4800, loss = 0.8942\n",
      "Got 773 / 1000 correct (77.30%)\n",
      "Got 568 / 1000 correct (56.80%)\n",
      "\n",
      "Iteration 4900, loss = 0.9087\n",
      "Got 752 / 1000 correct (75.20%)\n",
      "Got 529 / 1000 correct (52.90%)\n",
      "\n",
      "Iteration 5000, loss = 0.8257\n",
      "Got 752 / 1000 correct (75.20%)\n",
      "Got 562 / 1000 correct (56.20%)\n",
      "\n",
      "Iteration 5100, loss = 0.8975\n",
      "Got 729 / 1000 correct (72.90%)\n",
      "Got 550 / 1000 correct (55.00%)\n",
      "\n",
      "Iteration 5200, loss = 0.7865\n",
      "Got 730 / 1000 correct (73.00%)\n",
      "Got 552 / 1000 correct (55.20%)\n",
      "\n",
      "Iteration 5300, loss = 0.7180\n",
      "Got 718 / 1000 correct (71.80%)\n",
      "Got 535 / 1000 correct (53.50%)\n",
      "\n",
      "Starting epoch 7\n",
      "Iteration 5400, loss = 0.4547\n",
      "Got 889 / 1000 correct (88.90%)\n",
      "Got 551 / 1000 correct (55.10%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-e9f702a78c99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-c8b79c731626>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model_init_fn, optimizer_init_fn, num_epochs)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_np\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mloss_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "    ############################################################################\n",
    "    channel_1, channel_2, num_classes = 32, 16, 10\n",
    "    \n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    net = model(inputs)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return net\n",
    "\n",
    "pass\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct an optimizer that performs well on CIFAR-10              #\n",
    "    ############################################################################\n",
    "    optimizer = tf.train.MomentumOptimizer(1e-2, momentum=0.9, use_nesterov=True)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 10\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training accuracy is higher than validation accuracy, which indicates overfitting. Should reduce the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.7734\n",
      "Got 138 / 1000 correct (13.80%)\n",
      "Got 136 / 1000 correct (13.60%)\n",
      "\n",
      "Iteration 100, loss = 1.7228\n",
      "Got 438 / 1000 correct (43.80%)\n",
      "Got 417 / 1000 correct (41.70%)\n",
      "\n",
      "Iteration 200, loss = 1.3687\n",
      "Got 441 / 1000 correct (44.10%)\n",
      "Got 477 / 1000 correct (47.70%)\n",
      "\n",
      "Iteration 300, loss = 1.4914\n",
      "Got 465 / 1000 correct (46.50%)\n",
      "Got 468 / 1000 correct (46.80%)\n",
      "\n",
      "Iteration 400, loss = 1.3018\n",
      "Got 492 / 1000 correct (49.20%)\n",
      "Got 506 / 1000 correct (50.60%)\n",
      "\n",
      "Iteration 500, loss = 1.4800\n",
      "Got 517 / 1000 correct (51.70%)\n",
      "Got 527 / 1000 correct (52.70%)\n",
      "\n",
      "Iteration 600, loss = 1.3689\n",
      "Got 525 / 1000 correct (52.50%)\n",
      "Got 523 / 1000 correct (52.30%)\n",
      "\n",
      "Iteration 700, loss = 1.3352\n",
      "Got 533 / 1000 correct (53.30%)\n",
      "Got 531 / 1000 correct (53.10%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 1.3697\n",
      "Got 673 / 1000 correct (67.30%)\n",
      "Got 565 / 1000 correct (56.50%)\n",
      "\n",
      "Iteration 900, loss = 1.1937\n",
      "Got 609 / 1000 correct (60.90%)\n",
      "Got 538 / 1000 correct (53.80%)\n",
      "\n",
      "Iteration 1000, loss = 1.3047\n",
      "Got 576 / 1000 correct (57.60%)\n",
      "Got 542 / 1000 correct (54.20%)\n",
      "\n",
      "Iteration 1100, loss = 1.3358\n",
      "Got 591 / 1000 correct (59.10%)\n",
      "Got 586 / 1000 correct (58.60%)\n",
      "\n",
      "Iteration 1200, loss = 1.0431\n",
      "Got 577 / 1000 correct (57.70%)\n",
      "Got 552 / 1000 correct (55.20%)\n",
      "\n",
      "Iteration 1300, loss = 1.2669\n",
      "Got 569 / 1000 correct (56.90%)\n",
      "Got 532 / 1000 correct (53.20%)\n",
      "\n",
      "Iteration 1400, loss = 1.2183\n",
      "Got 569 / 1000 correct (56.90%)\n",
      "Got 559 / 1000 correct (55.90%)\n",
      "\n",
      "Iteration 1500, loss = 1.0862\n",
      "Got 610 / 1000 correct (61.00%)\n",
      "Got 573 / 1000 correct (57.30%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 1.0435\n",
      "Got 715 / 1000 correct (71.50%)\n",
      "Got 555 / 1000 correct (55.50%)\n",
      "\n",
      "Iteration 1700, loss = 0.9055\n",
      "Got 707 / 1000 correct (70.70%)\n",
      "Got 597 / 1000 correct (59.70%)\n",
      "\n",
      "Iteration 1800, loss = 0.9654\n",
      "Got 668 / 1000 correct (66.80%)\n",
      "Got 575 / 1000 correct (57.50%)\n",
      "\n",
      "Iteration 1900, loss = 1.0368\n",
      "Got 670 / 1000 correct (67.00%)\n",
      "Got 607 / 1000 correct (60.70%)\n",
      "\n",
      "Iteration 2000, loss = 0.9905\n",
      "Got 653 / 1000 correct (65.30%)\n",
      "Got 592 / 1000 correct (59.20%)\n",
      "\n",
      "Iteration 2100, loss = 0.7624\n",
      "Got 639 / 1000 correct (63.90%)\n",
      "Got 567 / 1000 correct (56.70%)\n",
      "\n",
      "Iteration 2200, loss = 1.1070\n",
      "Got 653 / 1000 correct (65.30%)\n",
      "Got 585 / 1000 correct (58.50%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2300, loss = 1.1702\n",
      "Got 655 / 1000 correct (65.50%)\n",
      "Got 577 / 1000 correct (57.70%)\n",
      "\n",
      "Iteration 2400, loss = 1.0452\n",
      "Got 769 / 1000 correct (76.90%)\n",
      "Got 586 / 1000 correct (58.60%)\n",
      "\n",
      "Iteration 2500, loss = 1.1820\n",
      "Got 702 / 1000 correct (70.20%)\n",
      "Got 585 / 1000 correct (58.50%)\n",
      "\n",
      "Iteration 2600, loss = 0.7945\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "Got 568 / 1000 correct (56.80%)\n",
      "\n",
      "Iteration 2700, loss = 1.0621\n",
      "Got 679 / 1000 correct (67.90%)\n",
      "Got 589 / 1000 correct (58.90%)\n",
      "\n",
      "Iteration 2800, loss = 0.9866\n",
      "Got 665 / 1000 correct (66.50%)\n",
      "Got 561 / 1000 correct (56.10%)\n",
      "\n",
      "Iteration 2900, loss = 0.6493\n",
      "Got 654 / 1000 correct (65.40%)\n",
      "Got 570 / 1000 correct (57.00%)\n",
      "\n",
      "Iteration 3000, loss = 0.8214\n",
      "Got 680 / 1000 correct (68.00%)\n",
      "Got 575 / 1000 correct (57.50%)\n",
      "\n",
      "Starting epoch 4\n",
      "Iteration 3100, loss = 0.8184\n",
      "Got 813 / 1000 correct (81.30%)\n",
      "Got 567 / 1000 correct (56.70%)\n",
      "\n",
      "Iteration 3200, loss = 1.0001\n",
      "Got 777 / 1000 correct (77.70%)\n",
      "Got 584 / 1000 correct (58.40%)\n",
      "\n",
      "Iteration 3300, loss = 1.0322\n",
      "Got 735 / 1000 correct (73.50%)\n",
      "Got 590 / 1000 correct (59.00%)\n",
      "\n",
      "Iteration 3400, loss = 0.8368\n",
      "Got 705 / 1000 correct (70.50%)\n",
      "Got 579 / 1000 correct (57.90%)\n",
      "\n",
      "Iteration 3500, loss = 0.7704\n",
      "Got 702 / 1000 correct (70.20%)\n",
      "Got 560 / 1000 correct (56.00%)\n",
      "\n",
      "Iteration 3600, loss = 1.0076\n",
      "Got 679 / 1000 correct (67.90%)\n",
      "Got 547 / 1000 correct (54.70%)\n",
      "\n",
      "Iteration 3700, loss = 1.0284\n",
      "Got 659 / 1000 correct (65.90%)\n",
      "Got 553 / 1000 correct (55.30%)\n",
      "\n",
      "Iteration 3800, loss = 0.7376\n",
      "Got 684 / 1000 correct (68.40%)\n",
      "Got 551 / 1000 correct (55.10%)\n",
      "\n",
      "Starting epoch 5\n",
      "Iteration 3900, loss = 1.1805\n",
      "Got 786 / 1000 correct (78.60%)\n",
      "Got 543 / 1000 correct (54.30%)\n",
      "\n",
      "Iteration 4000, loss = 1.2012\n",
      "Got 742 / 1000 correct (74.20%)\n",
      "Got 572 / 1000 correct (57.20%)\n",
      "\n",
      "Iteration 4100, loss = 0.7610\n",
      "Got 730 / 1000 correct (73.00%)\n",
      "Got 543 / 1000 correct (54.30%)\n",
      "\n",
      "Iteration 4200, loss = 0.5937\n",
      "Got 732 / 1000 correct (73.20%)\n",
      "Got 578 / 1000 correct (57.80%)\n",
      "\n",
      "Iteration 4300, loss = 0.6809\n",
      "Got 709 / 1000 correct (70.90%)\n",
      "Got 565 / 1000 correct (56.50%)\n",
      "\n",
      "Iteration 4400, loss = 0.8208\n",
      "Got 672 / 1000 correct (67.20%)\n",
      "Got 550 / 1000 correct (55.00%)\n",
      "\n",
      "Iteration 4500, loss = 0.9166\n",
      "Got 692 / 1000 correct (69.20%)\n",
      "Got 557 / 1000 correct (55.70%)\n",
      "\n",
      "Starting epoch 6\n",
      "Iteration 4600, loss = 0.6922\n",
      "Got 764 / 1000 correct (76.40%)\n",
      "Got 554 / 1000 correct (55.40%)\n",
      "\n",
      "Iteration 4700, loss = 0.6028\n",
      "Got 804 / 1000 correct (80.40%)\n",
      "Got 546 / 1000 correct (54.60%)\n",
      "\n",
      "Iteration 4800, loss = 1.0424\n",
      "Got 763 / 1000 correct (76.30%)\n",
      "Got 549 / 1000 correct (54.90%)\n",
      "\n",
      "Iteration 4900, loss = 0.8215\n",
      "Got 733 / 1000 correct (73.30%)\n",
      "Got 561 / 1000 correct (56.10%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-3caf9df6e61a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-c8b79c731626>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model_init_fn, optimizer_init_fn, num_epochs)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_np\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mloss_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "    ############################################################################\n",
    "    channel_1, channel_2, num_classes = 16, 16, 10\n",
    "    \n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    net = model(inputs)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return net\n",
    "\n",
    "pass\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct an optimizer that performs well on CIFAR-10              #\n",
    "    ############################################################################\n",
    "    optimizer = tf.train.MomentumOptimizer(1e-2, momentum=0.9, use_nesterov=True)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 10\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FiveLayerConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #\n",
    "        # should instantiate layer objects to be used in the forward pass.     #\n",
    "        ########################################################################\n",
    "        initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "        self.conv1 = tf.layers.Conv2D(filters=channel_1, kernel_size=(5, 5),\n",
    "                                      strides=(1, 1), padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      activation=tf.nn.relu)\n",
    "        self.conv2 = tf.layers.Conv2D(filters=channel_2, kernel_size=(3, 3),\n",
    "                                      strides=(1, 1), padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      activation=tf.nn.relu)\n",
    "        self.fc = tf.layers.Dense(num_classes, kernel_initializer=initializer)\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward pass for a three-layer ConvNet. You      #\n",
    "        # should use the layer objects defined in the __init__ method.         #\n",
    "        ########################################################################\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.layers.flatten(x)\n",
    "        scores = self.fc(x)\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.6521\n",
      "Got 93 / 1000 correct (9.30%)\n",
      "Got 113 / 1000 correct (11.30%)\n",
      "\n",
      "Iteration 100, loss = 1.9378\n",
      "Got 333 / 1000 correct (33.30%)\n",
      "Got 323 / 1000 correct (32.30%)\n",
      "\n",
      "Iteration 200, loss = 1.5910\n",
      "Got 408 / 1000 correct (40.80%)\n",
      "Got 417 / 1000 correct (41.70%)\n",
      "\n",
      "Iteration 300, loss = 1.6727\n",
      "Got 428 / 1000 correct (42.80%)\n",
      "Got 412 / 1000 correct (41.20%)\n",
      "\n",
      "Iteration 400, loss = 1.5895\n",
      "Got 419 / 1000 correct (41.90%)\n",
      "Got 436 / 1000 correct (43.60%)\n",
      "\n",
      "Iteration 500, loss = 1.5646\n",
      "Got 458 / 1000 correct (45.80%)\n",
      "Got 477 / 1000 correct (47.70%)\n",
      "\n",
      "Iteration 600, loss = 1.5474\n",
      "Got 477 / 1000 correct (47.70%)\n",
      "Got 466 / 1000 correct (46.60%)\n",
      "\n",
      "Iteration 700, loss = 1.6355\n",
      "Got 484 / 1000 correct (48.40%)\n",
      "Got 480 / 1000 correct (48.00%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 1.3021\n",
      "Got 559 / 1000 correct (55.90%)\n",
      "Got 511 / 1000 correct (51.10%)\n",
      "\n",
      "Iteration 900, loss = 1.4164\n",
      "Got 544 / 1000 correct (54.40%)\n",
      "Got 504 / 1000 correct (50.40%)\n",
      "\n",
      "Iteration 1000, loss = 1.4132\n",
      "Got 525 / 1000 correct (52.50%)\n",
      "Got 493 / 1000 correct (49.30%)\n",
      "\n",
      "Iteration 1100, loss = 1.3558\n",
      "Got 548 / 1000 correct (54.80%)\n",
      "Got 537 / 1000 correct (53.70%)\n",
      "\n",
      "Iteration 1200, loss = 1.3491\n",
      "Got 536 / 1000 correct (53.60%)\n",
      "Got 537 / 1000 correct (53.70%)\n",
      "\n",
      "Iteration 1300, loss = 1.3468\n",
      "Got 550 / 1000 correct (55.00%)\n",
      "Got 534 / 1000 correct (53.40%)\n",
      "\n",
      "Iteration 1400, loss = 1.3080\n",
      "Got 536 / 1000 correct (53.60%)\n",
      "Got 527 / 1000 correct (52.70%)\n",
      "\n",
      "Iteration 1500, loss = 1.2804\n",
      "Got 548 / 1000 correct (54.80%)\n",
      "Got 546 / 1000 correct (54.60%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 1.2141\n",
      "Got 633 / 1000 correct (63.30%)\n",
      "Got 548 / 1000 correct (54.80%)\n",
      "\n",
      "Iteration 1700, loss = 1.1154\n",
      "Got 580 / 1000 correct (58.00%)\n",
      "Got 540 / 1000 correct (54.00%)\n",
      "\n",
      "Iteration 1800, loss = 0.9103\n",
      "Got 618 / 1000 correct (61.80%)\n",
      "Got 536 / 1000 correct (53.60%)\n",
      "\n",
      "Iteration 1900, loss = 1.0720\n",
      "Got 599 / 1000 correct (59.90%)\n",
      "Got 554 / 1000 correct (55.40%)\n",
      "\n",
      "Iteration 2000, loss = 0.9902\n",
      "Got 582 / 1000 correct (58.20%)\n",
      "Got 549 / 1000 correct (54.90%)\n",
      "\n",
      "Iteration 2100, loss = 1.0599\n",
      "Got 600 / 1000 correct (60.00%)\n",
      "Got 551 / 1000 correct (55.10%)\n",
      "\n",
      "Iteration 2200, loss = 1.1852\n",
      "Got 595 / 1000 correct (59.50%)\n",
      "Got 557 / 1000 correct (55.70%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2300, loss = 1.2457\n",
      "Got 626 / 1000 correct (62.60%)\n",
      "Got 560 / 1000 correct (56.00%)\n",
      "\n",
      "Iteration 2400, loss = 1.0765\n",
      "Got 723 / 1000 correct (72.30%)\n",
      "Got 573 / 1000 correct (57.30%)\n",
      "\n",
      "Iteration 2500, loss = 1.1498\n",
      "Got 662 / 1000 correct (66.20%)\n",
      "Got 561 / 1000 correct (56.10%)\n",
      "\n",
      "Iteration 2600, loss = 1.0405\n",
      "Got 625 / 1000 correct (62.50%)\n",
      "Got 540 / 1000 correct (54.00%)\n",
      "\n",
      "Iteration 2700, loss = 1.2142\n",
      "Got 605 / 1000 correct (60.50%)\n",
      "Got 517 / 1000 correct (51.70%)\n",
      "\n",
      "Iteration 2800, loss = 1.1513\n",
      "Got 622 / 1000 correct (62.20%)\n",
      "Got 544 / 1000 correct (54.40%)\n",
      "\n",
      "Iteration 2900, loss = 0.8001\n",
      "Got 640 / 1000 correct (64.00%)\n",
      "Got 551 / 1000 correct (55.10%)\n",
      "\n",
      "Iteration 3000, loss = 1.1439\n",
      "Got 564 / 1000 correct (56.40%)\n",
      "Got 532 / 1000 correct (53.20%)\n",
      "\n",
      "Starting epoch 4\n",
      "Iteration 3100, loss = 0.8497\n",
      "Got 757 / 1000 correct (75.70%)\n",
      "Got 548 / 1000 correct (54.80%)\n",
      "\n",
      "Iteration 3200, loss = 1.1389\n",
      "Got 722 / 1000 correct (72.20%)\n",
      "Got 584 / 1000 correct (58.40%)\n",
      "\n",
      "Iteration 3300, loss = 1.0374\n",
      "Got 673 / 1000 correct (67.30%)\n",
      "Got 565 / 1000 correct (56.50%)\n",
      "\n",
      "Iteration 3400, loss = 1.0155\n",
      "Got 656 / 1000 correct (65.60%)\n",
      "Got 563 / 1000 correct (56.30%)\n",
      "\n",
      "Iteration 3500, loss = 0.8177\n",
      "Got 635 / 1000 correct (63.50%)\n",
      "Got 573 / 1000 correct (57.30%)\n",
      "\n",
      "Iteration 3600, loss = 1.1655\n",
      "Got 653 / 1000 correct (65.30%)\n",
      "Got 565 / 1000 correct (56.50%)\n",
      "\n",
      "Iteration 3700, loss = 0.8331\n",
      "Got 645 / 1000 correct (64.50%)\n",
      "Got 557 / 1000 correct (55.70%)\n",
      "\n",
      "Iteration 3800, loss = 1.1124\n",
      "Got 608 / 1000 correct (60.80%)\n",
      "Got 536 / 1000 correct (53.60%)\n",
      "\n",
      "Starting epoch 5\n",
      "Iteration 3900, loss = 1.0538\n",
      "Got 721 / 1000 correct (72.10%)\n",
      "Got 567 / 1000 correct (56.70%)\n",
      "\n",
      "Iteration 4000, loss = 1.1486\n",
      "Got 676 / 1000 correct (67.60%)\n",
      "Got 558 / 1000 correct (55.80%)\n",
      "\n",
      "Iteration 4100, loss = 1.1476\n",
      "Got 700 / 1000 correct (70.00%)\n",
      "Got 543 / 1000 correct (54.30%)\n",
      "\n",
      "Iteration 4200, loss = 0.8448\n",
      "Got 653 / 1000 correct (65.30%)\n",
      "Got 536 / 1000 correct (53.60%)\n",
      "\n",
      "Iteration 4300, loss = 1.0620\n",
      "Got 655 / 1000 correct (65.50%)\n",
      "Got 544 / 1000 correct (54.40%)\n",
      "\n",
      "Iteration 4400, loss = 1.0044\n",
      "Got 649 / 1000 correct (64.90%)\n",
      "Got 536 / 1000 correct (53.60%)\n",
      "\n",
      "Iteration 4500, loss = 1.0894\n",
      "Got 661 / 1000 correct (66.10%)\n",
      "Got 547 / 1000 correct (54.70%)\n",
      "\n",
      "Starting epoch 6\n",
      "Iteration 4600, loss = 0.9460\n",
      "Got 708 / 1000 correct (70.80%)\n",
      "Got 555 / 1000 correct (55.50%)\n",
      "\n",
      "Iteration 4700, loss = 0.9399\n",
      "Got 746 / 1000 correct (74.60%)\n",
      "Got 552 / 1000 correct (55.20%)\n",
      "\n",
      "Iteration 4800, loss = 0.9636\n",
      "Got 701 / 1000 correct (70.10%)\n",
      "Got 538 / 1000 correct (53.80%)\n",
      "\n",
      "Iteration 4900, loss = 0.9391\n",
      "Got 684 / 1000 correct (68.40%)\n",
      "Got 543 / 1000 correct (54.30%)\n",
      "\n",
      "Iteration 5000, loss = 0.8748\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "Got 530 / 1000 correct (53.00%)\n",
      "\n",
      "Iteration 5100, loss = 0.8947\n",
      "Got 647 / 1000 correct (64.70%)\n",
      "Got 542 / 1000 correct (54.20%)\n",
      "\n",
      "Iteration 5200, loss = 0.7881\n",
      "Got 673 / 1000 correct (67.30%)\n",
      "Got 543 / 1000 correct (54.30%)\n",
      "\n",
      "Iteration 5300, loss = 1.0436\n",
      "Got 630 / 1000 correct (63.00%)\n",
      "Got 509 / 1000 correct (50.90%)\n",
      "\n",
      "Starting epoch 7\n",
      "Iteration 5400, loss = 0.7696\n",
      "Got 808 / 1000 correct (80.80%)\n",
      "Got 528 / 1000 correct (52.80%)\n",
      "\n",
      "Iteration 5500, loss = 0.8251\n",
      "Got 741 / 1000 correct (74.10%)\n",
      "Got 542 / 1000 correct (54.20%)\n",
      "\n",
      "Iteration 5600, loss = 0.5935\n",
      "Got 741 / 1000 correct (74.10%)\n",
      "Got 535 / 1000 correct (53.50%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-c1290bb1a891>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-c8b79c731626>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model_init_fn, optimizer_init_fn, num_epochs)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_np\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mloss_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "    ############################################################################\n",
    "    channel_1, channel_2, num_classes = 16, 16, 10\n",
    "    \n",
    "    model = FiveLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    net = model(inputs)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return net\n",
    "\n",
    "pass\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct an optimizer that performs well on CIFAR-10              #\n",
    "    ############################################################################\n",
    "    optimizer = tf.train.MomentumOptimizer(1e-2, momentum=0.9, use_nesterov=True)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 10\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SixLayerConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #\n",
    "        # should instantiate layer objects to be used in the forward pass.     #\n",
    "        ########################################################################\n",
    "        initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "        self.conv1 = tf.layers.Conv2D(filters=channel_1, kernel_size=(5, 5),\n",
    "                                      strides=(1, 1), padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      activation=tf.nn.relu)\n",
    "        self.conv2 = tf.layers.Conv2D(filters=channel_2, kernel_size=(3, 3),\n",
    "                                      strides=(1, 1), padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      activation=tf.nn.relu)\n",
    "        self.fc1 = tf.layers.Dense(3072, kernel_initializer=initializer)\n",
    "        self.fc2 = tf.layers.Dense(num_classes, kernel_initializer=initializer)\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward pass for a three-layer ConvNet. You      #\n",
    "        # should use the layer objects defined in the __init__ method.         #\n",
    "        ########################################################################\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.layers.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        scores = self.fc2(x)\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.5678\n",
      "Got 92 / 1000 correct (9.20%)\n",
      "Got 112 / 1000 correct (11.20%)\n",
      "\n",
      "Iteration 100, loss = 2.0386\n",
      "Got 244 / 1000 correct (24.40%)\n",
      "Got 234 / 1000 correct (23.40%)\n",
      "\n",
      "Iteration 200, loss = 1.7814\n",
      "Got 295 / 1000 correct (29.50%)\n",
      "Got 313 / 1000 correct (31.30%)\n",
      "\n",
      "Iteration 300, loss = 1.8986\n",
      "Got 303 / 1000 correct (30.30%)\n",
      "Got 292 / 1000 correct (29.20%)\n",
      "\n",
      "Iteration 400, loss = 1.8461\n",
      "Got 377 / 1000 correct (37.70%)\n",
      "Got 377 / 1000 correct (37.70%)\n",
      "\n",
      "Iteration 500, loss = 1.6532\n",
      "Got 382 / 1000 correct (38.20%)\n",
      "Got 409 / 1000 correct (40.90%)\n",
      "\n",
      "Iteration 600, loss = 1.6139\n",
      "Got 419 / 1000 correct (41.90%)\n",
      "Got 437 / 1000 correct (43.70%)\n",
      "\n",
      "Iteration 700, loss = 1.5128\n",
      "Got 415 / 1000 correct (41.50%)\n",
      "Got 442 / 1000 correct (44.20%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 1.5266\n",
      "Got 516 / 1000 correct (51.60%)\n",
      "Got 455 / 1000 correct (45.50%)\n",
      "\n",
      "Iteration 900, loss = 1.6764\n",
      "Got 515 / 1000 correct (51.50%)\n",
      "Got 463 / 1000 correct (46.30%)\n",
      "\n",
      "Iteration 1000, loss = 1.4928\n",
      "Got 495 / 1000 correct (49.50%)\n",
      "Got 462 / 1000 correct (46.20%)\n",
      "\n",
      "Iteration 1100, loss = 1.3982\n",
      "Got 492 / 1000 correct (49.20%)\n",
      "Got 463 / 1000 correct (46.30%)\n",
      "\n",
      "Iteration 1200, loss = 1.4502\n",
      "Got 519 / 1000 correct (51.90%)\n",
      "Got 500 / 1000 correct (50.00%)\n",
      "\n",
      "Iteration 1300, loss = 1.5085\n",
      "Got 521 / 1000 correct (52.10%)\n",
      "Got 509 / 1000 correct (50.90%)\n",
      "\n",
      "Iteration 1400, loss = 1.3969\n",
      "Got 518 / 1000 correct (51.80%)\n",
      "Got 488 / 1000 correct (48.80%)\n",
      "\n",
      "Iteration 1500, loss = 1.3128\n",
      "Got 519 / 1000 correct (51.90%)\n",
      "Got 500 / 1000 correct (50.00%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 1.0089\n",
      "Got 584 / 1000 correct (58.40%)\n",
      "Got 505 / 1000 correct (50.50%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-229c9d893aa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-c8b79c731626>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model_init_fn, optimizer_init_fn, num_epochs)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_np\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mloss_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kelvin/anaconda3/envs/cs231n/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "    ############################################################################\n",
    "    channel_1, channel_2, num_classes = 16, 16, 10\n",
    "    \n",
    "    model = SixLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    net = model(inputs)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return net\n",
    "\n",
    "pass\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct an optimizer that performs well on CIFAR-10              #\n",
    "    ############################################################################\n",
    "    optimizer = tf.train.MomentumOptimizer(1e-2, momentum=0.9, use_nesterov=True)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 10\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.9305\n",
      "Got 119 / 1000 correct (11.90%)\n",
      "Got 94 / 1000 correct (9.40%)\n",
      "\n",
      "Iteration 100, loss = 2.2727\n",
      "Got 154 / 1000 correct (15.40%)\n",
      "Got 127 / 1000 correct (12.70%)\n",
      "\n",
      "Iteration 200, loss = 2.1077\n",
      "Got 207 / 1000 correct (20.70%)\n",
      "Got 203 / 1000 correct (20.30%)\n",
      "\n",
      "Iteration 300, loss = 2.0101\n",
      "Got 282 / 1000 correct (28.20%)\n",
      "Got 292 / 1000 correct (29.20%)\n",
      "\n",
      "Iteration 400, loss = 1.8355\n",
      "Got 358 / 1000 correct (35.80%)\n",
      "Got 351 / 1000 correct (35.10%)\n",
      "\n",
      "Iteration 500, loss = 1.7890\n",
      "Got 383 / 1000 correct (38.30%)\n",
      "Got 396 / 1000 correct (39.60%)\n",
      "\n",
      "Iteration 600, loss = 1.6442\n",
      "Got 422 / 1000 correct (42.20%)\n",
      "Got 434 / 1000 correct (43.40%)\n",
      "\n",
      "Iteration 700, loss = 1.5902\n",
      "Got 414 / 1000 correct (41.40%)\n",
      "Got 449 / 1000 correct (44.90%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 1.5757\n",
      "Got 521 / 1000 correct (52.10%)\n",
      "Got 490 / 1000 correct (49.00%)\n",
      "\n",
      "Iteration 900, loss = 1.4995\n",
      "Got 503 / 1000 correct (50.30%)\n",
      "Got 491 / 1000 correct (49.10%)\n",
      "\n",
      "Iteration 1000, loss = 1.4952\n",
      "Got 501 / 1000 correct (50.10%)\n",
      "Got 469 / 1000 correct (46.90%)\n",
      "\n",
      "Iteration 1100, loss = 1.3732\n",
      "Got 505 / 1000 correct (50.50%)\n",
      "Got 529 / 1000 correct (52.90%)\n",
      "\n",
      "Iteration 1200, loss = 1.2595\n",
      "Got 510 / 1000 correct (51.00%)\n",
      "Got 525 / 1000 correct (52.50%)\n",
      "\n",
      "Iteration 1300, loss = 1.2208\n",
      "Got 534 / 1000 correct (53.40%)\n",
      "Got 521 / 1000 correct (52.10%)\n",
      "\n",
      "Iteration 1400, loss = 1.3339\n",
      "Got 562 / 1000 correct (56.20%)\n",
      "Got 564 / 1000 correct (56.40%)\n",
      "\n",
      "Iteration 1500, loss = 1.1865\n",
      "Got 572 / 1000 correct (57.20%)\n",
      "Got 575 / 1000 correct (57.50%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 0.9786\n",
      "Got 607 / 1000 correct (60.70%)\n",
      "Got 552 / 1000 correct (55.20%)\n",
      "\n",
      "Iteration 1700, loss = 1.0150\n",
      "Got 606 / 1000 correct (60.60%)\n",
      "Got 575 / 1000 correct (57.50%)\n",
      "\n",
      "Iteration 1800, loss = 0.9368\n",
      "Got 626 / 1000 correct (62.60%)\n",
      "Got 566 / 1000 correct (56.60%)\n",
      "\n",
      "Iteration 1900, loss = 1.2052\n",
      "Got 619 / 1000 correct (61.90%)\n",
      "Got 583 / 1000 correct (58.30%)\n",
      "\n",
      "Iteration 2000, loss = 1.2414\n",
      "Got 612 / 1000 correct (61.20%)\n",
      "Got 584 / 1000 correct (58.40%)\n",
      "\n",
      "Iteration 2100, loss = 0.8135\n",
      "Got 640 / 1000 correct (64.00%)\n",
      "Got 590 / 1000 correct (59.00%)\n",
      "\n",
      "Iteration 2200, loss = 1.1567\n",
      "Got 627 / 1000 correct (62.70%)\n",
      "Got 604 / 1000 correct (60.40%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2300, loss = 1.2080\n",
      "Got 669 / 1000 correct (66.90%)\n",
      "Got 623 / 1000 correct (62.30%)\n",
      "\n",
      "Iteration 2400, loss = 0.8282\n",
      "Got 728 / 1000 correct (72.80%)\n",
      "Got 621 / 1000 correct (62.10%)\n",
      "\n",
      "Iteration 2500, loss = 1.0766\n",
      "Got 703 / 1000 correct (70.30%)\n",
      "Got 633 / 1000 correct (63.30%)\n",
      "\n",
      "Iteration 2600, loss = 0.9649\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "Got 603 / 1000 correct (60.30%)\n",
      "\n",
      "Iteration 2700, loss = 1.0896\n",
      "Got 651 / 1000 correct (65.10%)\n",
      "Got 610 / 1000 correct (61.00%)\n",
      "\n",
      "Iteration 2800, loss = 0.9286\n",
      "Got 634 / 1000 correct (63.40%)\n",
      "Got 596 / 1000 correct (59.60%)\n",
      "\n",
      "Iteration 2900, loss = 0.5510\n",
      "Got 680 / 1000 correct (68.00%)\n",
      "Got 623 / 1000 correct (62.30%)\n",
      "\n",
      "Iteration 3000, loss = 1.0054\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "Got 624 / 1000 correct (62.40%)\n",
      "\n",
      "Starting epoch 4\n",
      "Iteration 3100, loss = 0.9586\n",
      "Got 841 / 1000 correct (84.10%)\n",
      "Got 646 / 1000 correct (64.60%)\n",
      "\n",
      "Iteration 3200, loss = 0.9341\n",
      "Got 743 / 1000 correct (74.30%)\n",
      "Got 606 / 1000 correct (60.60%)\n",
      "\n",
      "Iteration 3300, loss = 0.8033\n",
      "Got 744 / 1000 correct (74.40%)\n",
      "Got 624 / 1000 correct (62.40%)\n",
      "\n",
      "Iteration 3400, loss = 0.8547\n",
      "Got 736 / 1000 correct (73.60%)\n",
      "Got 621 / 1000 correct (62.10%)\n",
      "\n",
      "Iteration 3500, loss = 0.9855\n",
      "Got 711 / 1000 correct (71.10%)\n",
      "Got 620 / 1000 correct (62.00%)\n",
      "\n",
      "Iteration 3600, loss = 0.9668\n",
      "Got 723 / 1000 correct (72.30%)\n",
      "Got 621 / 1000 correct (62.10%)\n",
      "\n",
      "Iteration 3700, loss = 0.9104\n",
      "Got 723 / 1000 correct (72.30%)\n",
      "Got 629 / 1000 correct (62.90%)\n",
      "\n",
      "Iteration 3800, loss = 0.5883\n",
      "Got 680 / 1000 correct (68.00%)\n",
      "Got 593 / 1000 correct (59.30%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INPUT -> [CONV -> RELU -> POOL]*2 -> FC -> RELU -> FC\n",
    "# common ConvNet architecture, found in notes\n",
    "\n",
    "def four_layer_conv_functional(inputs, channel_1, channel_2, hidden_size, num_classes):     \n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    conv1 = tf.layers.conv2d(inputs, filters=channel_1, kernel_size=5, strides=(1, 1),\n",
    "                             padding=\"same\", activation=tf.nn.relu,\n",
    "                             kernel_initializer=initializer)\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=2, strides=2)\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=channel_2, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=tf.nn.relu,\n",
    "                             kernel_initializer=initializer)\n",
    "    pool2 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "    flattened = tf.layers.flatten(pool2)\n",
    "    fc1 = tf.layers.dense(flattened, hidden_size, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "    scores = tf.layers.dense(fc1, num_classes,\n",
    "                             kernel_initializer=initializer)\n",
    "    return scores\n",
    "\n",
    "channel_1, channel_2, hidden_size, num_classes = 32, 16, 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return four_layer_conv_functional(inputs, channel_1, channel_2, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 5\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training accuracy and validation accuracy are similar but not high enough, so underfitting. Try adding another CONV -> RELU -> POOL layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 3.1731\n",
      "Got 99 / 1000 correct (9.90%)\n",
      "Got 83 / 1000 correct (8.30%)\n",
      "\n",
      "Iteration 100, loss = 2.2304\n",
      "Got 179 / 1000 correct (17.90%)\n",
      "Got 160 / 1000 correct (16.00%)\n",
      "\n",
      "Iteration 200, loss = 2.0542\n",
      "Got 238 / 1000 correct (23.80%)\n",
      "Got 251 / 1000 correct (25.10%)\n",
      "\n",
      "Iteration 300, loss = 1.9870\n",
      "Got 284 / 1000 correct (28.40%)\n",
      "Got 315 / 1000 correct (31.50%)\n",
      "\n",
      "Iteration 400, loss = 1.8308\n",
      "Got 351 / 1000 correct (35.10%)\n",
      "Got 362 / 1000 correct (36.20%)\n",
      "\n",
      "Iteration 500, loss = 1.7339\n",
      "Got 353 / 1000 correct (35.30%)\n",
      "Got 355 / 1000 correct (35.50%)\n",
      "\n",
      "Iteration 600, loss = 1.7241\n",
      "Got 386 / 1000 correct (38.60%)\n",
      "Got 382 / 1000 correct (38.20%)\n",
      "\n",
      "Iteration 700, loss = 1.6157\n",
      "Got 409 / 1000 correct (40.90%)\n",
      "Got 400 / 1000 correct (40.00%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 1.6472\n",
      "Got 478 / 1000 correct (47.80%)\n",
      "Got 458 / 1000 correct (45.80%)\n",
      "\n",
      "Iteration 900, loss = 1.4736\n",
      "Got 488 / 1000 correct (48.80%)\n",
      "Got 470 / 1000 correct (47.00%)\n",
      "\n",
      "Iteration 1000, loss = 1.4718\n",
      "Got 489 / 1000 correct (48.90%)\n",
      "Got 474 / 1000 correct (47.40%)\n",
      "\n",
      "Iteration 1100, loss = 1.4426\n",
      "Got 516 / 1000 correct (51.60%)\n",
      "Got 490 / 1000 correct (49.00%)\n",
      "\n",
      "Iteration 1200, loss = 1.3530\n",
      "Got 530 / 1000 correct (53.00%)\n",
      "Got 511 / 1000 correct (51.10%)\n",
      "\n",
      "Iteration 1300, loss = 1.3966\n",
      "Got 525 / 1000 correct (52.50%)\n",
      "Got 505 / 1000 correct (50.50%)\n",
      "\n",
      "Iteration 1400, loss = 1.3189\n",
      "Got 525 / 1000 correct (52.50%)\n",
      "Got 547 / 1000 correct (54.70%)\n",
      "\n",
      "Iteration 1500, loss = 1.1433\n",
      "Got 530 / 1000 correct (53.00%)\n",
      "Got 546 / 1000 correct (54.60%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 1.0466\n",
      "Got 528 / 1000 correct (52.80%)\n",
      "Got 502 / 1000 correct (50.20%)\n",
      "\n",
      "Iteration 1700, loss = 0.9997\n",
      "Got 585 / 1000 correct (58.50%)\n",
      "Got 558 / 1000 correct (55.80%)\n",
      "\n",
      "Iteration 1800, loss = 1.1863\n",
      "Got 591 / 1000 correct (59.10%)\n",
      "Got 584 / 1000 correct (58.40%)\n",
      "\n",
      "Iteration 1900, loss = 1.2198\n",
      "Got 590 / 1000 correct (59.00%)\n",
      "Got 596 / 1000 correct (59.60%)\n",
      "\n",
      "Iteration 2000, loss = 0.9613\n",
      "Got 575 / 1000 correct (57.50%)\n",
      "Got 596 / 1000 correct (59.60%)\n",
      "\n",
      "Iteration 2100, loss = 1.1227\n",
      "Got 563 / 1000 correct (56.30%)\n",
      "Got 562 / 1000 correct (56.20%)\n",
      "\n",
      "Iteration 2200, loss = 1.1722\n",
      "Got 590 / 1000 correct (59.00%)\n",
      "Got 613 / 1000 correct (61.30%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2300, loss = 1.1218\n",
      "Got 603 / 1000 correct (60.30%)\n",
      "Got 577 / 1000 correct (57.70%)\n",
      "\n",
      "Iteration 2400, loss = 1.1436\n",
      "Got 603 / 1000 correct (60.30%)\n",
      "Got 591 / 1000 correct (59.10%)\n",
      "\n",
      "Iteration 2500, loss = 1.1373\n",
      "Got 614 / 1000 correct (61.40%)\n",
      "Got 587 / 1000 correct (58.70%)\n",
      "\n",
      "Iteration 2600, loss = 1.0203\n",
      "Got 600 / 1000 correct (60.00%)\n",
      "Got 597 / 1000 correct (59.70%)\n",
      "\n",
      "Iteration 2700, loss = 1.1391\n",
      "Got 603 / 1000 correct (60.30%)\n",
      "Got 592 / 1000 correct (59.20%)\n",
      "\n",
      "Iteration 2800, loss = 1.1133\n",
      "Got 616 / 1000 correct (61.60%)\n",
      "Got 628 / 1000 correct (62.80%)\n",
      "\n",
      "Iteration 2900, loss = 0.7087\n",
      "Got 617 / 1000 correct (61.70%)\n",
      "Got 652 / 1000 correct (65.20%)\n",
      "\n",
      "Iteration 3000, loss = 1.0136\n",
      "Got 596 / 1000 correct (59.60%)\n",
      "Got 607 / 1000 correct (60.70%)\n",
      "\n",
      "Starting epoch 4\n",
      "Iteration 3100, loss = 0.9970\n",
      "Got 676 / 1000 correct (67.60%)\n",
      "Got 625 / 1000 correct (62.50%)\n",
      "\n",
      "Iteration 3200, loss = 1.0856\n",
      "Got 661 / 1000 correct (66.10%)\n",
      "Got 633 / 1000 correct (63.30%)\n",
      "\n",
      "Iteration 3300, loss = 0.9571\n",
      "Got 654 / 1000 correct (65.40%)\n",
      "Got 638 / 1000 correct (63.80%)\n",
      "\n",
      "Iteration 3400, loss = 1.1715\n",
      "Got 654 / 1000 correct (65.40%)\n",
      "Got 616 / 1000 correct (61.60%)\n",
      "\n",
      "Iteration 3500, loss = 0.9358\n",
      "Got 637 / 1000 correct (63.70%)\n",
      "Got 614 / 1000 correct (61.40%)\n",
      "\n",
      "Iteration 3600, loss = 1.1290\n",
      "Got 645 / 1000 correct (64.50%)\n",
      "Got 622 / 1000 correct (62.20%)\n",
      "\n",
      "Iteration 3700, loss = 0.9234\n",
      "Got 641 / 1000 correct (64.10%)\n",
      "Got 646 / 1000 correct (64.60%)\n",
      "\n",
      "Iteration 3800, loss = 1.0158\n",
      "Got 663 / 1000 correct (66.30%)\n",
      "Got 645 / 1000 correct (64.50%)\n",
      "\n",
      "Accuracy of final model on the test set:\n",
      "Got 6225 / 10000 correct (62.25%)\n"
     ]
    }
   ],
   "source": [
    "# INPUT -> [CONV -> RELU -> POOL]*3 -> FC -> RELU -> FC\n",
    "# common ConvNet architecture, found in notes\n",
    "\n",
    "def five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes):     \n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    conv1 = tf.layers.conv2d(inputs, filters=channel_1, kernel_size=5, strides=(1, 1),\n",
    "                             padding=\"same\", activation=tf.nn.relu,\n",
    "                             kernel_initializer=initializer)\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=2, strides=2)\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=channel_2, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=tf.nn.relu,\n",
    "                             kernel_initializer=initializer)\n",
    "    pool2 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "    conv3 = tf.layers.conv2d(pool2, filters=channel_3, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=tf.nn.relu,\n",
    "                             kernel_initializer=initializer)\n",
    "    pool3 = tf.layers.max_pooling2d(conv3, pool_size=2, strides=2)\n",
    "    flattened = tf.layers.flatten(pool3)\n",
    "    fc1 = tf.layers.dense(flattened, hidden_size, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "    scores = tf.layers.dense(fc1, num_classes,\n",
    "                             kernel_initializer=initializer)\n",
    "    return scores\n",
    "\n",
    "channel_1, channel_2, channel_3, hidden_size, num_classes = 32, 16, 16, 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 5\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried 3 layers of CONV -> RELU -> POOL, but still did not get 70% accuracy after 5 epochs. Try adding batch normalization to the 2-layer version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.4566\n",
      "Got 119 / 1000 correct (11.90%)\n",
      "Got 115 / 1000 correct (11.50%)\n",
      "\n",
      "Iteration 100, loss = 2.2737\n",
      "Got 150 / 1000 correct (15.00%)\n",
      "Got 121 / 1000 correct (12.10%)\n",
      "\n",
      "Iteration 200, loss = 2.0732\n",
      "Got 220 / 1000 correct (22.00%)\n",
      "Got 226 / 1000 correct (22.60%)\n",
      "\n",
      "Iteration 300, loss = 2.0254\n",
      "Got 286 / 1000 correct (28.60%)\n",
      "Got 297 / 1000 correct (29.70%)\n",
      "\n",
      "Iteration 400, loss = 1.8032\n",
      "Got 337 / 1000 correct (33.70%)\n",
      "Got 345 / 1000 correct (34.50%)\n",
      "\n",
      "Iteration 500, loss = 1.9320\n",
      "Got 383 / 1000 correct (38.30%)\n",
      "Got 372 / 1000 correct (37.20%)\n",
      "\n",
      "Iteration 600, loss = 1.6093\n",
      "Got 403 / 1000 correct (40.30%)\n",
      "Got 402 / 1000 correct (40.20%)\n",
      "\n",
      "Iteration 700, loss = 1.6208\n",
      "Got 443 / 1000 correct (44.30%)\n",
      "Got 433 / 1000 correct (43.30%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 1.5471\n",
      "Got 531 / 1000 correct (53.10%)\n",
      "Got 466 / 1000 correct (46.60%)\n",
      "\n",
      "Iteration 900, loss = 1.6749\n",
      "Got 501 / 1000 correct (50.10%)\n",
      "Got 472 / 1000 correct (47.20%)\n",
      "\n",
      "Iteration 1000, loss = 1.5905\n",
      "Got 502 / 1000 correct (50.20%)\n",
      "Got 467 / 1000 correct (46.70%)\n",
      "\n",
      "Iteration 1100, loss = 1.3018\n",
      "Got 523 / 1000 correct (52.30%)\n",
      "Got 523 / 1000 correct (52.30%)\n",
      "\n",
      "Iteration 1200, loss = 1.3939\n",
      "Got 544 / 1000 correct (54.40%)\n",
      "Got 529 / 1000 correct (52.90%)\n",
      "\n",
      "Iteration 1300, loss = 1.1867\n",
      "Got 546 / 1000 correct (54.60%)\n",
      "Got 516 / 1000 correct (51.60%)\n",
      "\n",
      "Iteration 1400, loss = 1.3210\n",
      "Got 549 / 1000 correct (54.90%)\n",
      "Got 530 / 1000 correct (53.00%)\n",
      "\n",
      "Iteration 1500, loss = 1.1586\n",
      "Got 543 / 1000 correct (54.30%)\n",
      "Got 537 / 1000 correct (53.70%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 1.1318\n",
      "Got 606 / 1000 correct (60.60%)\n",
      "Got 537 / 1000 correct (53.70%)\n",
      "\n",
      "Iteration 1700, loss = 1.0998\n",
      "Got 580 / 1000 correct (58.00%)\n",
      "Got 527 / 1000 correct (52.70%)\n",
      "\n",
      "Iteration 1800, loss = 0.9886\n",
      "Got 589 / 1000 correct (58.90%)\n",
      "Got 552 / 1000 correct (55.20%)\n",
      "\n",
      "Iteration 1900, loss = 1.1562\n",
      "Got 584 / 1000 correct (58.40%)\n",
      "Got 582 / 1000 correct (58.20%)\n",
      "\n",
      "Iteration 2000, loss = 1.1255\n",
      "Got 588 / 1000 correct (58.80%)\n",
      "Got 584 / 1000 correct (58.40%)\n",
      "\n",
      "Iteration 2100, loss = 1.1251\n",
      "Got 592 / 1000 correct (59.20%)\n",
      "Got 582 / 1000 correct (58.20%)\n",
      "\n",
      "Iteration 2200, loss = 1.2378\n",
      "Got 623 / 1000 correct (62.30%)\n",
      "Got 621 / 1000 correct (62.10%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2300, loss = 1.0655\n",
      "Got 623 / 1000 correct (62.30%)\n",
      "Got 603 / 1000 correct (60.30%)\n",
      "\n",
      "Iteration 2400, loss = 0.9420\n",
      "Got 656 / 1000 correct (65.60%)\n",
      "Got 592 / 1000 correct (59.20%)\n",
      "\n",
      "Iteration 2500, loss = 1.0325\n",
      "Got 654 / 1000 correct (65.40%)\n",
      "Got 625 / 1000 correct (62.50%)\n",
      "\n",
      "Iteration 2600, loss = 1.0724\n",
      "Got 622 / 1000 correct (62.20%)\n",
      "Got 590 / 1000 correct (59.00%)\n",
      "\n",
      "Iteration 2700, loss = 1.0336\n",
      "Got 608 / 1000 correct (60.80%)\n",
      "Got 614 / 1000 correct (61.40%)\n",
      "\n",
      "Iteration 2800, loss = 1.2407\n",
      "Got 637 / 1000 correct (63.70%)\n",
      "Got 613 / 1000 correct (61.30%)\n",
      "\n",
      "Iteration 2900, loss = 0.6258\n",
      "Got 649 / 1000 correct (64.90%)\n",
      "Got 628 / 1000 correct (62.80%)\n",
      "\n",
      "Iteration 3000, loss = 1.0971\n",
      "Got 645 / 1000 correct (64.50%)\n",
      "Got 613 / 1000 correct (61.30%)\n",
      "\n",
      "Starting epoch 4\n",
      "Iteration 3100, loss = 1.0782\n",
      "Got 731 / 1000 correct (73.10%)\n",
      "Got 611 / 1000 correct (61.10%)\n",
      "\n",
      "Iteration 3200, loss = 1.1062\n",
      "Got 676 / 1000 correct (67.60%)\n",
      "Got 607 / 1000 correct (60.70%)\n",
      "\n",
      "Iteration 3300, loss = 0.8468\n",
      "Got 651 / 1000 correct (65.10%)\n",
      "Got 590 / 1000 correct (59.00%)\n",
      "\n",
      "Iteration 3400, loss = 1.1544\n",
      "Got 640 / 1000 correct (64.00%)\n",
      "Got 602 / 1000 correct (60.20%)\n",
      "\n",
      "Iteration 3500, loss = 1.0159\n",
      "Got 663 / 1000 correct (66.30%)\n",
      "Got 620 / 1000 correct (62.00%)\n",
      "\n",
      "Iteration 3600, loss = 1.1701\n",
      "Got 676 / 1000 correct (67.60%)\n",
      "Got 633 / 1000 correct (63.30%)\n",
      "\n",
      "Iteration 3700, loss = 0.9951\n",
      "Got 676 / 1000 correct (67.60%)\n",
      "Got 636 / 1000 correct (63.60%)\n",
      "\n",
      "Iteration 3800, loss = 0.8423\n",
      "Got 650 / 1000 correct (65.00%)\n",
      "Got 619 / 1000 correct (61.90%)\n",
      "\n",
      "Accuracy of final model on the test set:\n",
      "Got 6158 / 10000 correct (61.58%)\n"
     ]
    }
   ],
   "source": [
    "# INPUT -> [CONV -> BATCHNORM -> RELU -> POOL]*2 -> FC -> RELU -> FC\n",
    "# add in batch norm\n",
    "\n",
    "def four_layer_conv_functional(inputs, channel_1, channel_2, hidden_size, num_classes):     \n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    conv1 = tf.layers.conv2d(inputs, filters=channel_1, kernel_size=5, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch1 = tf.layers.batch_normalization(conv1)\n",
    "    relu1 = tf.nn.relu(batch1)\n",
    "    pool1 = tf.layers.max_pooling2d(relu1, pool_size=2, strides=2)\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=channel_2, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch2 = tf.layers.batch_normalization(conv2)\n",
    "    relu2 = tf.nn.relu(batch2)\n",
    "    pool2 = tf.layers.max_pooling2d(relu2, pool_size=2, strides=2)\n",
    "    flattened = tf.layers.flatten(pool2)\n",
    "    fc1 = tf.layers.dense(flattened, hidden_size, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "    scores = tf.layers.dense(fc1, num_classes,\n",
    "                             kernel_initializer=initializer)\n",
    "    return scores\n",
    "\n",
    "channel_1, channel_2, hidden_size, num_classes = 32, 16, 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return four_layer_conv_functional(inputs, channel_1, channel_2, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 5\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kind of plateaued, not sure if it will make it to 70% even with more epochs. Try 3 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.8032\n",
      "Got 134 / 1000 correct (13.40%)\n",
      "Got 143 / 1000 correct (14.30%)\n",
      "\n",
      "Iteration 100, loss = 2.2496\n",
      "Got 146 / 1000 correct (14.60%)\n",
      "Got 151 / 1000 correct (15.10%)\n",
      "\n",
      "Iteration 200, loss = 2.0878\n",
      "Got 180 / 1000 correct (18.00%)\n",
      "Got 196 / 1000 correct (19.60%)\n",
      "\n",
      "Iteration 300, loss = 1.9677\n",
      "Got 279 / 1000 correct (27.90%)\n",
      "Got 262 / 1000 correct (26.20%)\n",
      "\n",
      "Iteration 400, loss = 1.6859\n",
      "Got 348 / 1000 correct (34.80%)\n",
      "Got 367 / 1000 correct (36.70%)\n",
      "\n",
      "Iteration 500, loss = 1.8151\n",
      "Got 378 / 1000 correct (37.80%)\n",
      "Got 401 / 1000 correct (40.10%)\n",
      "\n",
      "Iteration 600, loss = 1.6249\n",
      "Got 415 / 1000 correct (41.50%)\n",
      "Got 416 / 1000 correct (41.60%)\n",
      "\n",
      "Iteration 700, loss = 1.5180\n",
      "Got 445 / 1000 correct (44.50%)\n",
      "Got 461 / 1000 correct (46.10%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 1.5285\n",
      "Got 532 / 1000 correct (53.20%)\n",
      "Got 505 / 1000 correct (50.50%)\n",
      "\n",
      "Iteration 900, loss = 1.2633\n",
      "Got 492 / 1000 correct (49.20%)\n",
      "Got 503 / 1000 correct (50.30%)\n",
      "\n",
      "Iteration 1000, loss = 1.3243\n",
      "Got 523 / 1000 correct (52.30%)\n",
      "Got 506 / 1000 correct (50.60%)\n",
      "\n",
      "Iteration 1100, loss = 1.4181\n",
      "Got 555 / 1000 correct (55.50%)\n",
      "Got 533 / 1000 correct (53.30%)\n",
      "\n",
      "Iteration 1200, loss = 1.1730\n",
      "Got 543 / 1000 correct (54.30%)\n",
      "Got 550 / 1000 correct (55.00%)\n",
      "\n",
      "Iteration 1300, loss = 1.2066\n",
      "Got 525 / 1000 correct (52.50%)\n",
      "Got 556 / 1000 correct (55.60%)\n",
      "\n",
      "Iteration 1400, loss = 1.2910\n",
      "Got 554 / 1000 correct (55.40%)\n",
      "Got 556 / 1000 correct (55.60%)\n",
      "\n",
      "Iteration 1500, loss = 1.1894\n",
      "Got 550 / 1000 correct (55.00%)\n",
      "Got 571 / 1000 correct (57.10%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 1.1109\n",
      "Got 578 / 1000 correct (57.80%)\n",
      "Got 557 / 1000 correct (55.70%)\n",
      "\n",
      "Iteration 1700, loss = 1.0051\n",
      "Got 595 / 1000 correct (59.50%)\n",
      "Got 565 / 1000 correct (56.50%)\n",
      "\n",
      "Iteration 1800, loss = 0.9642\n",
      "Got 600 / 1000 correct (60.00%)\n",
      "Got 581 / 1000 correct (58.10%)\n",
      "\n",
      "Iteration 1900, loss = 1.1292\n",
      "Got 610 / 1000 correct (61.00%)\n",
      "Got 588 / 1000 correct (58.80%)\n",
      "\n",
      "Iteration 2000, loss = 0.9842\n",
      "Got 603 / 1000 correct (60.30%)\n",
      "Got 585 / 1000 correct (58.50%)\n",
      "\n",
      "Iteration 2100, loss = 0.9680\n",
      "Got 607 / 1000 correct (60.70%)\n",
      "Got 595 / 1000 correct (59.50%)\n",
      "\n",
      "Iteration 2200, loss = 1.0706\n",
      "Got 625 / 1000 correct (62.50%)\n",
      "Got 617 / 1000 correct (61.70%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2300, loss = 1.0895\n",
      "Got 650 / 1000 correct (65.00%)\n",
      "Got 627 / 1000 correct (62.70%)\n",
      "\n",
      "Iteration 2400, loss = 1.0349\n",
      "Got 673 / 1000 correct (67.30%)\n",
      "Got 621 / 1000 correct (62.10%)\n",
      "\n",
      "Iteration 2500, loss = 1.1048\n",
      "Got 648 / 1000 correct (64.80%)\n",
      "Got 631 / 1000 correct (63.10%)\n",
      "\n",
      "Iteration 2600, loss = 0.9994\n",
      "Got 635 / 1000 correct (63.50%)\n",
      "Got 596 / 1000 correct (59.60%)\n",
      "\n",
      "Iteration 2700, loss = 1.0352\n",
      "Got 621 / 1000 correct (62.10%)\n",
      "Got 614 / 1000 correct (61.40%)\n",
      "\n",
      "Iteration 2800, loss = 1.0632\n",
      "Got 649 / 1000 correct (64.90%)\n",
      "Got 618 / 1000 correct (61.80%)\n",
      "\n",
      "Iteration 2900, loss = 0.6640\n",
      "Got 655 / 1000 correct (65.50%)\n",
      "Got 642 / 1000 correct (64.20%)\n",
      "\n",
      "Iteration 3000, loss = 0.9050\n",
      "Got 647 / 1000 correct (64.70%)\n",
      "Got 626 / 1000 correct (62.60%)\n",
      "\n",
      "Starting epoch 4\n",
      "Iteration 3100, loss = 0.7845\n",
      "Got 724 / 1000 correct (72.40%)\n",
      "Got 645 / 1000 correct (64.50%)\n",
      "\n",
      "Iteration 3200, loss = 1.0037\n",
      "Got 675 / 1000 correct (67.50%)\n",
      "Got 636 / 1000 correct (63.60%)\n",
      "\n",
      "Iteration 3300, loss = 1.0279\n",
      "Got 673 / 1000 correct (67.30%)\n",
      "Got 645 / 1000 correct (64.50%)\n",
      "\n",
      "Iteration 3400, loss = 1.1231\n",
      "Got 692 / 1000 correct (69.20%)\n",
      "Got 623 / 1000 correct (62.30%)\n",
      "\n",
      "Iteration 3500, loss = 0.9003\n",
      "Got 689 / 1000 correct (68.90%)\n",
      "Got 633 / 1000 correct (63.30%)\n",
      "\n",
      "Iteration 3600, loss = 1.0996\n",
      "Got 692 / 1000 correct (69.20%)\n",
      "Got 647 / 1000 correct (64.70%)\n",
      "\n",
      "Iteration 3700, loss = 0.8336\n",
      "Got 669 / 1000 correct (66.90%)\n",
      "Got 649 / 1000 correct (64.90%)\n",
      "\n",
      "Iteration 3800, loss = 0.8844\n",
      "Got 670 / 1000 correct (67.00%)\n",
      "Got 646 / 1000 correct (64.60%)\n",
      "\n",
      "Accuracy of final model on the test set:\n",
      "Got 6416 / 10000 correct (64.16%)\n"
     ]
    }
   ],
   "source": [
    "# INPUT -> [CONV -> BATCHNORM -> RELU -> POOL]*3 -> FC -> RELU -> FC\n",
    "# add in batch norm\n",
    "\n",
    "def five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes):     \n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    conv1 = tf.layers.conv2d(inputs, filters=channel_1, kernel_size=5, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch1 = tf.layers.batch_normalization(conv1)\n",
    "    relu1 = tf.nn.relu(batch1)\n",
    "    pool1 = tf.layers.max_pooling2d(relu1, pool_size=2, strides=2)\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=channel_2, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch2 = tf.layers.batch_normalization(conv2)\n",
    "    relu2 = tf.nn.relu(batch2)\n",
    "    pool2 = tf.layers.max_pooling2d(relu2, pool_size=2, strides=2)\n",
    "    conv3 = tf.layers.conv2d(pool2, filters=channel_3, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch3 = tf.layers.batch_normalization(conv3)\n",
    "    relu3 = tf.nn.relu(batch3)\n",
    "    pool3 = tf.layers.max_pooling2d(relu3, pool_size=2, strides=2)\n",
    "    flattened = tf.layers.flatten(pool3)\n",
    "    fc1 = tf.layers.dense(flattened, hidden_size, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "    scores = tf.layers.dense(fc1, num_classes,\n",
    "                             kernel_initializer=initializer)\n",
    "    return scores\n",
    "\n",
    "channel_1, channel_2, channel_3, hidden_size, num_classes = 32, 16, 16, 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 5\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64.16% on test set after training for 5 epochs. Seems close, try 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 3.1653\n",
      "Got 99 / 1000 correct (9.90%)\n",
      "Got 82 / 1000 correct (8.20%)\n",
      "\n",
      "Iteration 100, loss = 2.3048\n",
      "Got 99 / 1000 correct (9.90%)\n",
      "Got 113 / 1000 correct (11.30%)\n",
      "\n",
      "Iteration 200, loss = 2.3034\n",
      "Got 107 / 1000 correct (10.70%)\n",
      "Got 102 / 1000 correct (10.20%)\n",
      "\n",
      "Iteration 300, loss = 2.2998\n",
      "Got 99 / 1000 correct (9.90%)\n",
      "Got 79 / 1000 correct (7.90%)\n",
      "\n",
      "Iteration 400, loss = 2.3015\n",
      "Got 112 / 1000 correct (11.20%)\n",
      "Got 119 / 1000 correct (11.90%)\n",
      "\n",
      "Iteration 500, loss = 2.3054\n",
      "Got 85 / 1000 correct (8.50%)\n",
      "Got 98 / 1000 correct (9.80%)\n",
      "\n",
      "Iteration 600, loss = 2.3039\n",
      "Got 99 / 1000 correct (9.90%)\n",
      "Got 79 / 1000 correct (7.90%)\n",
      "\n",
      "Iteration 700, loss = 2.2990\n",
      "Got 103 / 1000 correct (10.30%)\n",
      "Got 87 / 1000 correct (8.70%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 2.2989\n",
      "Got 160 / 1000 correct (16.00%)\n",
      "Got 137 / 1000 correct (13.70%)\n",
      "\n",
      "Iteration 900, loss = 2.2058\n",
      "Got 178 / 1000 correct (17.80%)\n",
      "Got 164 / 1000 correct (16.40%)\n",
      "\n",
      "Iteration 1000, loss = 2.0491\n",
      "Got 211 / 1000 correct (21.10%)\n",
      "Got 200 / 1000 correct (20.00%)\n",
      "\n",
      "Iteration 1100, loss = 1.9582\n",
      "Got 250 / 1000 correct (25.00%)\n",
      "Got 249 / 1000 correct (24.90%)\n",
      "\n",
      "Iteration 1200, loss = 1.7835\n",
      "Got 301 / 1000 correct (30.10%)\n",
      "Got 298 / 1000 correct (29.80%)\n",
      "\n",
      "Iteration 1300, loss = 1.9513\n",
      "Got 314 / 1000 correct (31.40%)\n",
      "Got 328 / 1000 correct (32.80%)\n",
      "\n",
      "Iteration 1400, loss = 1.7374\n",
      "Got 382 / 1000 correct (38.20%)\n",
      "Got 372 / 1000 correct (37.20%)\n",
      "\n",
      "Iteration 1500, loss = 1.5571\n",
      "Got 382 / 1000 correct (38.20%)\n",
      "Got 395 / 1000 correct (39.50%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 1.2430\n",
      "Got 454 / 1000 correct (45.40%)\n",
      "Got 450 / 1000 correct (45.00%)\n",
      "\n",
      "Iteration 1700, loss = 1.4040\n",
      "Got 486 / 1000 correct (48.60%)\n",
      "Got 460 / 1000 correct (46.00%)\n",
      "\n",
      "Iteration 1800, loss = 1.4128\n",
      "Got 474 / 1000 correct (47.40%)\n",
      "Got 519 / 1000 correct (51.90%)\n",
      "\n",
      "Iteration 1900, loss = 1.1589\n",
      "Got 501 / 1000 correct (50.10%)\n",
      "Got 501 / 1000 correct (50.10%)\n",
      "\n",
      "Iteration 2000, loss = 1.2543\n",
      "Got 500 / 1000 correct (50.00%)\n",
      "Got 510 / 1000 correct (51.00%)\n",
      "\n",
      "Iteration 2100, loss = 1.2123\n",
      "Got 532 / 1000 correct (53.20%)\n",
      "Got 521 / 1000 correct (52.10%)\n",
      "\n",
      "Iteration 2200, loss = 1.2987\n",
      "Got 517 / 1000 correct (51.70%)\n",
      "Got 523 / 1000 correct (52.30%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2300, loss = 1.3165\n",
      "Got 558 / 1000 correct (55.80%)\n",
      "Got 547 / 1000 correct (54.70%)\n",
      "\n",
      "Iteration 2400, loss = 1.2763\n",
      "Got 547 / 1000 correct (54.70%)\n",
      "Got 542 / 1000 correct (54.20%)\n",
      "\n",
      "Iteration 2500, loss = 1.3705\n",
      "Got 570 / 1000 correct (57.00%)\n",
      "Got 577 / 1000 correct (57.70%)\n",
      "\n",
      "Iteration 2600, loss = 1.0634\n",
      "Got 578 / 1000 correct (57.80%)\n",
      "Got 561 / 1000 correct (56.10%)\n",
      "\n",
      "Iteration 2700, loss = 1.2329\n",
      "Got 540 / 1000 correct (54.00%)\n",
      "Got 551 / 1000 correct (55.10%)\n",
      "\n",
      "Iteration 2800, loss = 1.1361\n",
      "Got 594 / 1000 correct (59.40%)\n",
      "Got 575 / 1000 correct (57.50%)\n",
      "\n",
      "Iteration 2900, loss = 0.9021\n",
      "Got 577 / 1000 correct (57.70%)\n",
      "Got 595 / 1000 correct (59.50%)\n",
      "\n",
      "Iteration 3000, loss = 1.0773\n",
      "Got 595 / 1000 correct (59.50%)\n",
      "Got 580 / 1000 correct (58.00%)\n",
      "\n",
      "Starting epoch 4\n",
      "Iteration 3100, loss = 1.0669\n",
      "Got 651 / 1000 correct (65.10%)\n",
      "Got 593 / 1000 correct (59.30%)\n",
      "\n",
      "Iteration 3200, loss = 1.1100\n",
      "Got 617 / 1000 correct (61.70%)\n",
      "Got 603 / 1000 correct (60.30%)\n",
      "\n",
      "Iteration 3300, loss = 1.1845\n",
      "Got 599 / 1000 correct (59.90%)\n",
      "Got 590 / 1000 correct (59.00%)\n",
      "\n",
      "Iteration 3400, loss = 1.0362\n",
      "Got 643 / 1000 correct (64.30%)\n",
      "Got 611 / 1000 correct (61.10%)\n",
      "\n",
      "Iteration 3500, loss = 0.8907\n",
      "Got 614 / 1000 correct (61.40%)\n",
      "Got 611 / 1000 correct (61.10%)\n",
      "\n",
      "Iteration 3600, loss = 1.1541\n",
      "Got 637 / 1000 correct (63.70%)\n",
      "Got 611 / 1000 correct (61.10%)\n",
      "\n",
      "Iteration 3700, loss = 1.1316\n",
      "Got 623 / 1000 correct (62.30%)\n",
      "Got 623 / 1000 correct (62.30%)\n",
      "\n",
      "Iteration 3800, loss = 0.8184\n",
      "Got 635 / 1000 correct (63.50%)\n",
      "Got 627 / 1000 correct (62.70%)\n",
      "\n",
      "Starting epoch 5\n",
      "Iteration 3900, loss = 1.1648\n",
      "Got 663 / 1000 correct (66.30%)\n",
      "Got 644 / 1000 correct (64.40%)\n",
      "\n",
      "Iteration 4000, loss = 1.0993\n",
      "Got 667 / 1000 correct (66.70%)\n",
      "Got 638 / 1000 correct (63.80%)\n",
      "\n",
      "Iteration 4100, loss = 0.9861\n",
      "Got 645 / 1000 correct (64.50%)\n",
      "Got 613 / 1000 correct (61.30%)\n",
      "\n",
      "Iteration 4200, loss = 0.6722\n",
      "Got 634 / 1000 correct (63.40%)\n",
      "Got 643 / 1000 correct (64.30%)\n",
      "\n",
      "Iteration 4300, loss = 0.9174\n",
      "Got 639 / 1000 correct (63.90%)\n",
      "Got 629 / 1000 correct (62.90%)\n",
      "\n",
      "Iteration 4400, loss = 0.9313\n",
      "Got 657 / 1000 correct (65.70%)\n",
      "Got 633 / 1000 correct (63.30%)\n",
      "\n",
      "Iteration 4500, loss = 1.0202\n",
      "Got 676 / 1000 correct (67.60%)\n",
      "Got 663 / 1000 correct (66.30%)\n",
      "\n",
      "Starting epoch 6\n",
      "Iteration 4600, loss = 0.7860\n",
      "Got 693 / 1000 correct (69.30%)\n",
      "Got 650 / 1000 correct (65.00%)\n",
      "\n",
      "Iteration 4700, loss = 1.0957\n",
      "Got 714 / 1000 correct (71.40%)\n",
      "Got 648 / 1000 correct (64.80%)\n",
      "\n",
      "Iteration 4800, loss = 0.8376\n",
      "Got 701 / 1000 correct (70.10%)\n",
      "Got 655 / 1000 correct (65.50%)\n",
      "\n",
      "Iteration 4900, loss = 0.7474\n",
      "Got 688 / 1000 correct (68.80%)\n",
      "Got 665 / 1000 correct (66.50%)\n",
      "\n",
      "Iteration 5000, loss = 0.9252\n",
      "Got 675 / 1000 correct (67.50%)\n",
      "Got 652 / 1000 correct (65.20%)\n",
      "\n",
      "Iteration 5100, loss = 0.9133\n",
      "Got 664 / 1000 correct (66.40%)\n",
      "Got 660 / 1000 correct (66.00%)\n",
      "\n",
      "Iteration 5200, loss = 0.8727\n",
      "Got 687 / 1000 correct (68.70%)\n",
      "Got 677 / 1000 correct (67.70%)\n",
      "\n",
      "Iteration 5300, loss = 0.9654\n",
      "Got 674 / 1000 correct (67.40%)\n",
      "Got 668 / 1000 correct (66.80%)\n",
      "\n",
      "Starting epoch 7\n",
      "Iteration 5400, loss = 0.6242\n",
      "Got 755 / 1000 correct (75.50%)\n",
      "Got 676 / 1000 correct (67.60%)\n",
      "\n",
      "Iteration 5500, loss = 0.7516\n",
      "Got 727 / 1000 correct (72.70%)\n",
      "Got 678 / 1000 correct (67.80%)\n",
      "\n",
      "Iteration 5600, loss = 0.6915\n",
      "Got 699 / 1000 correct (69.90%)\n",
      "Got 678 / 1000 correct (67.80%)\n",
      "\n",
      "Iteration 5700, loss = 0.7606\n",
      "Got 711 / 1000 correct (71.10%)\n",
      "Got 666 / 1000 correct (66.60%)\n",
      "\n",
      "Iteration 5800, loss = 0.8411\n",
      "Got 693 / 1000 correct (69.30%)\n",
      "Got 686 / 1000 correct (68.60%)\n",
      "\n",
      "Iteration 5900, loss = 0.8647\n",
      "Got 707 / 1000 correct (70.70%)\n",
      "Got 697 / 1000 correct (69.70%)\n",
      "\n",
      "Iteration 6000, loss = 0.9055\n",
      "Got 714 / 1000 correct (71.40%)\n",
      "Got 691 / 1000 correct (69.10%)\n",
      "\n",
      "Iteration 6100, loss = 0.8136\n",
      "Got 693 / 1000 correct (69.30%)\n",
      "Got 683 / 1000 correct (68.30%)\n",
      "\n",
      "Starting epoch 8\n",
      "Iteration 6200, loss = 0.8095\n",
      "Got 764 / 1000 correct (76.40%)\n",
      "Got 674 / 1000 correct (67.40%)\n",
      "\n",
      "Iteration 6300, loss = 0.8881\n",
      "Got 735 / 1000 correct (73.50%)\n",
      "Got 669 / 1000 correct (66.90%)\n",
      "\n",
      "Iteration 6400, loss = 0.8649\n",
      "Got 731 / 1000 correct (73.10%)\n",
      "Got 676 / 1000 correct (67.60%)\n",
      "\n",
      "Iteration 6500, loss = 0.6710\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "Got 663 / 1000 correct (66.30%)\n",
      "\n",
      "Iteration 6600, loss = 0.5557\n",
      "Got 713 / 1000 correct (71.30%)\n",
      "Got 694 / 1000 correct (69.40%)\n",
      "\n",
      "Iteration 6700, loss = 0.7174\n",
      "Got 708 / 1000 correct (70.80%)\n",
      "Got 679 / 1000 correct (67.90%)\n",
      "\n",
      "Iteration 6800, loss = 0.5718\n",
      "Got 729 / 1000 correct (72.90%)\n",
      "Got 692 / 1000 correct (69.20%)\n",
      "\n",
      "Starting epoch 9\n",
      "Iteration 6900, loss = 0.6019\n",
      "Got 770 / 1000 correct (77.00%)\n",
      "Got 697 / 1000 correct (69.70%)\n",
      "\n",
      "Iteration 7000, loss = 0.5580\n",
      "Got 752 / 1000 correct (75.20%)\n",
      "Got 674 / 1000 correct (67.40%)\n",
      "\n",
      "Iteration 7100, loss = 0.7505\n",
      "Got 734 / 1000 correct (73.40%)\n",
      "Got 697 / 1000 correct (69.70%)\n",
      "\n",
      "Iteration 7200, loss = 0.6334\n",
      "Got 738 / 1000 correct (73.80%)\n",
      "Got 696 / 1000 correct (69.60%)\n",
      "\n",
      "Iteration 7300, loss = 0.5910\n",
      "Got 719 / 1000 correct (71.90%)\n",
      "Got 689 / 1000 correct (68.90%)\n",
      "\n",
      "Iteration 7400, loss = 0.7581\n",
      "Got 726 / 1000 correct (72.60%)\n",
      "Got 695 / 1000 correct (69.50%)\n",
      "\n",
      "Iteration 7500, loss = 0.4897\n",
      "Got 723 / 1000 correct (72.30%)\n",
      "Got 694 / 1000 correct (69.40%)\n",
      "\n",
      "Iteration 7600, loss = 0.7537\n",
      "Got 723 / 1000 correct (72.30%)\n",
      "Got 680 / 1000 correct (68.00%)\n",
      "\n",
      "Accuracy of final model on the test set:\n",
      "Got 6799 / 10000 correct (67.99%)\n"
     ]
    }
   ],
   "source": [
    "# INPUT -> [CONV -> BATCHNORM -> RELU -> POOL]*3 -> FC -> RELU -> FC\n",
    "# add in batch norm\n",
    "\n",
    "def five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes):     \n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    conv1 = tf.layers.conv2d(inputs, filters=channel_1, kernel_size=5, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch1 = tf.layers.batch_normalization(conv1)\n",
    "    relu1 = tf.nn.relu(batch1)\n",
    "    pool1 = tf.layers.max_pooling2d(relu1, pool_size=2, strides=2)\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=channel_2, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch2 = tf.layers.batch_normalization(conv2)\n",
    "    relu2 = tf.nn.relu(batch2)\n",
    "    pool2 = tf.layers.max_pooling2d(relu2, pool_size=2, strides=2)\n",
    "    conv3 = tf.layers.conv2d(pool2, filters=channel_3, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch3 = tf.layers.batch_normalization(conv3)\n",
    "    relu3 = tf.nn.relu(batch3)\n",
    "    pool3 = tf.layers.max_pooling2d(relu3, pool_size=2, strides=2)\n",
    "    flattened = tf.layers.flatten(pool3)\n",
    "    fc1 = tf.layers.dense(flattened, hidden_size, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "    scores = tf.layers.dense(fc1, num_classes,\n",
    "                             kernel_initializer=initializer)\n",
    "    return scores\n",
    "\n",
    "channel_1, channel_2, channel_3, hidden_size, num_classes = 32, 16, 16, 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 10\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close, but underfitting. Try adding more channels/weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 3.3091\n",
      "Got 100 / 1000 correct (10.00%)\n",
      "Got 80 / 1000 correct (8.00%)\n",
      "\n",
      "Iteration 100, loss = 2.2342\n",
      "Got 136 / 1000 correct (13.60%)\n",
      "Got 163 / 1000 correct (16.30%)\n",
      "\n",
      "Iteration 200, loss = 2.1228\n",
      "Got 140 / 1000 correct (14.00%)\n",
      "Got 146 / 1000 correct (14.60%)\n",
      "\n",
      "Iteration 300, loss = 2.1292\n",
      "Got 190 / 1000 correct (19.00%)\n",
      "Got 225 / 1000 correct (22.50%)\n",
      "\n",
      "Iteration 400, loss = 2.0205\n",
      "Got 263 / 1000 correct (26.30%)\n",
      "Got 283 / 1000 correct (28.30%)\n",
      "\n",
      "Iteration 500, loss = 1.8703\n",
      "Got 326 / 1000 correct (32.60%)\n",
      "Got 325 / 1000 correct (32.50%)\n",
      "\n",
      "Iteration 600, loss = 1.7901\n",
      "Got 319 / 1000 correct (31.90%)\n",
      "Got 334 / 1000 correct (33.40%)\n",
      "\n",
      "Iteration 700, loss = 1.6125\n",
      "Got 371 / 1000 correct (37.10%)\n",
      "Got 407 / 1000 correct (40.70%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 1.6379\n",
      "Got 452 / 1000 correct (45.20%)\n",
      "Got 468 / 1000 correct (46.80%)\n",
      "\n",
      "Iteration 900, loss = 1.5847\n",
      "Got 454 / 1000 correct (45.40%)\n",
      "Got 462 / 1000 correct (46.20%)\n",
      "\n",
      "Iteration 1000, loss = 1.5216\n",
      "Got 463 / 1000 correct (46.30%)\n",
      "Got 466 / 1000 correct (46.60%)\n",
      "\n",
      "Iteration 1100, loss = 1.4331\n",
      "Got 513 / 1000 correct (51.30%)\n",
      "Got 536 / 1000 correct (53.60%)\n",
      "\n",
      "Iteration 1200, loss = 1.3295\n",
      "Got 517 / 1000 correct (51.70%)\n",
      "Got 542 / 1000 correct (54.20%)\n",
      "\n",
      "Iteration 1300, loss = 1.2701\n",
      "Got 554 / 1000 correct (55.40%)\n",
      "Got 559 / 1000 correct (55.90%)\n",
      "\n",
      "Iteration 1400, loss = 1.2821\n",
      "Got 559 / 1000 correct (55.90%)\n",
      "Got 563 / 1000 correct (56.30%)\n",
      "\n",
      "Iteration 1500, loss = 1.2113\n",
      "Got 568 / 1000 correct (56.80%)\n",
      "Got 558 / 1000 correct (55.80%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 0.9902\n",
      "Got 592 / 1000 correct (59.20%)\n",
      "Got 580 / 1000 correct (58.00%)\n",
      "\n",
      "Iteration 1700, loss = 1.2115\n",
      "Got 588 / 1000 correct (58.80%)\n",
      "Got 590 / 1000 correct (59.00%)\n",
      "\n",
      "Iteration 1800, loss = 0.9701\n",
      "Got 599 / 1000 correct (59.90%)\n",
      "Got 606 / 1000 correct (60.60%)\n",
      "\n",
      "Iteration 1900, loss = 0.9736\n",
      "Got 617 / 1000 correct (61.70%)\n",
      "Got 598 / 1000 correct (59.80%)\n",
      "\n",
      "Iteration 2000, loss = 0.9351\n",
      "Got 628 / 1000 correct (62.80%)\n",
      "Got 626 / 1000 correct (62.60%)\n",
      "\n",
      "Iteration 2100, loss = 1.1465\n",
      "Got 613 / 1000 correct (61.30%)\n",
      "Got 614 / 1000 correct (61.40%)\n",
      "\n",
      "Iteration 2200, loss = 1.1112\n",
      "Got 626 / 1000 correct (62.60%)\n",
      "Got 629 / 1000 correct (62.90%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2300, loss = 1.0277\n",
      "Got 631 / 1000 correct (63.10%)\n",
      "Got 647 / 1000 correct (64.70%)\n",
      "\n",
      "Iteration 2400, loss = 1.0167\n",
      "Got 686 / 1000 correct (68.60%)\n",
      "Got 638 / 1000 correct (63.80%)\n",
      "\n",
      "Iteration 2500, loss = 1.1221\n",
      "Got 674 / 1000 correct (67.40%)\n",
      "Got 661 / 1000 correct (66.10%)\n",
      "\n",
      "Iteration 2600, loss = 0.8657\n",
      "Got 677 / 1000 correct (67.70%)\n",
      "Got 637 / 1000 correct (63.70%)\n",
      "\n",
      "Iteration 2700, loss = 0.9663\n",
      "Got 640 / 1000 correct (64.00%)\n",
      "Got 652 / 1000 correct (65.20%)\n",
      "\n",
      "Iteration 2800, loss = 1.0622\n",
      "Got 663 / 1000 correct (66.30%)\n",
      "Got 658 / 1000 correct (65.80%)\n",
      "\n",
      "Iteration 2900, loss = 0.6307\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "Got 676 / 1000 correct (67.60%)\n",
      "\n",
      "Iteration 3000, loss = 1.0676\n",
      "Got 643 / 1000 correct (64.30%)\n",
      "Got 659 / 1000 correct (65.90%)\n",
      "\n",
      "Starting epoch 4\n",
      "Iteration 3100, loss = 0.8642\n",
      "Got 768 / 1000 correct (76.80%)\n",
      "Got 681 / 1000 correct (68.10%)\n",
      "\n",
      "Iteration 3200, loss = 0.9429\n",
      "Got 731 / 1000 correct (73.10%)\n",
      "Got 693 / 1000 correct (69.30%)\n",
      "\n",
      "Iteration 3300, loss = 0.8881\n",
      "Got 700 / 1000 correct (70.00%)\n",
      "Got 677 / 1000 correct (67.70%)\n",
      "\n",
      "Iteration 3400, loss = 0.9725\n",
      "Got 705 / 1000 correct (70.50%)\n",
      "Got 675 / 1000 correct (67.50%)\n",
      "\n",
      "Iteration 3500, loss = 0.9389\n",
      "Got 707 / 1000 correct (70.70%)\n",
      "Got 681 / 1000 correct (68.10%)\n",
      "\n",
      "Iteration 3600, loss = 0.9941\n",
      "Got 713 / 1000 correct (71.30%)\n",
      "Got 681 / 1000 correct (68.10%)\n",
      "\n",
      "Iteration 3700, loss = 0.8351\n",
      "Got 710 / 1000 correct (71.00%)\n",
      "Got 710 / 1000 correct (71.00%)\n",
      "\n",
      "Iteration 3800, loss = 0.7890\n",
      "Got 686 / 1000 correct (68.60%)\n",
      "Got 682 / 1000 correct (68.20%)\n",
      "\n",
      "Starting epoch 5\n",
      "Iteration 3900, loss = 1.0535\n",
      "Got 784 / 1000 correct (78.40%)\n",
      "Got 703 / 1000 correct (70.30%)\n",
      "\n",
      "Iteration 4000, loss = 0.8688\n",
      "Got 760 / 1000 correct (76.00%)\n",
      "Got 717 / 1000 correct (71.70%)\n",
      "\n",
      "Iteration 4100, loss = 0.9959\n",
      "Got 742 / 1000 correct (74.20%)\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "\n",
      "Iteration 4200, loss = 0.6300\n",
      "Got 751 / 1000 correct (75.10%)\n",
      "Got 698 / 1000 correct (69.80%)\n",
      "\n",
      "Iteration 4300, loss = 0.6078\n",
      "Got 738 / 1000 correct (73.80%)\n",
      "Got 707 / 1000 correct (70.70%)\n",
      "\n",
      "Iteration 4400, loss = 0.6133\n",
      "Got 718 / 1000 correct (71.80%)\n",
      "Got 697 / 1000 correct (69.70%)\n",
      "\n",
      "Iteration 4500, loss = 0.7852\n",
      "Got 741 / 1000 correct (74.10%)\n",
      "Got 713 / 1000 correct (71.30%)\n",
      "\n",
      "Starting epoch 6\n",
      "Iteration 4600, loss = 0.7088\n",
      "Got 790 / 1000 correct (79.00%)\n",
      "Got 687 / 1000 correct (68.70%)\n",
      "\n",
      "Iteration 4700, loss = 0.6910\n",
      "Got 763 / 1000 correct (76.30%)\n",
      "Got 677 / 1000 correct (67.70%)\n",
      "\n",
      "Iteration 4800, loss = 0.7946\n",
      "Got 759 / 1000 correct (75.90%)\n",
      "Got 702 / 1000 correct (70.20%)\n",
      "\n",
      "Iteration 4900, loss = 0.5402\n",
      "Got 778 / 1000 correct (77.80%)\n",
      "Got 711 / 1000 correct (71.10%)\n",
      "\n",
      "Iteration 5000, loss = 0.6706\n",
      "Got 753 / 1000 correct (75.30%)\n",
      "Got 696 / 1000 correct (69.60%)\n",
      "\n",
      "Iteration 5100, loss = 0.7476\n",
      "Got 749 / 1000 correct (74.90%)\n",
      "Got 692 / 1000 correct (69.20%)\n",
      "\n",
      "Iteration 5200, loss = 0.5374\n",
      "Got 759 / 1000 correct (75.90%)\n",
      "Got 716 / 1000 correct (71.60%)\n",
      "\n",
      "Iteration 5300, loss = 0.6338\n",
      "Got 765 / 1000 correct (76.50%)\n",
      "Got 709 / 1000 correct (70.90%)\n",
      "\n",
      "Starting epoch 7\n",
      "Iteration 5400, loss = 0.5227\n",
      "Got 870 / 1000 correct (87.00%)\n",
      "Got 705 / 1000 correct (70.50%)\n",
      "\n",
      "Iteration 5500, loss = 0.4735\n",
      "Got 799 / 1000 correct (79.90%)\n",
      "Got 702 / 1000 correct (70.20%)\n",
      "\n",
      "Iteration 5600, loss = 0.7654\n",
      "Got 791 / 1000 correct (79.10%)\n",
      "Got 706 / 1000 correct (70.60%)\n",
      "\n",
      "Iteration 5700, loss = 0.5113\n",
      "Got 789 / 1000 correct (78.90%)\n",
      "Got 707 / 1000 correct (70.70%)\n",
      "\n",
      "Iteration 5800, loss = 0.6004\n",
      "Got 797 / 1000 correct (79.70%)\n",
      "Got 700 / 1000 correct (70.00%)\n",
      "\n",
      "Iteration 5900, loss = 0.4972\n",
      "Got 765 / 1000 correct (76.50%)\n",
      "Got 704 / 1000 correct (70.40%)\n",
      "\n",
      "Iteration 6000, loss = 0.5868\n",
      "Got 764 / 1000 correct (76.40%)\n",
      "Got 710 / 1000 correct (71.00%)\n",
      "\n",
      "Iteration 6100, loss = 0.6310\n",
      "Got 754 / 1000 correct (75.40%)\n",
      "Got 707 / 1000 correct (70.70%)\n",
      "\n",
      "Starting epoch 8\n",
      "Iteration 6200, loss = 0.5004\n",
      "Got 846 / 1000 correct (84.60%)\n",
      "Got 688 / 1000 correct (68.80%)\n",
      "\n",
      "Iteration 6300, loss = 0.4958\n",
      "Got 802 / 1000 correct (80.20%)\n",
      "Got 719 / 1000 correct (71.90%)\n",
      "\n",
      "Iteration 6400, loss = 0.4566\n",
      "Got 791 / 1000 correct (79.10%)\n",
      "Got 691 / 1000 correct (69.10%)\n",
      "\n",
      "Iteration 6500, loss = 0.4623\n",
      "Got 795 / 1000 correct (79.50%)\n",
      "Got 705 / 1000 correct (70.50%)\n",
      "\n",
      "Iteration 6600, loss = 0.5028\n",
      "Got 795 / 1000 correct (79.50%)\n",
      "Got 705 / 1000 correct (70.50%)\n",
      "\n",
      "Iteration 6700, loss = 0.5280\n",
      "Got 712 / 1000 correct (71.20%)\n",
      "Got 666 / 1000 correct (66.60%)\n",
      "\n",
      "Iteration 6800, loss = 0.4927\n",
      "Got 752 / 1000 correct (75.20%)\n",
      "Got 702 / 1000 correct (70.20%)\n",
      "\n",
      "Starting epoch 9\n",
      "Iteration 6900, loss = 0.3193\n",
      "Got 890 / 1000 correct (89.00%)\n",
      "Got 686 / 1000 correct (68.60%)\n",
      "\n",
      "Iteration 7000, loss = 0.3950\n",
      "Got 822 / 1000 correct (82.20%)\n",
      "Got 670 / 1000 correct (67.00%)\n",
      "\n",
      "Iteration 7100, loss = 0.4363\n",
      "Got 790 / 1000 correct (79.00%)\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "\n",
      "Iteration 7200, loss = 0.4245\n",
      "Got 819 / 1000 correct (81.90%)\n",
      "Got 706 / 1000 correct (70.60%)\n",
      "\n",
      "Iteration 7300, loss = 0.4981\n",
      "Got 775 / 1000 correct (77.50%)\n",
      "Got 694 / 1000 correct (69.40%)\n",
      "\n",
      "Iteration 7400, loss = 0.2947\n",
      "Got 773 / 1000 correct (77.30%)\n",
      "Got 700 / 1000 correct (70.00%)\n",
      "\n",
      "Iteration 7500, loss = 0.4160\n",
      "Got 768 / 1000 correct (76.80%)\n",
      "Got 692 / 1000 correct (69.20%)\n",
      "\n",
      "Iteration 7600, loss = 0.5882\n",
      "Got 787 / 1000 correct (78.70%)\n",
      "Got 685 / 1000 correct (68.50%)\n",
      "\n",
      "Accuracy of final model on the test set:\n",
      "Got 6867 / 10000 correct (68.67%)\n"
     ]
    }
   ],
   "source": [
    "# INPUT -> [CONV -> BATCHNORM -> RELU -> POOL]*3 -> FC -> RELU -> FC\n",
    "# add in batch norm\n",
    "\n",
    "def five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes):     \n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    conv1 = tf.layers.conv2d(inputs, filters=channel_1, kernel_size=5, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch1 = tf.layers.batch_normalization(conv1)\n",
    "    relu1 = tf.nn.relu(batch1)\n",
    "    pool1 = tf.layers.max_pooling2d(relu1, pool_size=2, strides=2)\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=channel_2, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch2 = tf.layers.batch_normalization(conv2)\n",
    "    relu2 = tf.nn.relu(batch2)\n",
    "    pool2 = tf.layers.max_pooling2d(relu2, pool_size=2, strides=2)\n",
    "    conv3 = tf.layers.conv2d(pool2, filters=channel_3, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch3 = tf.layers.batch_normalization(conv3)\n",
    "    relu3 = tf.nn.relu(batch3)\n",
    "    pool3 = tf.layers.max_pooling2d(relu3, pool_size=2, strides=2)\n",
    "    flattened = tf.layers.flatten(pool3)\n",
    "    fc1 = tf.layers.dense(flattened, hidden_size, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "    scores = tf.layers.dense(fc1, num_classes,\n",
    "                             kernel_initializer=initializer)\n",
    "    return scores\n",
    "\n",
    "channel_1, channel_2, channel_3, hidden_size, num_classes = 64, 32, 32, 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 10\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried 32, 32, 32 but loss was not going down. Tried 64, 64, 64 but loss was not going down. Tried 64, 32, 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 4.0399\n",
      "Got 135 / 1000 correct (13.50%)\n",
      "Got 136 / 1000 correct (13.60%)\n",
      "\n",
      "Iteration 100, loss = 1.7765\n",
      "Got 383 / 1000 correct (38.30%)\n",
      "Got 373 / 1000 correct (37.30%)\n",
      "\n",
      "Iteration 200, loss = 1.4618\n",
      "Got 460 / 1000 correct (46.00%)\n",
      "Got 449 / 1000 correct (44.90%)\n",
      "\n",
      "Iteration 300, loss = 1.7447\n",
      "Got 489 / 1000 correct (48.90%)\n",
      "Got 477 / 1000 correct (47.70%)\n",
      "\n",
      "Iteration 400, loss = 1.3213\n",
      "Got 516 / 1000 correct (51.60%)\n",
      "Got 510 / 1000 correct (51.00%)\n",
      "\n",
      "Iteration 500, loss = 1.6129\n",
      "Got 507 / 1000 correct (50.70%)\n",
      "Got 512 / 1000 correct (51.20%)\n",
      "\n",
      "Iteration 600, loss = 1.4345\n",
      "Got 512 / 1000 correct (51.20%)\n",
      "Got 519 / 1000 correct (51.90%)\n",
      "\n",
      "Iteration 700, loss = 1.5176\n",
      "Got 525 / 1000 correct (52.50%)\n",
      "Got 510 / 1000 correct (51.00%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 1.3131\n",
      "Got 593 / 1000 correct (59.30%)\n",
      "Got 545 / 1000 correct (54.50%)\n",
      "\n",
      "Iteration 900, loss = 1.2233\n",
      "Got 575 / 1000 correct (57.50%)\n",
      "Got 557 / 1000 correct (55.70%)\n",
      "\n",
      "Iteration 1000, loss = 1.0733\n",
      "Got 566 / 1000 correct (56.60%)\n",
      "Got 541 / 1000 correct (54.10%)\n",
      "\n",
      "Iteration 1100, loss = 1.2781\n",
      "Got 575 / 1000 correct (57.50%)\n",
      "Got 567 / 1000 correct (56.70%)\n",
      "\n",
      "Iteration 1200, loss = 1.1052\n",
      "Got 583 / 1000 correct (58.30%)\n",
      "Got 562 / 1000 correct (56.20%)\n",
      "\n",
      "Iteration 1300, loss = 1.1401\n",
      "Got 579 / 1000 correct (57.90%)\n",
      "Got 575 / 1000 correct (57.50%)\n",
      "\n",
      "Iteration 1400, loss = 1.2242\n",
      "Got 584 / 1000 correct (58.40%)\n",
      "Got 589 / 1000 correct (58.90%)\n",
      "\n",
      "Iteration 1500, loss = 1.1434\n",
      "Got 592 / 1000 correct (59.20%)\n",
      "Got 590 / 1000 correct (59.00%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 0.9782\n",
      "Got 634 / 1000 correct (63.40%)\n",
      "Got 611 / 1000 correct (61.10%)\n",
      "\n",
      "Iteration 1700, loss = 0.9875\n",
      "Got 621 / 1000 correct (62.10%)\n",
      "Got 627 / 1000 correct (62.70%)\n",
      "\n",
      "Iteration 1800, loss = 0.7548\n",
      "Got 639 / 1000 correct (63.90%)\n",
      "Got 618 / 1000 correct (61.80%)\n",
      "\n",
      "Iteration 1900, loss = 0.9273\n",
      "Got 630 / 1000 correct (63.00%)\n",
      "Got 629 / 1000 correct (62.90%)\n",
      "\n",
      "Iteration 2000, loss = 0.9372\n",
      "Got 620 / 1000 correct (62.00%)\n",
      "Got 618 / 1000 correct (61.80%)\n",
      "\n",
      "Iteration 2100, loss = 0.8850\n",
      "Got 644 / 1000 correct (64.40%)\n",
      "Got 619 / 1000 correct (61.90%)\n",
      "\n",
      "Iteration 2200, loss = 1.0285\n",
      "Got 646 / 1000 correct (64.60%)\n",
      "Got 637 / 1000 correct (63.70%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2300, loss = 1.2126\n",
      "Got 651 / 1000 correct (65.10%)\n",
      "Got 630 / 1000 correct (63.00%)\n",
      "\n",
      "Iteration 2400, loss = 0.8459\n",
      "Got 671 / 1000 correct (67.10%)\n",
      "Got 631 / 1000 correct (63.10%)\n",
      "\n",
      "Iteration 2500, loss = 1.1522\n",
      "Got 689 / 1000 correct (68.90%)\n",
      "Got 649 / 1000 correct (64.90%)\n",
      "\n",
      "Iteration 2600, loss = 0.9118\n",
      "Got 644 / 1000 correct (64.40%)\n",
      "Got 636 / 1000 correct (63.60%)\n",
      "\n",
      "Iteration 2700, loss = 0.9939\n",
      "Got 661 / 1000 correct (66.10%)\n",
      "Got 638 / 1000 correct (63.80%)\n",
      "\n",
      "Iteration 2800, loss = 0.9181\n",
      "Got 675 / 1000 correct (67.50%)\n",
      "Got 643 / 1000 correct (64.30%)\n",
      "\n",
      "Iteration 2900, loss = 0.6237\n",
      "Got 685 / 1000 correct (68.50%)\n",
      "Got 636 / 1000 correct (63.60%)\n",
      "\n",
      "Iteration 3000, loss = 0.9797\n",
      "Got 669 / 1000 correct (66.90%)\n",
      "Got 640 / 1000 correct (64.00%)\n",
      "\n",
      "Starting epoch 4\n",
      "Iteration 3100, loss = 0.8088\n",
      "Got 731 / 1000 correct (73.10%)\n",
      "Got 656 / 1000 correct (65.60%)\n",
      "\n",
      "Iteration 3200, loss = 1.0704\n",
      "Got 709 / 1000 correct (70.90%)\n",
      "Got 657 / 1000 correct (65.70%)\n",
      "\n",
      "Iteration 3300, loss = 0.8587\n",
      "Got 699 / 1000 correct (69.90%)\n",
      "Got 662 / 1000 correct (66.20%)\n",
      "\n",
      "Iteration 3400, loss = 0.9357\n",
      "Got 699 / 1000 correct (69.90%)\n",
      "Got 661 / 1000 correct (66.10%)\n",
      "\n",
      "Iteration 3500, loss = 0.8753\n",
      "Got 670 / 1000 correct (67.00%)\n",
      "Got 647 / 1000 correct (64.70%)\n",
      "\n",
      "Iteration 3600, loss = 1.0029\n",
      "Got 718 / 1000 correct (71.80%)\n",
      "Got 668 / 1000 correct (66.80%)\n",
      "\n",
      "Iteration 3700, loss = 0.8251\n",
      "Got 695 / 1000 correct (69.50%)\n",
      "Got 665 / 1000 correct (66.50%)\n",
      "\n",
      "Iteration 3800, loss = 0.6746\n",
      "Got 687 / 1000 correct (68.70%)\n",
      "Got 639 / 1000 correct (63.90%)\n",
      "\n",
      "Starting epoch 5\n",
      "Iteration 3900, loss = 1.0880\n",
      "Got 746 / 1000 correct (74.60%)\n",
      "Got 669 / 1000 correct (66.90%)\n",
      "\n",
      "Iteration 4000, loss = 0.9583\n",
      "Got 731 / 1000 correct (73.10%)\n",
      "Got 663 / 1000 correct (66.30%)\n",
      "\n",
      "Iteration 4100, loss = 0.8665\n",
      "Got 723 / 1000 correct (72.30%)\n",
      "Got 674 / 1000 correct (67.40%)\n",
      "\n",
      "Iteration 4200, loss = 0.6976\n",
      "Got 719 / 1000 correct (71.90%)\n",
      "Got 664 / 1000 correct (66.40%)\n",
      "\n",
      "Iteration 4300, loss = 0.8473\n",
      "Got 721 / 1000 correct (72.10%)\n",
      "Got 676 / 1000 correct (67.60%)\n",
      "\n",
      "Iteration 4400, loss = 0.6938\n",
      "Got 734 / 1000 correct (73.40%)\n",
      "Got 681 / 1000 correct (68.10%)\n",
      "\n",
      "Iteration 4500, loss = 0.8709\n",
      "Got 738 / 1000 correct (73.80%)\n",
      "Got 687 / 1000 correct (68.70%)\n",
      "\n",
      "Starting epoch 6\n",
      "Iteration 4600, loss = 0.7190\n",
      "Got 730 / 1000 correct (73.00%)\n",
      "Got 665 / 1000 correct (66.50%)\n",
      "\n",
      "Iteration 4700, loss = 0.8034\n",
      "Got 734 / 1000 correct (73.40%)\n",
      "Got 673 / 1000 correct (67.30%)\n",
      "\n",
      "Iteration 4800, loss = 0.8405\n",
      "Got 737 / 1000 correct (73.70%)\n",
      "Got 675 / 1000 correct (67.50%)\n",
      "\n",
      "Iteration 4900, loss = 0.6398\n",
      "Got 734 / 1000 correct (73.40%)\n",
      "Got 689 / 1000 correct (68.90%)\n",
      "\n",
      "Iteration 5000, loss = 0.7808\n",
      "Got 712 / 1000 correct (71.20%)\n",
      "Got 681 / 1000 correct (68.10%)\n",
      "\n",
      "Iteration 5100, loss = 0.8258\n",
      "Got 740 / 1000 correct (74.00%)\n",
      "Got 672 / 1000 correct (67.20%)\n",
      "\n",
      "Iteration 5200, loss = 0.6349\n",
      "Got 745 / 1000 correct (74.50%)\n",
      "Got 680 / 1000 correct (68.00%)\n",
      "\n",
      "Iteration 5300, loss = 0.7672\n",
      "Got 737 / 1000 correct (73.70%)\n",
      "Got 680 / 1000 correct (68.00%)\n",
      "\n",
      "Starting epoch 7\n",
      "Iteration 5400, loss = 0.4968\n",
      "Got 806 / 1000 correct (80.60%)\n",
      "Got 700 / 1000 correct (70.00%)\n",
      "\n",
      "Iteration 5500, loss = 0.5996\n",
      "Got 755 / 1000 correct (75.50%)\n",
      "Got 687 / 1000 correct (68.70%)\n",
      "\n",
      "Iteration 5600, loss = 0.6424\n",
      "Got 759 / 1000 correct (75.90%)\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "\n",
      "Iteration 5700, loss = 0.7548\n",
      "Got 744 / 1000 correct (74.40%)\n",
      "Got 693 / 1000 correct (69.30%)\n",
      "\n",
      "Iteration 5800, loss = 0.6175\n",
      "Got 758 / 1000 correct (75.80%)\n",
      "Got 698 / 1000 correct (69.80%)\n",
      "\n",
      "Iteration 5900, loss = 0.5250\n",
      "Got 776 / 1000 correct (77.60%)\n",
      "Got 701 / 1000 correct (70.10%)\n",
      "\n",
      "Iteration 6000, loss = 0.8413\n",
      "Got 754 / 1000 correct (75.40%)\n",
      "Got 679 / 1000 correct (67.90%)\n",
      "\n",
      "Iteration 6100, loss = 0.6423\n",
      "Got 732 / 1000 correct (73.20%)\n",
      "Got 680 / 1000 correct (68.00%)\n",
      "\n",
      "Starting epoch 8\n",
      "Iteration 6200, loss = 0.6582\n",
      "Got 798 / 1000 correct (79.80%)\n",
      "Got 696 / 1000 correct (69.60%)\n",
      "\n",
      "Iteration 6300, loss = 0.7213\n",
      "Got 771 / 1000 correct (77.10%)\n",
      "Got 682 / 1000 correct (68.20%)\n",
      "\n",
      "Iteration 6400, loss = 0.5648\n",
      "Got 769 / 1000 correct (76.90%)\n",
      "Got 706 / 1000 correct (70.60%)\n",
      "\n",
      "Iteration 6500, loss = 0.6498\n",
      "Got 748 / 1000 correct (74.80%)\n",
      "Got 687 / 1000 correct (68.70%)\n",
      "\n",
      "Iteration 6600, loss = 0.4584\n",
      "Got 761 / 1000 correct (76.10%)\n",
      "Got 689 / 1000 correct (68.90%)\n",
      "\n",
      "Iteration 6700, loss = 0.6531\n",
      "Got 758 / 1000 correct (75.80%)\n",
      "Got 699 / 1000 correct (69.90%)\n",
      "\n",
      "Iteration 6800, loss = 0.6703\n",
      "Got 760 / 1000 correct (76.00%)\n",
      "Got 703 / 1000 correct (70.30%)\n",
      "\n",
      "Starting epoch 9\n",
      "Iteration 6900, loss = 0.7718\n",
      "Got 796 / 1000 correct (79.60%)\n",
      "Got 700 / 1000 correct (70.00%)\n",
      "\n",
      "Iteration 7000, loss = 0.5216\n",
      "Got 769 / 1000 correct (76.90%)\n",
      "Got 707 / 1000 correct (70.70%)\n",
      "\n",
      "Iteration 7100, loss = 0.5272\n",
      "Got 794 / 1000 correct (79.40%)\n",
      "Got 707 / 1000 correct (70.70%)\n",
      "\n",
      "Iteration 7200, loss = 0.5535\n",
      "Got 804 / 1000 correct (80.40%)\n",
      "Got 712 / 1000 correct (71.20%)\n",
      "\n",
      "Iteration 7300, loss = 0.6978\n",
      "Got 771 / 1000 correct (77.10%)\n",
      "Got 695 / 1000 correct (69.50%)\n",
      "\n",
      "Iteration 7400, loss = 0.6244\n",
      "Got 762 / 1000 correct (76.20%)\n",
      "Got 682 / 1000 correct (68.20%)\n",
      "\n",
      "Iteration 7500, loss = 0.4125\n",
      "Got 775 / 1000 correct (77.50%)\n",
      "Got 695 / 1000 correct (69.50%)\n",
      "\n",
      "Iteration 7600, loss = 0.6703\n",
      "Got 765 / 1000 correct (76.50%)\n",
      "Got 692 / 1000 correct (69.20%)\n",
      "\n",
      "Accuracy of final model on the test set:\n",
      "Got 6918 / 10000 correct (69.18%)\n"
     ]
    }
   ],
   "source": [
    "# INPUT -> [CONV -> BATCHNORM -> RELU -> POOL]*3 -> FC -> RELU -> FC\n",
    "# add in batch norm\n",
    "\n",
    "def five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes):     \n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    conv1 = tf.layers.conv2d(inputs, filters=channel_1, kernel_size=5, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch1 = tf.layers.batch_normalization(conv1)\n",
    "    relu1 = tf.nn.relu(batch1)\n",
    "    pool1 = tf.layers.max_pooling2d(relu1, pool_size=2, strides=2)\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=channel_2, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch2 = tf.layers.batch_normalization(conv2)\n",
    "    relu2 = tf.nn.relu(batch2)\n",
    "    pool2 = tf.layers.max_pooling2d(relu2, pool_size=2, strides=2)\n",
    "    conv3 = tf.layers.conv2d(pool2, filters=channel_3, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch3 = tf.layers.batch_normalization(conv3)\n",
    "    relu3 = tf.nn.relu(batch3)\n",
    "    pool3 = tf.layers.max_pooling2d(relu3, pool_size=2, strides=2)\n",
    "    flattened = tf.layers.flatten(pool3)\n",
    "    fc1 = tf.layers.dense(flattened, hidden_size, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "    scores = tf.layers.dense(fc1, num_classes,\n",
    "                             kernel_initializer=initializer)\n",
    "    return scores\n",
    "\n",
    "channel_1, channel_2, channel_3, hidden_size, num_classes = 64, 32, 32, 4000, 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 10\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 3.0159\n",
      "Got 105 / 1000 correct (10.50%)\n",
      "Got 115 / 1000 correct (11.50%)\n",
      "\n",
      "Iteration 100, loss = 1.6850\n",
      "Got 437 / 1000 correct (43.70%)\n",
      "Got 410 / 1000 correct (41.00%)\n",
      "\n",
      "Iteration 200, loss = 1.3213\n",
      "Got 466 / 1000 correct (46.60%)\n",
      "Got 468 / 1000 correct (46.80%)\n",
      "\n",
      "Iteration 300, loss = 1.5470\n",
      "Got 493 / 1000 correct (49.30%)\n",
      "Got 486 / 1000 correct (48.60%)\n",
      "\n",
      "Iteration 400, loss = 1.3266\n",
      "Got 514 / 1000 correct (51.40%)\n",
      "Got 510 / 1000 correct (51.00%)\n",
      "\n",
      "Iteration 500, loss = 1.5634\n",
      "Got 522 / 1000 correct (52.20%)\n",
      "Got 516 / 1000 correct (51.60%)\n",
      "\n",
      "Iteration 600, loss = 1.3816\n",
      "Got 528 / 1000 correct (52.80%)\n",
      "Got 538 / 1000 correct (53.80%)\n",
      "\n",
      "Iteration 700, loss = 1.4600\n",
      "Got 553 / 1000 correct (55.30%)\n",
      "Got 548 / 1000 correct (54.80%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 1.2371\n",
      "Got 619 / 1000 correct (61.90%)\n",
      "Got 579 / 1000 correct (57.90%)\n",
      "\n",
      "Iteration 900, loss = 1.0789\n",
      "Got 609 / 1000 correct (60.90%)\n",
      "Got 570 / 1000 correct (57.00%)\n",
      "\n",
      "Iteration 1000, loss = 1.2114\n",
      "Got 591 / 1000 correct (59.10%)\n",
      "Got 552 / 1000 correct (55.20%)\n",
      "\n",
      "Iteration 1100, loss = 1.3439\n",
      "Got 619 / 1000 correct (61.90%)\n",
      "Got 582 / 1000 correct (58.20%)\n",
      "\n",
      "Iteration 1200, loss = 1.0247\n",
      "Got 627 / 1000 correct (62.70%)\n",
      "Got 586 / 1000 correct (58.60%)\n",
      "\n",
      "Iteration 1300, loss = 1.1128\n",
      "Got 627 / 1000 correct (62.70%)\n",
      "Got 593 / 1000 correct (59.30%)\n",
      "\n",
      "Iteration 1400, loss = 1.1455\n",
      "Got 607 / 1000 correct (60.70%)\n",
      "Got 592 / 1000 correct (59.20%)\n",
      "\n",
      "Iteration 1500, loss = 1.1221\n",
      "Got 627 / 1000 correct (62.70%)\n",
      "Got 605 / 1000 correct (60.50%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 0.9008\n",
      "Got 669 / 1000 correct (66.90%)\n",
      "Got 606 / 1000 correct (60.60%)\n",
      "\n",
      "Iteration 1700, loss = 0.9757\n",
      "Got 640 / 1000 correct (64.00%)\n",
      "Got 600 / 1000 correct (60.00%)\n",
      "\n",
      "Iteration 1800, loss = 0.8876\n",
      "Got 677 / 1000 correct (67.70%)\n",
      "Got 621 / 1000 correct (62.10%)\n",
      "\n",
      "Iteration 1900, loss = 0.9767\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "Got 628 / 1000 correct (62.80%)\n",
      "\n",
      "Iteration 2000, loss = 0.9310\n",
      "Got 656 / 1000 correct (65.60%)\n",
      "Got 614 / 1000 correct (61.40%)\n",
      "\n",
      "Iteration 2100, loss = 1.0110\n",
      "Got 679 / 1000 correct (67.90%)\n",
      "Got 638 / 1000 correct (63.80%)\n",
      "\n",
      "Iteration 2200, loss = 0.9284\n",
      "Got 656 / 1000 correct (65.60%)\n",
      "Got 628 / 1000 correct (62.80%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2300, loss = 1.0747\n",
      "Got 684 / 1000 correct (68.40%)\n",
      "Got 657 / 1000 correct (65.70%)\n",
      "\n",
      "Iteration 2400, loss = 0.8756\n",
      "Got 697 / 1000 correct (69.70%)\n",
      "Got 635 / 1000 correct (63.50%)\n",
      "\n",
      "Iteration 2500, loss = 1.0935\n",
      "Got 724 / 1000 correct (72.40%)\n",
      "Got 658 / 1000 correct (65.80%)\n",
      "\n",
      "Iteration 2600, loss = 0.9422\n",
      "Got 686 / 1000 correct (68.60%)\n",
      "Got 625 / 1000 correct (62.50%)\n",
      "\n",
      "Iteration 2700, loss = 1.0320\n",
      "Got 686 / 1000 correct (68.60%)\n",
      "Got 642 / 1000 correct (64.20%)\n",
      "\n",
      "Iteration 2800, loss = 0.7960\n",
      "Got 712 / 1000 correct (71.20%)\n",
      "Got 641 / 1000 correct (64.10%)\n",
      "\n",
      "Iteration 2900, loss = 0.6760\n",
      "Got 716 / 1000 correct (71.60%)\n",
      "Got 648 / 1000 correct (64.80%)\n",
      "\n",
      "Iteration 3000, loss = 0.8889\n",
      "Got 699 / 1000 correct (69.90%)\n",
      "Got 651 / 1000 correct (65.10%)\n",
      "\n",
      "Starting epoch 4\n",
      "Iteration 3100, loss = 0.7101\n",
      "Got 764 / 1000 correct (76.40%)\n",
      "Got 669 / 1000 correct (66.90%)\n",
      "\n",
      "Iteration 3200, loss = 0.8375\n",
      "Got 734 / 1000 correct (73.40%)\n",
      "Got 662 / 1000 correct (66.20%)\n",
      "\n",
      "Iteration 3300, loss = 0.8997\n",
      "Got 726 / 1000 correct (72.60%)\n",
      "Got 660 / 1000 correct (66.00%)\n",
      "\n",
      "Iteration 3400, loss = 0.9267\n",
      "Got 732 / 1000 correct (73.20%)\n",
      "Got 670 / 1000 correct (67.00%)\n",
      "\n",
      "Iteration 3500, loss = 0.7887\n",
      "Got 717 / 1000 correct (71.70%)\n",
      "Got 638 / 1000 correct (63.80%)\n",
      "\n",
      "Iteration 3600, loss = 0.9726\n",
      "Got 736 / 1000 correct (73.60%)\n",
      "Got 671 / 1000 correct (67.10%)\n",
      "\n",
      "Iteration 3700, loss = 0.8328\n",
      "Got 738 / 1000 correct (73.80%)\n",
      "Got 664 / 1000 correct (66.40%)\n",
      "\n",
      "Iteration 3800, loss = 0.6694\n",
      "Got 708 / 1000 correct (70.80%)\n",
      "Got 650 / 1000 correct (65.00%)\n",
      "\n",
      "Starting epoch 5\n",
      "Iteration 3900, loss = 1.0260\n",
      "Got 757 / 1000 correct (75.70%)\n",
      "Got 673 / 1000 correct (67.30%)\n",
      "\n",
      "Iteration 4000, loss = 0.9793\n",
      "Got 765 / 1000 correct (76.50%)\n",
      "Got 681 / 1000 correct (68.10%)\n",
      "\n",
      "Iteration 4100, loss = 0.7908\n",
      "Got 770 / 1000 correct (77.00%)\n",
      "Got 681 / 1000 correct (68.10%)\n",
      "\n",
      "Iteration 4200, loss = 0.4932\n",
      "Got 775 / 1000 correct (77.50%)\n",
      "Got 672 / 1000 correct (67.20%)\n",
      "\n",
      "Iteration 4300, loss = 0.7868\n",
      "Got 741 / 1000 correct (74.10%)\n",
      "Got 671 / 1000 correct (67.10%)\n",
      "\n",
      "Iteration 4400, loss = 0.6591\n",
      "Got 743 / 1000 correct (74.30%)\n",
      "Got 665 / 1000 correct (66.50%)\n",
      "\n",
      "Iteration 4500, loss = 1.0405\n",
      "Got 748 / 1000 correct (74.80%)\n",
      "Got 681 / 1000 correct (68.10%)\n",
      "\n",
      "Starting epoch 6\n",
      "Iteration 4600, loss = 0.6754\n",
      "Got 744 / 1000 correct (74.40%)\n",
      "Got 673 / 1000 correct (67.30%)\n",
      "\n",
      "Iteration 4700, loss = 0.6614\n",
      "Got 758 / 1000 correct (75.80%)\n",
      "Got 673 / 1000 correct (67.30%)\n",
      "\n",
      "Iteration 4800, loss = 0.6666\n",
      "Got 782 / 1000 correct (78.20%)\n",
      "Got 688 / 1000 correct (68.80%)\n",
      "\n",
      "Iteration 4900, loss = 0.5293\n",
      "Got 789 / 1000 correct (78.90%)\n",
      "Got 685 / 1000 correct (68.50%)\n",
      "\n",
      "Iteration 5000, loss = 0.6910\n",
      "Got 775 / 1000 correct (77.50%)\n",
      "Got 682 / 1000 correct (68.20%)\n",
      "\n",
      "Iteration 5100, loss = 0.7569\n",
      "Got 747 / 1000 correct (74.70%)\n",
      "Got 673 / 1000 correct (67.30%)\n",
      "\n",
      "Iteration 5200, loss = 0.6030\n",
      "Got 778 / 1000 correct (77.80%)\n",
      "Got 689 / 1000 correct (68.90%)\n",
      "\n",
      "Iteration 5300, loss = 0.7455\n",
      "Got 757 / 1000 correct (75.70%)\n",
      "Got 670 / 1000 correct (67.00%)\n",
      "\n",
      "Starting epoch 7\n",
      "Iteration 5400, loss = 0.4575\n",
      "Got 836 / 1000 correct (83.60%)\n",
      "Got 699 / 1000 correct (69.90%)\n",
      "\n",
      "Iteration 5500, loss = 0.4671\n",
      "Got 790 / 1000 correct (79.00%)\n",
      "Got 685 / 1000 correct (68.50%)\n",
      "\n",
      "Iteration 5600, loss = 0.4868\n",
      "Got 780 / 1000 correct (78.00%)\n",
      "Got 704 / 1000 correct (70.40%)\n",
      "\n",
      "Iteration 5700, loss = 0.6047\n",
      "Got 773 / 1000 correct (77.30%)\n",
      "Got 683 / 1000 correct (68.30%)\n",
      "\n",
      "Iteration 5800, loss = 0.5978\n",
      "Got 789 / 1000 correct (78.90%)\n",
      "Got 689 / 1000 correct (68.90%)\n",
      "\n",
      "Iteration 5900, loss = 0.7132\n",
      "Got 770 / 1000 correct (77.00%)\n",
      "Got 679 / 1000 correct (67.90%)\n",
      "\n",
      "Iteration 6000, loss = 0.6812\n",
      "Got 772 / 1000 correct (77.20%)\n",
      "Got 695 / 1000 correct (69.50%)\n",
      "\n",
      "Iteration 6100, loss = 0.5258\n",
      "Got 780 / 1000 correct (78.00%)\n",
      "Got 696 / 1000 correct (69.60%)\n",
      "\n",
      "Starting epoch 8\n",
      "Iteration 6200, loss = 0.5611\n",
      "Got 829 / 1000 correct (82.90%)\n",
      "Got 691 / 1000 correct (69.10%)\n",
      "\n",
      "Iteration 6300, loss = 0.6619\n",
      "Got 819 / 1000 correct (81.90%)\n",
      "Got 701 / 1000 correct (70.10%)\n",
      "\n",
      "Iteration 6400, loss = 0.5445\n",
      "Got 821 / 1000 correct (82.10%)\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "\n",
      "Iteration 6500, loss = 0.5213\n",
      "Got 818 / 1000 correct (81.80%)\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "\n",
      "Iteration 6600, loss = 0.4126\n",
      "Got 811 / 1000 correct (81.10%)\n",
      "Got 703 / 1000 correct (70.30%)\n",
      "\n",
      "Iteration 6700, loss = 0.5691\n",
      "Got 789 / 1000 correct (78.90%)\n",
      "Got 688 / 1000 correct (68.80%)\n",
      "\n",
      "Iteration 6800, loss = 0.6631\n",
      "Got 807 / 1000 correct (80.70%)\n",
      "Got 689 / 1000 correct (68.90%)\n",
      "\n",
      "Starting epoch 9\n",
      "Iteration 6900, loss = 0.6477\n",
      "Got 830 / 1000 correct (83.00%)\n",
      "Got 707 / 1000 correct (70.70%)\n",
      "\n",
      "Iteration 7000, loss = 0.4737\n",
      "Got 812 / 1000 correct (81.20%)\n",
      "Got 695 / 1000 correct (69.50%)\n",
      "\n",
      "Iteration 7100, loss = 0.5143\n",
      "Got 843 / 1000 correct (84.30%)\n",
      "Got 709 / 1000 correct (70.90%)\n",
      "\n",
      "Iteration 7200, loss = 0.6091\n",
      "Got 828 / 1000 correct (82.80%)\n",
      "Got 703 / 1000 correct (70.30%)\n",
      "\n",
      "Iteration 7300, loss = 0.5271\n",
      "Got 812 / 1000 correct (81.20%)\n",
      "Got 703 / 1000 correct (70.30%)\n",
      "\n",
      "Iteration 7400, loss = 0.4540\n",
      "Got 806 / 1000 correct (80.60%)\n",
      "Got 688 / 1000 correct (68.80%)\n",
      "\n",
      "Iteration 7500, loss = 0.3877\n",
      "Got 830 / 1000 correct (83.00%)\n",
      "Got 714 / 1000 correct (71.40%)\n",
      "\n",
      "Iteration 7600, loss = 0.6279\n",
      "Got 798 / 1000 correct (79.80%)\n",
      "Got 696 / 1000 correct (69.60%)\n",
      "\n",
      "Accuracy of final model on the test set:\n",
      "Got 6836 / 10000 correct (68.36%)\n"
     ]
    }
   ],
   "source": [
    "# INPUT -> [CONV -> BATCHNORM -> RELU -> POOL]*3 -> FC -> RELU -> FC\n",
    "# add in batch norm\n",
    "\n",
    "def five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes):     \n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    conv1 = tf.layers.conv2d(inputs, filters=channel_1, kernel_size=5, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch1 = tf.layers.batch_normalization(conv1)\n",
    "    relu1 = tf.nn.relu(batch1)\n",
    "    pool1 = tf.layers.max_pooling2d(relu1, pool_size=2, strides=2)\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=channel_2, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch2 = tf.layers.batch_normalization(conv2)\n",
    "    relu2 = tf.nn.relu(batch2)\n",
    "    pool2 = tf.layers.max_pooling2d(relu2, pool_size=2, strides=2)\n",
    "    conv3 = tf.layers.conv2d(pool2, filters=channel_3, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch3 = tf.layers.batch_normalization(conv3)\n",
    "    relu3 = tf.nn.relu(batch3)\n",
    "    pool3 = tf.layers.max_pooling2d(relu3, pool_size=2, strides=2)\n",
    "    flattened = tf.layers.flatten(pool3)\n",
    "    fc1 = tf.layers.dense(flattened, hidden_size, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "    scores = tf.layers.dense(fc1, num_classes,\n",
    "                             kernel_initializer=initializer)\n",
    "    return scores\n",
    "\n",
    "channel_1, channel_2, channel_3, hidden_size, num_classes = 32, 64, 64, 4000, 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 10\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.6608\n",
      "Got 101 / 1000 correct (10.10%)\n",
      "Got 81 / 1000 correct (8.10%)\n",
      "\n",
      "Iteration 100, loss = 1.7584\n",
      "Got 398 / 1000 correct (39.80%)\n",
      "Got 402 / 1000 correct (40.20%)\n",
      "\n",
      "Iteration 200, loss = 1.4576\n",
      "Got 457 / 1000 correct (45.70%)\n",
      "Got 462 / 1000 correct (46.20%)\n",
      "\n",
      "Iteration 300, loss = 1.5017\n",
      "Got 460 / 1000 correct (46.00%)\n",
      "Got 474 / 1000 correct (47.40%)\n",
      "\n",
      "Iteration 400, loss = 1.2632\n",
      "Got 491 / 1000 correct (49.10%)\n",
      "Got 509 / 1000 correct (50.90%)\n",
      "\n",
      "Iteration 500, loss = 1.4354\n",
      "Got 507 / 1000 correct (50.70%)\n",
      "Got 521 / 1000 correct (52.10%)\n",
      "\n",
      "Iteration 600, loss = 1.3727\n",
      "Got 497 / 1000 correct (49.70%)\n",
      "Got 525 / 1000 correct (52.50%)\n",
      "\n",
      "Iteration 700, loss = 1.3930\n",
      "Got 507 / 1000 correct (50.70%)\n",
      "Got 519 / 1000 correct (51.90%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 1.3361\n",
      "Got 609 / 1000 correct (60.90%)\n",
      "Got 582 / 1000 correct (58.20%)\n",
      "\n",
      "Iteration 900, loss = 1.1515\n",
      "Got 592 / 1000 correct (59.20%)\n",
      "Got 580 / 1000 correct (58.00%)\n",
      "\n",
      "Iteration 1000, loss = 1.0583\n",
      "Got 564 / 1000 correct (56.40%)\n",
      "Got 559 / 1000 correct (55.90%)\n",
      "\n",
      "Iteration 1100, loss = 1.3256\n",
      "Got 604 / 1000 correct (60.40%)\n",
      "Got 591 / 1000 correct (59.10%)\n",
      "\n",
      "Iteration 1200, loss = 1.0864\n",
      "Got 579 / 1000 correct (57.90%)\n",
      "Got 585 / 1000 correct (58.50%)\n",
      "\n",
      "Iteration 1300, loss = 1.1970\n",
      "Got 595 / 1000 correct (59.50%)\n",
      "Got 597 / 1000 correct (59.70%)\n",
      "\n",
      "Iteration 1400, loss = 1.2013\n",
      "Got 589 / 1000 correct (58.90%)\n",
      "Got 601 / 1000 correct (60.10%)\n",
      "\n",
      "Iteration 1500, loss = 1.3208\n",
      "Got 603 / 1000 correct (60.30%)\n",
      "Got 612 / 1000 correct (61.20%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 0.9206\n",
      "Got 638 / 1000 correct (63.80%)\n",
      "Got 612 / 1000 correct (61.20%)\n",
      "\n",
      "Iteration 1700, loss = 0.9586\n",
      "Got 622 / 1000 correct (62.20%)\n",
      "Got 620 / 1000 correct (62.00%)\n",
      "\n",
      "Iteration 1800, loss = 0.9126\n",
      "Got 653 / 1000 correct (65.30%)\n",
      "Got 627 / 1000 correct (62.70%)\n",
      "\n",
      "Iteration 1900, loss = 1.0865\n",
      "Got 634 / 1000 correct (63.40%)\n",
      "Got 633 / 1000 correct (63.30%)\n",
      "\n",
      "Iteration 2000, loss = 0.8689\n",
      "Got 631 / 1000 correct (63.10%)\n",
      "Got 629 / 1000 correct (62.90%)\n",
      "\n",
      "Iteration 2100, loss = 0.9509\n",
      "Got 661 / 1000 correct (66.10%)\n",
      "Got 631 / 1000 correct (63.10%)\n",
      "\n",
      "Iteration 2200, loss = 0.9268\n",
      "Got 651 / 1000 correct (65.10%)\n",
      "Got 644 / 1000 correct (64.40%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2300, loss = 1.1008\n",
      "Got 666 / 1000 correct (66.60%)\n",
      "Got 657 / 1000 correct (65.70%)\n",
      "\n",
      "Iteration 2400, loss = 0.8352\n",
      "Got 688 / 1000 correct (68.80%)\n",
      "Got 651 / 1000 correct (65.10%)\n",
      "\n",
      "Iteration 2500, loss = 0.9954\n",
      "Got 695 / 1000 correct (69.50%)\n",
      "Got 664 / 1000 correct (66.40%)\n",
      "\n",
      "Iteration 2600, loss = 0.9376\n",
      "Got 660 / 1000 correct (66.00%)\n",
      "Got 648 / 1000 correct (64.80%)\n",
      "\n",
      "Iteration 2700, loss = 0.9602\n",
      "Got 673 / 1000 correct (67.30%)\n",
      "Got 647 / 1000 correct (64.70%)\n",
      "\n",
      "Iteration 2800, loss = 0.9338\n",
      "Got 699 / 1000 correct (69.90%)\n",
      "Got 654 / 1000 correct (65.40%)\n",
      "\n",
      "Iteration 2900, loss = 0.6089\n",
      "Got 680 / 1000 correct (68.00%)\n",
      "Got 654 / 1000 correct (65.40%)\n",
      "\n",
      "Iteration 3000, loss = 0.9354\n",
      "Got 689 / 1000 correct (68.90%)\n",
      "Got 642 / 1000 correct (64.20%)\n",
      "\n",
      "Starting epoch 4\n",
      "Iteration 3100, loss = 0.7556\n",
      "Got 728 / 1000 correct (72.80%)\n",
      "Got 669 / 1000 correct (66.90%)\n",
      "\n",
      "Iteration 3200, loss = 0.8392\n",
      "Got 711 / 1000 correct (71.10%)\n",
      "Got 678 / 1000 correct (67.80%)\n",
      "\n",
      "Iteration 3300, loss = 0.8442\n",
      "Got 714 / 1000 correct (71.40%)\n",
      "Got 677 / 1000 correct (67.70%)\n",
      "\n",
      "Iteration 3400, loss = 0.9490\n",
      "Got 706 / 1000 correct (70.60%)\n",
      "Got 674 / 1000 correct (67.40%)\n",
      "\n",
      "Iteration 3500, loss = 0.8388\n",
      "Got 694 / 1000 correct (69.40%)\n",
      "Got 645 / 1000 correct (64.50%)\n",
      "\n",
      "Iteration 3600, loss = 1.0366\n",
      "Got 730 / 1000 correct (73.00%)\n",
      "Got 685 / 1000 correct (68.50%)\n",
      "\n",
      "Iteration 3700, loss = 0.8808\n",
      "Got 693 / 1000 correct (69.30%)\n",
      "Got 668 / 1000 correct (66.80%)\n",
      "\n",
      "Iteration 3800, loss = 0.6082\n",
      "Got 710 / 1000 correct (71.00%)\n",
      "Got 672 / 1000 correct (67.20%)\n",
      "\n",
      "Starting epoch 5\n",
      "Iteration 3900, loss = 0.9400\n",
      "Got 726 / 1000 correct (72.60%)\n",
      "Got 660 / 1000 correct (66.00%)\n",
      "\n",
      "Iteration 4000, loss = 0.8762\n",
      "Got 739 / 1000 correct (73.90%)\n",
      "Got 675 / 1000 correct (67.50%)\n",
      "\n",
      "Iteration 4100, loss = 0.8613\n",
      "Got 740 / 1000 correct (74.00%)\n",
      "Got 680 / 1000 correct (68.00%)\n",
      "\n",
      "Iteration 4200, loss = 0.6322\n",
      "Got 723 / 1000 correct (72.30%)\n",
      "Got 678 / 1000 correct (67.80%)\n",
      "\n",
      "Iteration 4300, loss = 0.7661\n",
      "Got 728 / 1000 correct (72.80%)\n",
      "Got 686 / 1000 correct (68.60%)\n",
      "\n",
      "Iteration 4400, loss = 0.6664\n",
      "Got 732 / 1000 correct (73.20%)\n",
      "Got 688 / 1000 correct (68.80%)\n",
      "\n",
      "Iteration 4500, loss = 0.8945\n",
      "Got 733 / 1000 correct (73.30%)\n",
      "Got 684 / 1000 correct (68.40%)\n",
      "\n",
      "Starting epoch 6\n",
      "Iteration 4600, loss = 0.7562\n",
      "Got 746 / 1000 correct (74.60%)\n",
      "Got 691 / 1000 correct (69.10%)\n",
      "\n",
      "Iteration 4700, loss = 0.7583\n",
      "Got 773 / 1000 correct (77.30%)\n",
      "Got 691 / 1000 correct (69.10%)\n",
      "\n",
      "Iteration 4800, loss = 0.6497\n",
      "Got 757 / 1000 correct (75.70%)\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "\n",
      "Iteration 4900, loss = 0.5880\n",
      "Got 735 / 1000 correct (73.50%)\n",
      "Got 699 / 1000 correct (69.90%)\n",
      "\n",
      "Iteration 5000, loss = 0.8129\n",
      "Got 729 / 1000 correct (72.90%)\n",
      "Got 686 / 1000 correct (68.60%)\n",
      "\n",
      "Iteration 5100, loss = 0.8493\n",
      "Got 733 / 1000 correct (73.30%)\n",
      "Got 669 / 1000 correct (66.90%)\n",
      "\n",
      "Iteration 5200, loss = 0.5987\n",
      "Got 757 / 1000 correct (75.70%)\n",
      "Got 712 / 1000 correct (71.20%)\n",
      "\n",
      "Iteration 5300, loss = 0.6345\n",
      "Got 745 / 1000 correct (74.50%)\n",
      "Got 675 / 1000 correct (67.50%)\n",
      "\n",
      "Starting epoch 7\n",
      "Iteration 5400, loss = 0.5533\n",
      "Got 797 / 1000 correct (79.70%)\n",
      "Got 701 / 1000 correct (70.10%)\n",
      "\n",
      "Iteration 5500, loss = 0.5736\n",
      "Got 758 / 1000 correct (75.80%)\n",
      "Got 689 / 1000 correct (68.90%)\n",
      "\n",
      "Iteration 5600, loss = 0.5946\n",
      "Got 775 / 1000 correct (77.50%)\n",
      "Got 710 / 1000 correct (71.00%)\n",
      "\n",
      "Iteration 5700, loss = 0.7371\n",
      "Got 744 / 1000 correct (74.40%)\n",
      "Got 687 / 1000 correct (68.70%)\n",
      "\n",
      "Iteration 5800, loss = 0.7039\n",
      "Got 760 / 1000 correct (76.00%)\n",
      "Got 700 / 1000 correct (70.00%)\n",
      "\n",
      "Iteration 5900, loss = 0.5608\n",
      "Got 757 / 1000 correct (75.70%)\n",
      "Got 691 / 1000 correct (69.10%)\n",
      "\n",
      "Iteration 6000, loss = 0.7610\n",
      "Got 759 / 1000 correct (75.90%)\n",
      "Got 694 / 1000 correct (69.40%)\n",
      "\n",
      "Iteration 6100, loss = 0.5322\n",
      "Got 750 / 1000 correct (75.00%)\n",
      "Got 708 / 1000 correct (70.80%)\n",
      "\n",
      "Starting epoch 8\n",
      "Iteration 6200, loss = 0.6184\n",
      "Got 789 / 1000 correct (78.90%)\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "\n",
      "Iteration 6300, loss = 0.7098\n",
      "Got 769 / 1000 correct (76.90%)\n",
      "Got 694 / 1000 correct (69.40%)\n",
      "\n",
      "Iteration 6400, loss = 0.5118\n",
      "Got 801 / 1000 correct (80.10%)\n",
      "Got 713 / 1000 correct (71.30%)\n",
      "\n",
      "Iteration 6500, loss = 0.5383\n",
      "Got 760 / 1000 correct (76.00%)\n",
      "Got 688 / 1000 correct (68.80%)\n",
      "\n",
      "Iteration 6600, loss = 0.4619\n",
      "Got 781 / 1000 correct (78.10%)\n",
      "Got 717 / 1000 correct (71.70%)\n",
      "\n",
      "Iteration 6700, loss = 0.7177\n",
      "Got 767 / 1000 correct (76.70%)\n",
      "Got 699 / 1000 correct (69.90%)\n",
      "\n",
      "Iteration 6800, loss = 0.7338\n",
      "Got 772 / 1000 correct (77.20%)\n",
      "Got 695 / 1000 correct (69.50%)\n",
      "\n",
      "Starting epoch 9\n",
      "Iteration 6900, loss = 0.5604\n",
      "Got 793 / 1000 correct (79.30%)\n",
      "Got 694 / 1000 correct (69.40%)\n",
      "\n",
      "Iteration 7000, loss = 0.5145\n",
      "Got 796 / 1000 correct (79.60%)\n",
      "Got 699 / 1000 correct (69.90%)\n",
      "\n",
      "Iteration 7100, loss = 0.5635\n",
      "Got 803 / 1000 correct (80.30%)\n",
      "Got 727 / 1000 correct (72.70%)\n",
      "\n",
      "Iteration 7200, loss = 0.6132\n",
      "Got 799 / 1000 correct (79.90%)\n",
      "Got 725 / 1000 correct (72.50%)\n",
      "\n",
      "Iteration 7300, loss = 0.5162\n",
      "Got 767 / 1000 correct (76.70%)\n",
      "Got 715 / 1000 correct (71.50%)\n",
      "\n",
      "Iteration 7400, loss = 0.5867\n",
      "Got 758 / 1000 correct (75.80%)\n",
      "Got 696 / 1000 correct (69.60%)\n",
      "\n",
      "Iteration 7500, loss = 0.5651\n",
      "Got 795 / 1000 correct (79.50%)\n",
      "Got 714 / 1000 correct (71.40%)\n",
      "\n",
      "Iteration 7600, loss = 0.6313\n",
      "Got 791 / 1000 correct (79.10%)\n",
      "Got 702 / 1000 correct (70.20%)\n",
      "\n",
      "Accuracy of final model on the test set:\n",
      "Got 7051 / 10000 correct (70.51%)\n"
     ]
    }
   ],
   "source": [
    "# INPUT -> [CONV -> BATCHNORM -> RELU -> POOL]*3 -> FC -> RELU -> FC\n",
    "# add in batch norm\n",
    "\n",
    "def five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes):     \n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    conv1 = tf.layers.conv2d(inputs, filters=channel_1, kernel_size=5, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch1 = tf.contrib.layers.layer_norm(conv1)\n",
    "    relu1 = tf.nn.relu(batch1)\n",
    "    pool1 = tf.layers.max_pooling2d(relu1, pool_size=2, strides=2)\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=channel_2, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch2 = tf.contrib.layers.layer_norm(conv2)\n",
    "    relu2 = tf.nn.relu(batch2)\n",
    "    pool2 = tf.layers.max_pooling2d(relu2, pool_size=2, strides=2)\n",
    "    conv3 = tf.layers.conv2d(pool2, filters=channel_3, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch3 = tf.contrib.layers.layer_norm(conv3)\n",
    "    relu3 = tf.nn.relu(batch3)\n",
    "    pool3 = tf.layers.max_pooling2d(relu3, pool_size=2, strides=2)\n",
    "    flattened = tf.layers.flatten(pool3)\n",
    "    fc1 = tf.layers.dense(flattened, hidden_size, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "    scores = tf.layers.dense(fc1, num_classes,\n",
    "                             kernel_initializer=initializer)\n",
    "    return scores\n",
    "\n",
    "channel_1, channel_2, channel_3, hidden_size, num_classes = 64, 32, 32, 4000, 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 10\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, finally got it. I just want to check if including is_training in the batch_normalization function affects the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 3.3278\n",
      "Got 115 / 1000 correct (11.50%)\n",
      "Got 105 / 1000 correct (10.50%)\n",
      "\n",
      "Iteration 100, loss = 1.5786\n",
      "Got 418 / 1000 correct (41.80%)\n",
      "Got 406 / 1000 correct (40.60%)\n",
      "\n",
      "Iteration 200, loss = 1.3565\n",
      "Got 499 / 1000 correct (49.90%)\n",
      "Got 478 / 1000 correct (47.80%)\n",
      "\n",
      "Iteration 300, loss = 1.4401\n",
      "Got 547 / 1000 correct (54.70%)\n",
      "Got 513 / 1000 correct (51.30%)\n",
      "\n",
      "Iteration 400, loss = 1.1820\n",
      "Got 558 / 1000 correct (55.80%)\n",
      "Got 529 / 1000 correct (52.90%)\n",
      "\n",
      "Iteration 500, loss = 1.5419\n",
      "Got 557 / 1000 correct (55.70%)\n",
      "Got 517 / 1000 correct (51.70%)\n",
      "\n",
      "Iteration 600, loss = 1.2826\n",
      "Got 569 / 1000 correct (56.90%)\n",
      "Got 552 / 1000 correct (55.20%)\n",
      "\n",
      "Iteration 700, loss = 1.3648\n",
      "Got 576 / 1000 correct (57.60%)\n",
      "Got 549 / 1000 correct (54.90%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 800, loss = 1.1414\n",
      "Got 650 / 1000 correct (65.00%)\n",
      "Got 584 / 1000 correct (58.40%)\n",
      "\n",
      "Iteration 900, loss = 1.1456\n",
      "Got 649 / 1000 correct (64.90%)\n",
      "Got 591 / 1000 correct (59.10%)\n",
      "\n",
      "Iteration 1000, loss = 0.9596\n",
      "Got 599 / 1000 correct (59.90%)\n",
      "Got 565 / 1000 correct (56.50%)\n",
      "\n",
      "Iteration 1100, loss = 1.2173\n",
      "Got 612 / 1000 correct (61.20%)\n",
      "Got 579 / 1000 correct (57.90%)\n",
      "\n",
      "Iteration 1200, loss = 1.1018\n",
      "Got 636 / 1000 correct (63.60%)\n",
      "Got 625 / 1000 correct (62.50%)\n",
      "\n",
      "Iteration 1300, loss = 0.9320\n",
      "Got 623 / 1000 correct (62.30%)\n",
      "Got 609 / 1000 correct (60.90%)\n",
      "\n",
      "Iteration 1400, loss = 1.1380\n",
      "Got 611 / 1000 correct (61.10%)\n",
      "Got 597 / 1000 correct (59.70%)\n",
      "\n",
      "Iteration 1500, loss = 1.0759\n",
      "Got 647 / 1000 correct (64.70%)\n",
      "Got 607 / 1000 correct (60.70%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 1600, loss = 0.8382\n",
      "Got 682 / 1000 correct (68.20%)\n",
      "Got 619 / 1000 correct (61.90%)\n",
      "\n",
      "Iteration 1700, loss = 0.8398\n",
      "Got 643 / 1000 correct (64.30%)\n",
      "Got 624 / 1000 correct (62.40%)\n",
      "\n",
      "Iteration 1800, loss = 0.8034\n",
      "Got 679 / 1000 correct (67.90%)\n",
      "Got 640 / 1000 correct (64.00%)\n",
      "\n",
      "Iteration 1900, loss = 0.8827\n",
      "Got 678 / 1000 correct (67.80%)\n",
      "Got 642 / 1000 correct (64.20%)\n",
      "\n",
      "Iteration 2000, loss = 0.9049\n",
      "Got 667 / 1000 correct (66.70%)\n",
      "Got 614 / 1000 correct (61.40%)\n",
      "\n",
      "Iteration 2100, loss = 0.9120\n",
      "Got 689 / 1000 correct (68.90%)\n",
      "Got 640 / 1000 correct (64.00%)\n",
      "\n",
      "Iteration 2200, loss = 0.7845\n",
      "Got 653 / 1000 correct (65.30%)\n",
      "Got 645 / 1000 correct (64.50%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2300, loss = 1.0828\n",
      "Got 699 / 1000 correct (69.90%)\n",
      "Got 654 / 1000 correct (65.40%)\n",
      "\n",
      "Iteration 2400, loss = 0.8444\n",
      "Got 708 / 1000 correct (70.80%)\n",
      "Got 661 / 1000 correct (66.10%)\n",
      "\n",
      "Iteration 2500, loss = 0.9774\n",
      "Got 694 / 1000 correct (69.40%)\n",
      "Got 656 / 1000 correct (65.60%)\n",
      "\n",
      "Iteration 2600, loss = 0.8113\n",
      "Got 701 / 1000 correct (70.10%)\n",
      "Got 652 / 1000 correct (65.20%)\n",
      "\n",
      "Iteration 2700, loss = 0.9445\n",
      "Got 700 / 1000 correct (70.00%)\n",
      "Got 655 / 1000 correct (65.50%)\n",
      "\n",
      "Iteration 2800, loss = 0.8912\n",
      "Got 704 / 1000 correct (70.40%)\n",
      "Got 649 / 1000 correct (64.90%)\n",
      "\n",
      "Iteration 2900, loss = 0.4951\n",
      "Got 687 / 1000 correct (68.70%)\n",
      "Got 655 / 1000 correct (65.50%)\n",
      "\n",
      "Iteration 3000, loss = 0.8924\n",
      "Got 702 / 1000 correct (70.20%)\n",
      "Got 663 / 1000 correct (66.30%)\n",
      "\n",
      "Starting epoch 4\n",
      "Iteration 3100, loss = 0.7045\n",
      "Got 750 / 1000 correct (75.00%)\n",
      "Got 671 / 1000 correct (67.10%)\n",
      "\n",
      "Iteration 3200, loss = 0.9767\n",
      "Got 747 / 1000 correct (74.70%)\n",
      "Got 675 / 1000 correct (67.50%)\n",
      "\n",
      "Iteration 3300, loss = 0.7742\n",
      "Got 698 / 1000 correct (69.80%)\n",
      "Got 642 / 1000 correct (64.20%)\n",
      "\n",
      "Iteration 3400, loss = 0.9561\n",
      "Got 699 / 1000 correct (69.90%)\n",
      "Got 635 / 1000 correct (63.50%)\n",
      "\n",
      "Iteration 3500, loss = 0.8572\n",
      "Got 720 / 1000 correct (72.00%)\n",
      "Got 668 / 1000 correct (66.80%)\n",
      "\n",
      "Iteration 3600, loss = 0.9987\n",
      "Got 732 / 1000 correct (73.20%)\n",
      "Got 670 / 1000 correct (67.00%)\n",
      "\n",
      "Iteration 3700, loss = 0.8338\n",
      "Got 711 / 1000 correct (71.10%)\n",
      "Got 665 / 1000 correct (66.50%)\n",
      "\n",
      "Iteration 3800, loss = 0.6565\n",
      "Got 719 / 1000 correct (71.90%)\n",
      "Got 662 / 1000 correct (66.20%)\n",
      "\n",
      "Starting epoch 5\n",
      "Iteration 3900, loss = 1.0192\n",
      "Got 732 / 1000 correct (73.20%)\n",
      "Got 659 / 1000 correct (65.90%)\n",
      "\n",
      "Iteration 4000, loss = 1.0756\n",
      "Got 720 / 1000 correct (72.00%)\n",
      "Got 663 / 1000 correct (66.30%)\n",
      "\n",
      "Iteration 4100, loss = 0.8605\n",
      "Got 760 / 1000 correct (76.00%)\n",
      "Got 677 / 1000 correct (67.70%)\n",
      "\n",
      "Iteration 4200, loss = 0.5653\n",
      "Got 729 / 1000 correct (72.90%)\n",
      "Got 669 / 1000 correct (66.90%)\n",
      "\n",
      "Iteration 4300, loss = 0.7016\n",
      "Got 727 / 1000 correct (72.70%)\n",
      "Got 651 / 1000 correct (65.10%)\n",
      "\n",
      "Iteration 4400, loss = 0.6175\n",
      "Got 730 / 1000 correct (73.00%)\n",
      "Got 676 / 1000 correct (67.60%)\n",
      "\n",
      "Iteration 4500, loss = 0.7581\n",
      "Got 717 / 1000 correct (71.70%)\n",
      "Got 669 / 1000 correct (66.90%)\n",
      "\n",
      "Starting epoch 6\n",
      "Iteration 4600, loss = 0.5742\n",
      "Got 752 / 1000 correct (75.20%)\n",
      "Got 686 / 1000 correct (68.60%)\n",
      "\n",
      "Iteration 4700, loss = 0.7014\n",
      "Got 751 / 1000 correct (75.10%)\n",
      "Got 671 / 1000 correct (67.10%)\n",
      "\n",
      "Iteration 4800, loss = 0.6762\n",
      "Got 759 / 1000 correct (75.90%)\n",
      "Got 693 / 1000 correct (69.30%)\n",
      "\n",
      "Iteration 4900, loss = 0.5420\n",
      "Got 741 / 1000 correct (74.10%)\n",
      "Got 666 / 1000 correct (66.60%)\n",
      "\n",
      "Iteration 5000, loss = 0.5757\n",
      "Got 752 / 1000 correct (75.20%)\n",
      "Got 677 / 1000 correct (67.70%)\n",
      "\n",
      "Iteration 5100, loss = 0.8221\n",
      "Got 752 / 1000 correct (75.20%)\n",
      "Got 677 / 1000 correct (67.70%)\n",
      "\n",
      "Iteration 5200, loss = 0.7152\n",
      "Got 737 / 1000 correct (73.70%)\n",
      "Got 689 / 1000 correct (68.90%)\n",
      "\n",
      "Iteration 5300, loss = 0.6080\n",
      "Got 734 / 1000 correct (73.40%)\n",
      "Got 672 / 1000 correct (67.20%)\n",
      "\n",
      "Starting epoch 7\n",
      "Iteration 5400, loss = 0.5065\n",
      "Got 807 / 1000 correct (80.70%)\n",
      "Got 681 / 1000 correct (68.10%)\n",
      "\n",
      "Iteration 5500, loss = 0.5904\n",
      "Got 771 / 1000 correct (77.10%)\n",
      "Got 698 / 1000 correct (69.80%)\n",
      "\n",
      "Iteration 5600, loss = 0.6221\n",
      "Got 748 / 1000 correct (74.80%)\n",
      "Got 671 / 1000 correct (67.10%)\n",
      "\n",
      "Iteration 5700, loss = 0.6158\n",
      "Got 728 / 1000 correct (72.80%)\n",
      "Got 662 / 1000 correct (66.20%)\n",
      "\n",
      "Iteration 5800, loss = 0.6033\n",
      "Got 770 / 1000 correct (77.00%)\n",
      "Got 684 / 1000 correct (68.40%)\n",
      "\n",
      "Iteration 5900, loss = 0.6110\n",
      "Got 771 / 1000 correct (77.10%)\n",
      "Got 698 / 1000 correct (69.80%)\n",
      "\n",
      "Iteration 6000, loss = 0.7310\n",
      "Got 736 / 1000 correct (73.60%)\n",
      "Got 682 / 1000 correct (68.20%)\n",
      "\n",
      "Iteration 6100, loss = 0.4620\n",
      "Got 749 / 1000 correct (74.90%)\n",
      "Got 683 / 1000 correct (68.30%)\n",
      "\n",
      "Starting epoch 8\n",
      "Iteration 6200, loss = 0.6857\n",
      "Got 794 / 1000 correct (79.40%)\n",
      "Got 679 / 1000 correct (67.90%)\n",
      "\n",
      "Iteration 6300, loss = 0.7239\n",
      "Got 772 / 1000 correct (77.20%)\n",
      "Got 677 / 1000 correct (67.70%)\n",
      "\n",
      "Iteration 6400, loss = 0.6598\n",
      "Got 784 / 1000 correct (78.40%)\n",
      "Got 684 / 1000 correct (68.40%)\n",
      "\n",
      "Iteration 6500, loss = 0.5545\n",
      "Got 767 / 1000 correct (76.70%)\n",
      "Got 688 / 1000 correct (68.80%)\n",
      "\n",
      "Iteration 6600, loss = 0.4830\n",
      "Got 759 / 1000 correct (75.90%)\n",
      "Got 683 / 1000 correct (68.30%)\n",
      "\n",
      "Iteration 6700, loss = 0.7024\n",
      "Got 772 / 1000 correct (77.20%)\n",
      "Got 697 / 1000 correct (69.70%)\n",
      "\n",
      "Iteration 6800, loss = 0.6055\n",
      "Got 757 / 1000 correct (75.70%)\n",
      "Got 689 / 1000 correct (68.90%)\n",
      "\n",
      "Starting epoch 9\n",
      "Iteration 6900, loss = 0.5740\n",
      "Got 786 / 1000 correct (78.60%)\n",
      "Got 694 / 1000 correct (69.40%)\n",
      "\n",
      "Iteration 7000, loss = 0.4863\n",
      "Got 775 / 1000 correct (77.50%)\n",
      "Got 694 / 1000 correct (69.40%)\n",
      "\n",
      "Iteration 7100, loss = 0.4786\n",
      "Got 792 / 1000 correct (79.20%)\n",
      "Got 707 / 1000 correct (70.70%)\n",
      "\n",
      "Iteration 7200, loss = 0.5182\n",
      "Got 773 / 1000 correct (77.30%)\n",
      "Got 675 / 1000 correct (67.50%)\n",
      "\n",
      "Iteration 7300, loss = 0.5477\n",
      "Got 768 / 1000 correct (76.80%)\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "\n",
      "Iteration 7400, loss = 0.4461\n",
      "Got 771 / 1000 correct (77.10%)\n",
      "Got 691 / 1000 correct (69.10%)\n",
      "\n",
      "Iteration 7500, loss = 0.5023\n",
      "Got 775 / 1000 correct (77.50%)\n",
      "Got 695 / 1000 correct (69.50%)\n",
      "\n",
      "Iteration 7600, loss = 0.7665\n",
      "Got 757 / 1000 correct (75.70%)\n",
      "Got 697 / 1000 correct (69.70%)\n",
      "\n",
      "Accuracy of final model on the test set:\n",
      "Got 7081 / 10000 correct (70.81%)\n"
     ]
    }
   ],
   "source": [
    "# INPUT -> [CONV -> BATCHNORM -> RELU -> POOL]*3 -> FC -> RELU -> FC\n",
    "# add in batch norm\n",
    "\n",
    "def five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes, is_training):     \n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    conv1 = tf.layers.conv2d(inputs, filters=channel_1, kernel_size=5, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch1 = tf.layers.batch_normalization(conv1, training=is_training)\n",
    "    relu1 = tf.nn.relu(batch1)\n",
    "    pool1 = tf.layers.max_pooling2d(relu1, pool_size=2, strides=2)\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=channel_2, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch2 = tf.layers.batch_normalization(conv2, training=is_training)\n",
    "    relu2 = tf.nn.relu(batch2)\n",
    "    pool2 = tf.layers.max_pooling2d(relu2, pool_size=2, strides=2)\n",
    "    conv3 = tf.layers.conv2d(pool2, filters=channel_3, kernel_size=3, strides=(1, 1),\n",
    "                             padding=\"same\", activation=None,\n",
    "                             kernel_initializer=initializer)\n",
    "    batch3 = tf.layers.batch_normalization(conv3, training=is_training)\n",
    "    relu3 = tf.nn.relu(batch3)\n",
    "    pool3 = tf.layers.max_pooling2d(relu3, pool_size=2, strides=2)\n",
    "    flattened = tf.layers.flatten(pool3)\n",
    "    fc1 = tf.layers.dense(flattened, hidden_size, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "    scores = tf.layers.dense(fc1, num_classes,\n",
    "                             kernel_initializer=initializer)\n",
    "    return scores\n",
    "\n",
    "channel_1, channel_2, channel_3, hidden_size, num_classes = 64, 32, 32, 4000, 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return five_layer_conv_functional(inputs, channel_1, channel_2, channel_3, hidden_size, num_classes, is_training)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    return optimizer\n",
    "\n",
    "device = '/cpu:0'\n",
    "print_every = 100\n",
    "num_epochs = 10\n",
    "train_model(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Maybe the is_training flag did help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe what you did \n",
    "\n",
    "In the cell below you should write an explanation of what you did, any additional features that you implemented, and/or any graphs that you made in the process of training and evaluating your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I tried was the 3-layer ConvNet that I made in the notebook: conv(32 5x5 filters) -> relu -> conv(16 3x3 filters) -> relu -> fully-connected layer with 10 classes. I used a learning rate of 1e-3. This model achieved 58.50% validation accuracy after 3 epochs.\n",
    "\n",
    "I next tried changing the numbers of filters in the conv layers: conv(16 5x5 filters) -> relu -> conv(32 3x3 filters) -> relu -> fully-connected layer with 10 classes. This achieved also 54.50% validation accuracy after 1 epoch.\n",
    "\n",
    "Since the above architecture seemed to have accuracy plateauing around 60%, I wanted to try a different architecture. I tried a small ResNet with 1 residual block consisting of conv(32 5x5 filters) -> relu -> conv(16 3x3 filters) -> relu -> conv(32 3x3 filters) + first activation -> relu, and then a fully-connected layer with 10 classes. This achieved 56.70% validation accuracy after 1 epoch.\n",
    "\n",
    "I went back to the original architecture (conv(32 5x5 filters) -> relu -> conv(16 3x3 filters) -> relu -> fully-connected layer with 10 classes) and played with the hyperparameters. When I increased the learning rate to 1e-2, within the first few hundred iterataions, the loss goes down and validation accuracy goes up. So learning rate of 1e-2 seems to work.\n",
    "\n",
    "I next tried the same architecture with a learning rate of 1e-1. The loss did not really go down, and the validation accuracy decreased in the first few hundred iterations. So learning rate of 1e-1 is too high. I decided to stick with learning rate of 1e-2 until some later tests.\n",
    "\n",
    "Using the same architecture as before (conv(32 5x5 filters) -> relu -> conv(16 3x3 filters) -> relu -> fully-connected layer with 10 classes), I tried more epochs and achieved 55.10% validation accuracy after 7 epochs. I thought maybe the model was overfitting the training data based on the big gap between training accuracy and validation accuracy (88.90% vs. 55.10%), so I tried reducing the number of channels in the conv layers.\n",
    "\n",
    "I tried 16 channels for each layer: conv(16 5x5 filters) -> relu -> conv(16 3x3 filters) -> relu -> fully-connected layer with 10 classes. This achieved 55.70% validation accuracy after 6 epochs, which was not much better, so I thought maybe I should make the network deeper instead.\n",
    "\n",
    "I tried a 5-layer ConvNet: conv(16 5x5 filters) -> relu -> conv(16 3x3 filters) -> relu -> conv(16 3x3 filters) -> relu -> conv(16 3x3 filters) -> relu -> fully-connected layer with 10 classes. This achieved 50.90% validation accuracy after 7 epochs.\n",
    "\n",
    "I tried a 6-layer ConvNet (add fc layer near end): conv(16 5x5 filters) -> relu -> conv(16 3x3 filters) -> relu -> conv(16 3x3 filters) -> relu -> conv(16 3x3 filters) -> relu -> fc with 3072 units -> fully-connected layer with 10 classes. This achieved 50.50% validation accuracy after 2 epochs, which did not seem to be much better.\n",
    "\n",
    "Looking back at the class notes, I found this common ConvNet architecture, which includes max pooling: INPUT -> [CONV -> RELU -> POOL]*2 -> FC -> RELU -> FC. The corresponding layer sizes were 32 5x5, 16 3x3, 4000, 10. This achieved 59.30% validation accuracy after 5 epochs, which was better than before.\n",
    "\n",
    "I tried adding another layer to the architecture: INPUT -> [CONV -> RELU -> POOL]*3 -> FC -> RELU -> FC, with corresponding layer sizes of 32 5x5, 16 3x3, 16 3x3, 4000, 10. This achieved 62.25% test accuracy after 5 epochs, which was even better.\n",
    "\n",
    "Then I added batch normalization to INPUT -> [CONV -> BATCHNORM -> RELU -> POOL]*2 -> FC -> RELU -> FC, with layer sizes of 32 5x5, 16 3x3, 4000, 10. This achieved 61.58% test accuracy after 5 epochs.\n",
    "\n",
    "I also added batch normalization to INPUT -> [CONV -> BATCHNORM -> RELU -> POOL]*3 -> FC -> RELU -> FC, with layer sizes of 32 5x5, 16 3x3, 16 3x3, 4000, 10. This achieved 64.16% test accuracy after 5 epochs and 67.99% test accuracy after 10 epochs.\n",
    "\n",
    "Next, I tried playing with the numbers of channels in each conv layer. I tried 32 5x5, 32 3x3, 32 3x3, but the loss was not going down. I also tried 64 5x5, 64 3x3, 64 3x3, but again, the loss was not going down. Finally, I tried 64 5x5, 32 3x3, 32 3x3, and got 68.67% test accuracy after 10 epochs.\n",
    "\n",
    "I thought maybe my learning rate was too high, too close to the unstable learning rate of 1e-1, so I decreased the learning rate to 1e-3. With 64 5x5, 32 3x3, 32 3x3, I was able to get 69.18% test accuracy after 10 epochs. This was very close to the desired accuracy of 70% but not quite there.\n",
    "\n",
    "One last attempt at changing the numbers of channels to 32 5x5, 64 3x3, 64 3x3, yielded 68.36% test accuracy after 10 epochs, but this was still not what I wanted.\n",
    "\n",
    "I changed the normalization layer to use layer normalization instead of batchnorm and finally achieved 70.51% test accuracy after 10 epochs.\n",
    "\n",
    "I also went back to the batchnorm approach because I noticed that my implementation was not using the `is_training` label, which modifies what the batchnorm layer does. When I included `is_training` into these layers, I was able to get 70.81% test accuracy after 10 epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
